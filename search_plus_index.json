{"Overview.html":{"url":"Overview.html","title":"目录","keywords":"","body":"SummarySummary summary 序言 Docker Compose docker compose 安装 docker compose 安装 elasticsearch 集群 Docker Swarm docker swarm 快速入门 docker swarm 网络 docker swam 集群实现负载均衡 docker swarm 创建加密覆盖网络 docker swarm 管理 secrets docker swarm 健康检查 dcoker swarm 更新 docker swarm 部署界面 ui portainer docker swarm 通过 docker compose 部署应用 docker swarm 维护模式 Docker CICD docker makefile ci pipeline github action 构建镜像 自托管构建 如何使用 harbor 搭建企业级镜像仓库 自托管构建 如何使用 tekton 构建镜像 如何使用 gitlab ci 构建镜像 Command docker inspect format Dockerfile Image 构建镜像开源工具 buildah 如何为不同语言快速构建多平台镜像 如何将镜像体积缩减 90 docker 镜像源 docker 镜像管理 Install docker 安装 Issue Logging Manage docker 资源清理 docker 容器隔离与限制 Monitor Network docker 容器应用总是丢包如何分析 docker 配置 dns Registry harbor部署入门指南 podman 部署私有镜像仓库 docker registry 仓库搭建并配置证书 docker 部署 registry ui centos 7 9 harbor 部署镜像仓库 Security Storage docker device mapper 简介 docker storage 驱动 devicemapper 配置 docker thin provisioning 实践 docker devicemapper 扩容 docker nfs volume 创建与使用 原理 容器技术概述 容器架构概述 入门 Podman podman 部署私有镜像仓库 podman 快速入门 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 17:17:22 "},"./":{"url":"./","title":"序言","keywords":"","body":"序言参考联系公众号序言 这是一本关于集合 docker 的书。 参考 Docker 技术入门与实践 Docker 官网 联系 Email: 1zoxun1@gmail.com WeChat: weke59 Youtube: BlackSwanGreen Ins: cnghostwritten Bilibili: LoveDeatRobots 公众号 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker-Compose/":{"url":"Docker-Compose/","title":"Docker Compose","keywords":"","body":"Docker-compose overview1. 简介2. 使用步骤3. 生命周期4. 特色Docker-compose overview 1. 简介 docker-ompose 是一个用于定义和运行多容器 Docker 应用程序的工具。使用 docker-compose，您可以使用 YAML 文件来配置应用程序的服务。然后，使用一个命令，您可以从您的配置中创建并启动所有服务。 docker-compose 适用于所有环境：生产、登台、开发、测试以及 CI 工作流程。 2. 使用步骤 用 docker-compose 基本上是一个三步过程： 使用 a 定义您的应用程序的环境，Dockerfile以便可以在任何地方复制它。 定义构成您的应用程序的服务，docker-compose.yml 以便它们可以在隔离环境中一起运行。 运行docker compose up，Docker compose 命令启动并运行您的整个应用程序。您也可以docker-compose up使用 docker-compose 二进制文件运行。 docker-compose.yml version: \"3.9\" # optional since v1.27.0 services: web: build: . ports: - \"8000:5000\" volumes: - .:/code - logvolume01:/var/log links: - redis redis: image: redis volumes: logvolume01: {} 3. 生命周期 docker-compose 具有用于管理应用程序整个生命周期的命令： 启动、停止和重建服务 查看运行服务的状态 流式传输正在运行的服务的日志输出 在服务上运行一次性命令 4. 特色 docker-compose 使其有效的特点是： 单个主机上的多个隔离环境 docker-compose 使用项目名称将环境彼此隔离。您可以在几个不同的上下文中使用此项目名称： 在开发主机上，创建单个环境的多个副本，例如当您想要为项目的每个功能分支运行稳定副本时 在 CI 服务器上，为了防止构建相互干扰，您可以将项目名称设置为唯一的构建号 在共享主机或开发主机上，以防止可能使用相同服务名称的不同项目相互干扰 默认项目名称是项目目录的基本名称。您可以使用 -p命令行选项或 COMPOSE_PROJECT_NAME环境变量设置自定义项目名称。 默认项目目录是 docker-compose 文件的基本目录。可以使用--project-directory命令行选项定义它的自定义值。 创建容器时保留卷数据 docker-compose 会保留您的服务使用的所有卷。运行时docker-compose up ，如果它找到以前运行的任何容器，它会将卷从旧容器复制到新容器。此过程可确保您在卷中创建的任何数据都不会丢失。 仅重新创建已更改的容器 docker-compose 缓存用于创建容器的配置。当您重新启动未更改的服务时，docker-compose 会重新使用现有容器。重复使用容器意味着您可以非常快速地更改您的环境。 变量和在环境之间移动组合 docker-compose 支持 docker-compose 文件中的变量。您可以使用这些变量为不同的环境或不同的用户定制您的组合。有关详细信息，请参阅变量替换。 extends您可以使用该字段或通过创建多个 docker-compose 文件来扩展docker-compose 文件 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Compose/docker-compose_1_install.html":{"url":"Docker-Compose/docker-compose_1_install.html","title":"docker compose 安装","keywords":"","body":"docker-compose 安装1. 安装2. 命令docker-compose 安装 1. 安装 Linux Linux 上我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：https://github.com/docker/compose/releases。 运行以下命令以下载 Docker Compose 的当前稳定版本： $ sudo curl -L \"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose 要安装其他版本的 Compose，请替换 1.24.1。 将可执行权限应用于二进制文件： $ sudo chmod +x /usr/local/bin/docker-compose 创建软链： $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose 测试是否安装成功： $ docker-compose --version cker-compose version 1.24.1, build 4667896b 2. 命令 docker-compose up -d nginx 构建建启动nignx容器 docker-compose exec nginx bash 登录到nginx容器中 docker-compose down 删除所有nginx容器,镜像 docker-compose ps 显示所有容器 docker-compose restart nginx 重新启动nginx容器 docker-compose run --no-deps --rm php-fpm php -v 在php-fpm中不启动关联容器，并容器执行php -v 执行完成后删除容器 docker-compose build nginx 构建镜像 。 docker-compose build --no-cache nginx 不带缓存的构建。 docker-compose logs nginx 查看nginx的日志 docker-compose logs -f nginx 查看nginx的实时日志 docker-compose config -q 验证（docker-compose.yml）文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。 docker-compose events --json nginx 以json的形式输出nginx的docker日志 docker-compose pause nginx 暂停nignx容器 docker-compose unpause nginx 恢复ningx容器 docker-compose rm nginx 删除容器（删除前必须关闭容器） docker-compose stop nginx 停止nignx容器 docker-compose start nginx 启动nignx容器 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Compose/docker-compose_install_2_elasticsearch.html":{"url":"Docker-Compose/docker-compose_install_2_elasticsearch.html","title":"docker compose 安装 elasticsearch 集群","keywords":"","body":"docker-compose 安装 elasticsearch 集群1. 创建一个es节点的集群2. 创建两个节点的es集群docker-compose 安装 elasticsearch 集群 1. 创建一个es节点的集群 $vim docker-compose.yaml version: '2.2' services: cerebro: image: lmenezes/cerebro:0.9.2 container_name: cerebro ports: - \"9000:9000\" command: - -Dhosts.0.host=http://elasticsearch:9200 networks: - es7net kibana: image: kibana:7.2.0 container_name: kibana7 environment: - I18N_LOCALE=zh-CN - XPACK_GRAPH_ENABLED=true - TIMELION_ENABLED=true - XPACK_MONITORING_COLLECTION_ENABLED=\"true\" - ELASTICSEARCH_USERNAME=kibana - ELASTICSEARCH_PASSWORD=demo_password ports: - \"5601:5601\" networks: - es7net elasticsearch: #image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 image: elasticsearch:7.2.0 container_name: es7_01 environment: - cluster.name=test-es - node.name=es7_01 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms1G -Xmx1G\" - \"TZ=Asia/Shanghai\" - discovery.type=single-node - path.data=node0_data - xpack.security.enabled=false - xpack.security.transport.ssl.enabled=false ulimits: memlock: soft: -1 hard: -1 privileged: true volumes: - /data/es7data1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - es7net networks: es7net: driver: bridge $ docker-compose up -d $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b291185b6516 elasticsearch:7.2.0 \"/usr/local/bin/dock…\" 6 minutes ago Up 4 minutes 0.0.0.0:9200->9200/tcp, 9300/tcp es7_01 729527faefb0 kibana:7.2.0 \"/usr/local/bin/kiba…\" 12 minutes ago Up 4 minutes 0.0.0.0:5601->5601/tcp kibana7 bd4dbad9737c lmenezes/cerebro:0.9.2 \"/opt/cerebro/bin/ce…\" 12 minutes ago Up 4 minutes 0.0.0.0:9000->9000/tcp cerebro 2. 创建两个节点的es集群 version: '2.2' services: cerebro: image: lmenezes/cerebro:0.8.3 container_name: cerebro ports: - \"9000:9000\" command: - -Dhosts.0.host=http://elasticsearch:9200 networks: - es7net kibana: image: docker.elastic.co/kibana/kibana:7.1.0 container_name: kibana7 environment: - I18N_LOCALE=zh-CN - XPACK_GRAPH_ENABLED=true - TIMELION_ENABLED=true - XPACK_MONITORING_COLLECTION_ENABLED=\"true\" ports: - \"5601:5601\" networks: - es7net elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_01 environment: - cluster.name=geektime - node.name=es7_01 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - discovery.seed_hosts=es7_01,es7_02 - cluster.initial_master_nodes=es7_01,es7_02 ulimits: memlock: soft: -1 hard: -1 volumes: - es7data1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - es7net elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_02 environment: - cluster.name=geektime - node.name=es7_02 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - discovery.seed_hosts=es7_01,es7_02 - cluster.initial_master_nodes=es7_01,es7_02 ulimits: memlock: soft: -1 hard: -1 volumes: - es7data2:/usr/share/elasticsearch/data networks: - es7net volumes: es7data1: driver: local es7data2: driver: local networks: es7net: driver: bridge 访问http://ip:9000 更多阅读： 安装 docker 安装 docker-compose 镜像拉取慢得解决方法 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/":{"url":"Docker-Swarm/","title":"Docker Swarm","keywords":"","body":"Docker Swarm 介绍1. 什么是 Docker Swarm？2. Docker Swarm 是做什么用的？3. Docker Swarm 还在使用吗？4. Docker Swarm 与 Docker Compose5. Docker Swarm 做负载均衡吗？6. Docker Swarm 定义7. Docker Swarm 模式服务有哪两种类型？8. 什么是 Docker Swarm 节点？9. Docker Swarm 的好处：我需要 Docker Swarm 吗？Docker Swarm 介绍 1. 什么是 Docker Swarm？ Docker Swarm 解释： Docker Swarm 是一组物理机或虚拟机，它们运行 Docker 应用程序并已配置为加入集群。将一组机器集群在一起后，您仍然可以运行您习惯的 Docker 命令，但它们现在将由集群中的机器执行。集群的活动由 swarm manager 控制，加入集群的机器称为节点。 2. Docker Swarm 是做什么用的？ Docker swarm 是一个容器编排工具，这意味着它允许用户管理部署在多个主机上的多个容器。 与 docker swarm 操作相关的主要好处之一是为应用程序提供的高可用性。在 docker swarm 中，通常有多个工作节点和至少一个管理节点，负责有效地处理工作节点的资源并确保集群高效运行。 3. Docker Swarm 还在使用吗？ 是的，Docker Swarm 仍然包含在 docker-ce 中，但是 Docker Swarm 不再提供软件即服务。 4. Docker Swarm 与 Docker Compose Docker Swarm 和 Docker Compose 的区别在于 Compose 用于在同一主机上配置多个容器。Docker Swarm 的不同之处在于它是一个容器编排工具。这意味着 Docker Swarm 允许您将容器连接到类似于 Kubernetes 的多个主机。 5. Docker Swarm 做负载均衡吗？ 是的，Docker Swarm 做了负载均衡。Docker Swarm 的负载均衡器运行在每个节点上，能够跨多个容器和主机均衡负载请求。 6. Docker Swarm 定义 为了将我们对 Docker swam 的理解置于上下文中，让我们退后一步，定义一些围绕容器和 docker 应用程序的更基本的术语。 Docker是一个软件平台，使软件开发人员能够轻松地将容器的使用集成到软件开发过程中。Docker 平台是开源的，可用于 Windows和 Mac，使在各种平台上工作的开发人员都可以访问它。该应用程序在主机操作系统和容器化应用程序之间提供了一个控制接口。 容器及其在软件开发过程中的使用和管理是 docker 应用的主要关注点。容器允许开发人员将应用程序与在任何计算环境中运行所需的所有必要代码和依赖项打包在一起。因此，容器化应用程序在从一个计算环境转移到另一个计算环境时可以可靠地运行。在 docker 应用程序中，通过运行镜像来启动容器。 images是一个可执行文件包，其中包含运行应用程序所需的所有代码、库、运行时、二进制文件和配置文件。容器可以描述为图像的运行时实例。 Dockerfile 是定义可移植images内容的文件类型的名称。想象一下，您打算用 Java 编程语言编写一个程序。您的计算机本身不理解 Java，因此您需要一种将代码转换为机器代码的方法。执行此操作所需的库、配置文件和程序统称为“Java 运行时环境 (JRE)”。在 Docker 中，所有这些资产都将包含在 Dockerfile 中。 因此，无需将 JRE 安装到您的计算机上，您只需下载一个可移植的 JRE 作为images并将其与您的应用程序代码一起包含在容器中。从容器启动应用程序时，应用程序顺利运行所需的所有资源都将存在于隔离的容器化环境中。 通过这四个定义，我们可以拼凑出软件开发人员如何从Docker应用程序中受益的理解。容器是打包应用程序的理想方法，因为无论用于运行应用程序的计算机平台如何，它们都能实现一致的性能。容器通过运行镜像启动，镜像的数据存储在Dockerfile中。 7. Docker Swarm 模式服务有哪两种类型？ Docker Swarm 有两种类型的服务：复制的和全局的。 复制服务： Swarm 模式复制服务功能是通过您为 swarm 管理器指定一些复制任务分配给可用节点来实现的。 全局服务：全局服务通过使用 swam 管理器将一个任务调度到满足服务约束和资源要求的每个可用节点来发挥作用。 8. 什么是 Docker Swarm 节点？ docker swarm 由一组在集群中运行的物理或虚拟机组成。当一台机器加入集群时，它成为该集群中的一个节点。docker swarm 函数识别三种不同类型的节点，每一种在 docker swarm 生态系统中都有不同的角色： Docker Swarm 管理器节点 管理节点的主要功能是将任务分配给集群中的工作节点。管理器节点还有助于执行操作集群所需的一些管理任务。Docker 建议一个 swarm 最多使用七个管理器节点。 Docker Swarm 领导节点 建立集群时，使用 Raft 共识算法将其中的一个指定为“领导节点”。领导节点为集群做出所有集群管理和任务编排决策。如果领导节点由于中断或故障而变得不可用，则可以使用 Raft 共识算法选举新的领导节点。 Docker Swarm 工作节点 在具有众多主机的 docker swarm 中，每个工作节点通过接收和执行管理节点分配给它的任务来发挥作用。默认情况下，所有管理器模式也是工作节点，并且能够在有可用资源时执行任务。 9. Docker Swarm 的好处：我需要 Docker Swarm 吗？ 我们看到越来越多的开发人员采用 Docker 引擎并使用 docker swarms 来更有效地生产、更新和操作应用程序。甚至像谷歌这样的软件巨头也在采用基于容器的方法，比如 docker swarm。以下是 Docker Swarm 变得越来越流行的三个简单原因： 利用容器的力量 开发人员喜欢使用 docker swarm，因为它充分利用了容器提供的设计优势。容器允许开发人员在自包含的虚拟环境中部署应用程序或服务，这是以前虚拟机领域的任务。容器正在证明是一种更轻量级的虚拟机版本，因为它们的架构允许它们更有效地利用计算能力。 Docker Swarm 帮助保证高服务可用性 docker swarms 的主要好处之一是通过冗余提高应用程序的可用性。为了发挥作用，docker swarm 必须有一个可以将任务分配给工作节点的 swarm 管理器。通过实现多个管理器，开发人员可以确保即使其中一个管理器节点发生故障，系统也可以继续运行。Docker 建议每个集群最多使用七个管理器节点。 自动负载平衡 Docker swarm 使用各种方法来安排任务，以确保有足够的资源可供所有容器使用。通过一个可以描述为自动负载平衡的过程，swarm manager 确保将容器工作负载分配到最合适的主机上运行，​​以实现最佳效率。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_1_start.html":{"url":"Docker-Swarm/docker_swarm_1_start.html","title":"docker swarm 快速入门","keywords":"","body":"Docker Swarm 快速入门1.1 初始化集群1.2 加入新节点1.3 创建 overlay 网络1.4 部署服务 service1.5 状态监测1.6 弹缩服务 serviceDocker Swarm 快速入门 将单主机Docker主机转换为多主机Docker swarm集群模式。默认情况下，Docker作为一个隔离的单节点工作。所有容器仅部署在引擎上。群模式将它变成了一个多主机集群感知引擎。 初始化群模式的第一个节点成为管理器。当新的节点加入集群时，它们可以在管理者或工人之间调整角色。您应该在生产环境中运行3-5个管理器，以确保高可用性。 Docker CLI内置了Swarm Mode $ docker swarm --help Usage: docker swarm COMMAND Manage Swarm Commands: ca Display and rotate the root CA init Initialize a swarm join Join a swarm as a node and/or manager join-token Manage join tokens leave Leave the swarm unlock Unlock swarm unlock-key Manage the unlock key update Update the swarm Run 'docker swarm COMMAND --help' for more information on a command. 1.1 初始化集群 最重要的是如何初始化群模式。初始化是通过init完成的。 $ docker swarm init Swarm initialized: current node (d5oub8tip41v0iedsotw02k1r) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-04j3xv6fsp4h2sp2al7ast6hjcntgrztcf24e13ozxazsbuvpx-cln0834q5dzecdkeiorf84tip 172.17.0.14:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow 运行该命令后，Docker引擎知道如何与集群一起工作，并成为集群的管理器。初始化的结果是一个令牌，用于以安全的方式添加额外的节点。在扩展集群时，请确保此token 令牌的安全性和安全性。 在下一步中，我们将添加更多节点并在这些主机上部署容器。 1.2 加入新节点 启用集群模式后，可以添加额外的节点并在所有节点上发出命令。如果节点突然消失，例如，由于崩溃，在这些主机上运行的容器将自动重新调度到其他可用节点上。重新调度确保您不会损失容量，并提供高可用性。 在每个希望添加到集群的附加节点上，使用Docker CLI加入现有组。连接是通过将另一个主机指向集群的当前管理器来完成的。在本例中，是第一个主机。 Docker现在使用一个额外的端口2377来管理集群。应该阻止公共访问该端口，只允许受信任的用户和节点访问该端口。我们建议使用vpn或私有网络来确保访问安全。 第一个任务是获取向集群添加工作人员所需的令牌。出于演示的目的，我们将通过swarm join-token询问管理器这个令牌是什么。在生产中，此令牌应该安全地存储，并且只可由受信任的个人访问。 第二台机器执行： $ token=$(ssh -o StrictHostKeyChecking=no 172.17.0.14 \"docker swarm join-token -q worker\") && echo $token Warning: Permanently added '172.17.0.14' (ECDSA) to the list of known hosts. SWMTKN-1-04j3xv6fsp4h2sp2al7ast6hjcntgrztcf24e13ozxazsbuvpx-cln0834q5dzecdkeiorf84tip 在第二个主机上，通过管理器请求访问加入集群。令牌作为附加参数提供。 $ docker swarm join 172.17.0.14:2377 --token $token This node joined a swarm as a worker. 默认情况下，管理器将自动接受添加到集群的新节点。可以通过以下命令查看集群中的所有节点 $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION d5oub8tip41v0iedsotw02k1r * host01 Ready Active Leader 19.03.13 1.3 创建 overlay 网络 群模式还引入了一种改进的网络模型。在以前的版本中，Docker需要使用外部键值存储，如Consul，以确保整个网络的一致性。对协商一致意见和KV的需求现在已被纳入Docker内部，不再依赖外部服务. 改进的网络方法遵循与前面相同的语法。覆盖网络用于不同主机上的容器之间通信。在背后，这是一个虚拟可扩展LAN (VXLAN)，设计用于大规模基于云的部署。 下面的命令将创建一个新的覆盖网络称为skynet。所有注册到这个网络的容器都可以彼此通信，而不管它们部署到哪个节点上。 $ docker network create -d overlay skynet 4a687dx7ym4qj8wddr0vn1k0r 1.4 部署服务 service 默认情况下，Docker使用扩散复制模型来决定哪些容器应该在哪些主机上运行。扩展方法确保容器均匀地部署在集群中。这意味着，如果从集群中删除其中一个节点，实例将已经在其他节点上运行。删除节点上的工作负载将在其余可用节点上重新调度。 服务service的新概念用于跨集群运行容器。这是一个比容器更高级的概念。服务允许您定义应用程序应该如何大规模部署。通过更新服务，Docker以托管的方式更新所需的容器。 在本例中，我们将部署镜像 katacoda/docker-http-server。我们正在为一个名为http的服务定义一个友好的名称，并且它应该被附加到新创建的skynet网络。 为了确保复制和可用性，我们在集群中运行容器的两个副本实例。 最后，我们把这两个容器一起在port 80。向集群中的任何节点发送HTTP请求将由集群中的一个容器处理请求。接受请求的节点可能不是容器响应的节点。相反，Docker在所有可用的容器中对请求进行负载平衡。 $ docker service create --name http --network skynet --replicas 2 -p 80:80 katacoda/docker-http-server rt3hun6j3rtuy33vn2vrn2zv7 overall progress: 2 out of 2 tasks 1/2: running [==================================================>] 2/2: running [==================================================>] verify: Service converged 通过CLI命令可以查看集群中运行的服务 $ docker service ls ID NAME MODE REPLICAS IMAGE PORTS rt3hun6j3rtu http replicated 2/2 katacoda/docker-http-server:latest *:80->80/tcp 当容器启动时，您将看到它们使用ps命令。您应该在每个主机上看到该容器的一个实例。 列出第一个主机上的容器 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8f732975d0a5 katacoda/docker-http-server:latest \"/app\" 2 m 列出第二台主机上的容器 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7d5a1b2112cd katacoda/docker-http-server:latest \"/app\" 2 minutes ago Up 2 minutes 80/tcp http.2.vcxurhlb7j39mq4hmzz6nyagx 如果我们向公共端口发出HTTP请求，它将由两个容器处理 $ curl host01 This request was processed by host: 7d5a1b2112cd $ curl host01 This request was processed by host: 8f732975d0a5 1.5 状态监测 服务概念允许您检查集群和运行中的应用程序的运行状况和状态。 您可以在集群中查看与某个服务关联的所有任务的列表。在本例中，每个任务都是一个容器 $ docker service ps http ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ey77icv0scbo http.1 katacoda/docker-http-server:latest host01 Running Running 5 minutes ago vcxurhlb7j39 http.2 katacoda/docker-http-server:latest host02 Running Running 5 minutes ago 您可以通过以下方式查看服务的详细信息和配置 $ docker service inspect --pretty http ID: rt3hun6j3rtuy33vn2vrn2zv7 Name: http Service Mode: Replicated Replicas: 2 Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: katacoda/docker-http-server:latest@sha256:76dc8a47fd019f80f2a3163aba789faf55b41b2fb06397653610c754cb12d3ee Init: false Resources: Networks: skynet Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress 在每个节点上，您可以询问它当前正在运行哪些任务。Self是指管理节点Leader 通过节点ID可以查询单个主机 $ docker node ls -q | head -n1 d5oub8tip41v0iedsotw02k1r $ docker node ps $(docker node ls -q | head -n1) ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS ey77icv0scbo http.1 katacoda/docker-http-server:latest host01 Running Running 7 minutes ago 在下一步中，我们将扩展服务以运行容器的更多实例。 1.6 弹缩服务 service 服务允许我们扩展在集群中运行的任务实例的数量。由于它了解如何启动容器以及哪些容器正在运行，因此可以根据需要轻松地启动或删除容器。目前缩放是手动的。但是，API可以连接到external system，比如metrics dashboard。 目前，我们有两个运行的负载平衡容器，它们正在处理我们的请求 $ curl host01 This request was processed by host: 8f732975d0a5 $ curl host01 This request was processed by host: 7d5a1b2112cd 下面的命令将扩展我们的http服务，使其在五个容器中运行。 $ docker service scale http=5 http scaled to 5 overall progress: 5 out of 5 tasks 1/5: running [==================================================>] 2/5: running [==================================================>] 3/5: running [==================================================>] 4/5: running [==================================================>] 5/5: running [==================================================>] verify: Service converged 负载均衡器将自动更新。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_2_network.html":{"url":"Docker-Swarm/docker_swarm_2_network.html","title":"docker swarm 网络","keywords":"","body":"Docker Swarm 网络Docker Swarm 网络 默认情况下，Docker作为一个隔离的单节点工作。所有容器仅部署在引擎上。群模式将它变成了一个多主机集群感知引擎。 初始化集群 $ docker swarm init Swarm initialized: current node (korrjr24x2drfvlu78xi77lno) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-1hriikcsgzi577cl3xcu6s0x7kk3058by92vvbtdltjsz8mp9s-3j2cuvx64eqm0tmkmwru6tn9z 172.17.0.139:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. $ 在第二台主机上执行下面的命令，将它作为一个worker添加到集群中。 $ token=$(ssh -o StrictHostKeyChecking=no 172.17.0.139 \"docker swarm join-token -q worker\") && docker swarm join 172.17.0.139:2377 --token $token Warning: Permanently added '172.17.0.139' (ECDSA) to the list of known hosts. This node joined a swarm as a worker. Overlay Networks是通过Docker CLI创建的，类似于在主机之间创建桥接网络。当创建网络时，将使用一种覆盖驱动类型。当新的服务通过集群模式部署时，它们可以利用这个网络允许容器进行通信. 要创建Overlay Network，使用CLI并定义驱动程序。网络只能通过群管理器节点创建。网络名称为app1-network $ docker network create -d overlay app1-network vuq3m5hi0t0jkvo1djuhpeqsp $ docker network ls NETWORK ID NAME DRIVER SCOPE vuq3m5hi0t0j app1-network overlay swarm f9000dd7435e bridge bridge local d17623c76ebf docker_gwbridge bridge local 8b89e3388c32 host host local kuugxuiaalh1 ingress overlay swarm 注意:你创建的overlay网络不会出现在工作节点上。manager节点处理网络创建和正在部署的服务。 $ docker network ls NETWORK ID NAME DRIVER SCOPE 1d52a41d7ffb bridge bridge local 398c1bd88b1d docker_gwbridge bridge local 8b89e3388c32 host host local kuugxuiaalh1 ingress overlay swarm b3dc159371bf none null local 一旦创建了网络，就可以部署服务，并能够与网络上的其他容器通信。 下面将使用网络部署Redis服务。该服务的名称将是redis，可用于通过DNS发现。 $ docker service create --name redis --network app1-network redis:alpine wdz1i71gu6c1ep54jjfja1ziu 下一步将在不同的节点上部署一个web应用程序，通过网络与Redis进行交互。 $ docker service create \\ > --network app1-network -p 80:3000 \\ > --replicas 1 --name app1-web \\ > katacoda/redis-node-docker-example p8ktbxnju1cy9vopuaxwobe0q 对于双节点部署，每个容器将被部署到不同的主机上。 他们会利用覆盖网络和DNS发现进行通信。发送HTTP请求将在Redis中保持客户端的IP。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES cb586ee1000f redis:alpine \"docker-entrypoint.s…\" 55 seconds ago Up 52 seconds 6379/tcp redis.1.ajuo5o3z4snh0shbjur9qhnsg $ curl host01 This page was generated after talking to redis. Application Build: 1 Total requests: 1 IP count: ::ffff:10.0.0.2: 1 $ curl host01 This page was generated after talking to redis. Application Build: 1 Total requests: 2 IP count: 参考： Manage swarm service networks Use overlay networks Networking with overlay networks How Docker Swarm Container Networking Works – Under the Hood Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_3_load_balancing.html":{"url":"Docker-Swarm/docker_swarm_3_load_balancing.html","title":"docker swam 集群实现负载均衡","keywords":"","body":"docker swam 集群实现负载均衡1. 初始化集群2. 虚拟IP3. 服务发现4. 多主机LB和服务发现docker swam 集群实现负载均衡 1. 初始化集群 第一个节点（manager）： $ docker swarm init Swarm initialized: current node (9qksq9kb8tt9i5un3y2eqbhlj) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5b3w5w10px9ok0t39zs1nujng8x37rcapfkg885hmrokb5wvqi-9v45djurbjeajp3ln80zqcevu 172.17.0.6:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 第二个节点（worker）： $ docker swarm join 172.17.0.6:2377 --token $(ssh -o StrictHostKeyChecking=no 172.17.0.6 \"docker swarm join-token -q worker\") Warning: Permanently added '172.17.0.6' (ECDSA) to the list of known hosts. This node joined a swarm as a worker. 默认情况下，对服务的请求基于公共端口进行负载均衡。 2. 虚拟IP 下面的命令将创建一个名为lbapp1的新服务，其中运行两个容器。服务通过端口公开81 $ docker service create --name lbapp1 --replicas 2 -p 81:80 katacoda/docker-http-server 7w8k1esxuyx1gs17hwa4svop5 当向集群中端口81上的节点发出请求时，它会将负载分散到两个容器上。 $ curl host01:81 This request was processed by host: cbc4fa365043 $ curl host01:81 This request was processed by host: faecced3dc9f HTTP响应指示哪个容器处理请求。在第二台主机上运行命令会得到相同的结果，它会跨这两台主机处理请求。 在下一步中，我们将探讨如何使用它来部署一个实际的应用程序。 3. 服务发现 Docker群模式包括一个路由网，它支持多主机网络。它允许两个不同主机上的容器通信，就像它们在同一主机上一样。它通过创建虚拟可扩展LAN (VXLAN)来实现这一点，VXLAN是为基于云的网络设计的。 路由以两种不同的方式工作。首先，基于公共端口暴露的服务。任何对端口的请求都将被分发。其次，该服务被赋予一个虚拟IP地址，该IP地址仅在Docker网络内部可路由。当向IP地址发出请求时，它们被分发到底层容器。这个虚拟IP是在Docker中的嵌入式DNS服务器上注册的。当基于服务名进行DNS解析时，返回Virtual IP。 在这个步骤中，你将创建一个负载均衡的http，它被附加到一个覆盖网络，并查找它是虚拟IP。 $ docker network create --attachable -d overlay eg1 cf4fx7p2a75irup0pjafylpd4 这个网络将是一个“群体范围网络”。这意味着只有作为服务启动的容器才能将自己附加到网络上。 $ docker service create --name http --network eg1 --replicas 2 katacoda/docker-http-server 通过调用服务http, Docker添加了一个条目到它是嵌入式DNS服务器。网络上的其他容器可以使用友好的名称来发现IP地址。与端口一起，这个IP地址可以在网络内部使用，以达到负载均衡。 使用Dig查找内部虚拟IP。通过使用--attable标志，Swarm服务之外的容器可以访问网络。 $ docker run --name=dig --network eg1 benhall/dig dig http Unable to find image 'benhall/dig:latest' locally latest: Pulling from benhall/dig 12b41071e6ce: Pull complete d23aaa6caac4: Pull complete a3ed95caeb02: Pull complete Digest: sha256:ed7d241f0faea3a015d13117824c04a433a79032619862e4e3741a31eb9e4272 Status: Downloaded newer image for benhall/dig:latest ; > DiG 9.10.2 > http ;; global options: +cmd ;; Got answer: ;; ->>HEADER 通过ping该名称还可以发现IP地址。 $ docker run --name=ping --network eg1 alpine ping -c5 http PING http (10.0.1.2): 56 data bytes 64 bytes from 10.0.1.2: seq=0 ttl=64 time=0.287 ms 64 bytes from 10.0.1.2: seq=1 ttl=64 time=0.108 ms 64 bytes from 10.0.1.2: seq=2 ttl=64 time=0.160 ms 64 bytes from 10.0.1.2: seq=3 ttl=64 time=0.157 ms 64 bytes from 10.0.1.2: seq=4 ttl=64 time=0.131 ms --- http ping statistics --- 5 packets transmitted, 5 packets received, 0% packet loss round-trip min/avg/max = 0.108/0.168/0.287 ms 这应该与给服务的虚拟IP相匹配。您可以通过检查服务来发现这一点。 $ docker service inspect http --format=\"{{.Endpoint.VirtualIPs}}\" [{cf4fx7p2a75irup0pjafylpd4 10.0.1.2/24}] 每个容器仍然会被赋予一个唯一的IP地址。 $ docker inspect --format=\"{{.NetworkSettings.Networks.eg1.IPAddress}}\" $(docker'{print $1}')cker-http-server | head -n1 | awk 10.0.1.4 这个虚拟IP确保在集群中按照预期的方式进行负载平衡。而IP地址确保它在集群外工作。 4. 多主机LB和服务发现 虚拟IP和端口负载均衡和服务发现都可以用于多主机场景，应用程序可以在不同的主机上与不同的服务通信。 在这个步骤中，我们将部署一个复制的Node.js应用程序，它与Redis通信来存储数据。 首先，需要有一个覆盖网络，应用程序和数据存储可以连接。 $ docker network create -d overlay app1-network 部署Redis时，可以连接网络。应用程序希望能够连接到一个名为Redis的实例。为了使应用程序能够通过嵌入式DNS发现虚拟IP，我们调用服务Redis。 $ docker service create --name redis --network app1-network redis:alpine 每个主机都应该有一个Node.js容器实例，其中一个主机存储Redis。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 06f131fda216 redis:alpine \"docker-entrypoint.s…\" 27 seconds ago Up 25 seconds 6379/tcp redis.1.mq95cigfa7etgefqs1pye9crv 1890d08df8cc katacoda/docker-http-server:latest \"/app\" 7 minutes ago Up 7 minutes 80/tcp http.2.j5c4ari06egpy3zqw6b0n3iyl 调用HTTP服务器将在Redis中存储请求并返回结果。这是负载平衡，两个容器通过覆盖网络与Redis容器通信。 应用程序现在分布在多个主机上。 参考: Docker Swarm (cluster communication, service discovery, load balancing) The Definitive Guide to Docker Swarm How to get started with load balancing Docker Swarm mode Traefik and Docker Swarm: A Dynamic Duo for Cloud-Native Container Networking HAProxy on Docker Swarm: Load Balancing and DNS Service Discovery Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_4_encrypted_network.html":{"url":"Docker-Swarm/docker_swarm_4_encrypted_network.html","title":"docker swarm 创建加密覆盖网络","keywords":"","body":"Docker Swarm 创建加密覆盖网络1. 初始化 swarm2. 创建未加密的覆盖网络3. 监控网络4. 创建加密覆盖网络Docker Swarm 创建加密覆盖网络 1. 初始化 swarm 默认情况下，Docker作为一个隔离的单节点工作。所有容器仅部署在引擎上。Swarm Mode将它变成了一个多主机集群感知引擎。为了使用秘密功能，Docker必须处于“群模式”。这是通过 $ docker swarm init Swarm initialized: current node (rcy8eo8ipksi2urvndo8i38te) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5e7sg3ldmijd0sjz20h3382up1d0hbo7nibk465i325yup9jgf-8zhcijlpb1ufbhnzu4drcy7g3 172.17.0.71:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 在第二台主机上执行下面的命令，将它作为一个worker添加到集群中。 $ token=$(ssh -o StrictHostKeyChecking=no 172.17.0.71 \"docker swarm join-token -q worker\") && docker swarm join 172.17.0.71:2377 --token $token Warning: Permanently added '172.17.0.71' (ECDSA) to the list of known hosts. token=$(ssh -o StrictHostKeyChecking=no 172.17.0.71 \"docker swarm join-token -q worker\") && docker swarm join 172.17.0.71:2377 --token $token 2. 创建未加密的覆盖网络 下面的命令将创建一个未加密的覆盖网络，其中部署了两个服务 运行该命令。这将用于演示嗅探未加密网络上的流量。 docker network create -d overlay app1-network docker service create --name redis --network app1-network redis:alpine docker service create \\ --network app1-network -p 80:3000 \\ --replicas 1 --name app1-web \\ katacoda/redis-node-docker-example 3. 监控网络 部署好服务后，可以使用TCPDump查看不同主机之间的流量。这将在Docker主机上安装TCPDump，并开始收集通过覆盖网络发送的流量 请等待业务部署完成 第一台机器 docker service ls 一旦部署完成，就可以通过向web应用发送HTTP请求来产生流量。这反过来又会给Redis创建网络请求 curl host01 第二台机器执行 $ ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no root@host01 Warning: Permanently added 'host01,172.17.0.71' (ECDSA) to the list of known hosts. yes | pacman -Sy tcpdump openssl tcpdump -s 1500 -A -i ens3 port 4789 $ yes | pacman -Sy tcpdump openssl The program 'pacman' is currently not installed. You can install it by typing: apt install pacman $ tcpdump -s 1500 -A -i ens3 port 4789 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on ens3, link-type EN10MB (Ethernet), capture size 1500 bytes 06:31:12.096732 IP 172.17.0.71.35083 > host02.4789: VXLAN, flags [I] (0x08), vni 4096 IP 10.0.0.2.39040 > 10.0.0.5.http: Flags [S], seq 1229698674, win 43690, options [mss 65495,sackOK,TS val 64177 ecr 0,nop,wscale 7], length 0 E..n....@.^F...G...J.....Z...........B ....B .....E.. 172.17.0.71.4789: VXLAN, flags [I] (0x08), vni 4096 IP 10.0.0.5.http > 10.0.0.2.39040: Flags [S.], seq 354703990, ack 1229698675, win 27960, options [mss 1410,sackOK,TS val 62946 ecr 64177,nop,wscale 7], length 0 E..n....@.q....J...G.....Z...........B ....B .....E.. host02.4789: VXLAN, flags [I] (0x08), vni 4096 IP 10.0.0.2.39040 > 10.0.0.5.http: Flags [.], ack 1, win 342, options [nop,nop,TS val 64178 ecr 62946], length 0 E..f....@.^M...G...J.....R...........B ....B .....E..4h2@.@... ... ......PIK.s.$Zw...Vh...... ........ 06:31:12.098656 IP 172.17.0.71.35083 > host02.4789: VXLAN, flags [I] (0x08), vni 4096 IP 10.0.0.2.39040 > 10.0.0.5.http: Flags [P.], seq 1:71, ack 1, win 342, options [nop,nop,TS val 64178 ecr 62946], length 70: HTTP: GET / HTTP/1.1 E.......@.^....G...J.................B ....B .....E..zh3@.@..D ... ......PIK.s.$Zw...VD7..... ........GET / HTTP/1.1 Host: host01 User-Agent: curl/7.47.0 Accept: */* 06:31:12.098912 IP host02.38034 > 172.17.0.71.4789: VXLAN, flags [I] (0x08), vni 4096 IP 10.0.0.5.http > 10.0.0.2.39040: Flags [.], ack 71, win 219, options [nop,nop,TS val 62947 ecr 64178], length 0 E..f....@.q....J...G.....R...........B ....B .....E..4Jb@.@..[ ... ....P...$ZwIK......h...... ........ 当查看TCPDump流时，可以识别底层的Redis API调用来设置和获取数据。例如 RESP \"hgetall\" \"ip\" RESP \"::ffff:10.255.0.3\" \"8\" 如果这是敏感信息，那么如果攻击者能够拦截网络流量，就可能会带来潜在的安全风险。 使用此命令删除服务和网络。下一步，我们将使用一个安全网络重新部署它们。 docker service rm redis app1-web && docker network rm app1-network 4. 创建加密覆盖网络 附加--opt encrypted选项使数据包在通过覆盖网络发送之前被加密，加密选项是在创建网络时定义的。 docker network create -d overlay --opt encrypted app1-network 加密对应用程序是透明的，允许它们以标准方式使用网络 使用下面的命令部署Redis服务和Web UI docker service create --name redis --network app1-network redis:alpine docker service create \\ --network app1-network -p 80:3000 \\ --replicas 1 --name app1-web \\ katacoda/redis-node-docker-example 现在，当流量生成时，你将无法拦截和监控进出Redis的流量，因为它现在是加密的。 $ curl host01 This page was generated after talking to redis. Application Build: 1 Total requests: 1 IP count: ::ffff:10.0.0.2: 1 $ curl host01 This page was generated after talking to redis. Application Build: 1 Total requests: 2 然而，您仍然会看到返回给客户端的HTTP web响应。这是因为应用程序不使用HTTPS在客户端和服务器之间进行通信。添加HTTPS将为应用程序创建一个完整的端到端加密解决方案。 参考： Use overlay networks Docker Networking Security Basics Docker swarm: overlay network encryption and MTLS Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_5_secrets.html":{"url":"Docker-Swarm/docker_swarm_5_secrets.html","title":"docker swarm 管理 secrets","keywords":"","body":"Docker swarm 管理 secrets1. 初始化 swarm2. 创建 secrets3. 用Compose创建Docker stack4. 部署访问 secrets5. File Based Secret6. 使用Compose部署和访问secretsDocker swarm 管理 secrets @[toc] 1. 初始化 swarm 默认情况下，Docker作为一个隔离的单节点工作。所有容器仅部署在引擎上。群模式将它变成了一个多主机集群感知引擎。为了使用秘密功能，Docker必须处于“群模式”。这是通过 $ docker swarm init Swarm initialized: current node (o6ngy0xskvvhxaaiyfye21znh) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-4oa8jjlavmoihusp73vgu71mhjek6ut1qkapzqnhtxdq5xzv0t-04anuyasgyv0p4xiqn4ga16fe 172.17.0.9:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 2. 创建 secrets 下面的命令将首先创建一个随机的64个字符的令牌，该令牌将存储在一个文件中以供测试之用。令牌文件用于创建名为deep_thought_answer_secure的秘密文件 $ tokenfile $ docker secret create deep_thought_answer_secure tokenfile 5yk3llezwli4atuua81dw6hg5 例如，还可以使用stdin创建秘密 $ echo \"the_answer_is_42\" | docker secret create lesssecure - sxzk4itvh9dwvcenfz037uwab 注意，这种方法将在用户bash历史文件中保留the_answer_is_42的值。 所有的秘密名称都可以使用 $ docker secret ls ID NAME DRIVER CREATED UPDATED 5yk3llezwli4atuua81dw6hg5 deep_thought_answer_secure About a minute ago About a minute ago sxzk4itvh9dwvcenfz037uwab lesssecure 53 seconds ago 53 seconds ago 这将不会暴露底层的secrets的values，这个秘密可以在通过Swarm部署服务时使用。例如，deploy让Redis服务可以访问这个秘密。 $ docker service create --name=\"redis\" --secret=\"deep_thought_answer_secure\" redis llfxs9rk9e88n7jh99q971uwb overall progress: 1 out of 1 tasks 1/1: running [==================================================>] verify: Service converged secret作为一个文件出现在secrets目录中。 $ docker exec $(docker ps --filter name=redis -q) ls -l /run/secrets ls: cannot access '/run/secrets': Operation not permitted 这可以作为一个普通文件从磁盘读取。 $ docker exec $(docker ps --filter name=redis -q) cat /run/secrets/deep_thought_answer_secure SbrptUbQhcF7oWdfhmlSn70XCDvCNH2REuYSRv55tgUPEjPjKvB1zeLDTZTTcAxf$ 3. 用Compose创建Docker stack 使用Docker Compose Stacks也可以使用secrets功能。在下面的例子中，观众服务可以访问我们的Swarm Secret _deep_thoughtanswer。它被安装并被称为deep_thoughtanswer version: '3.1' services: viewer: image: 'alpine' command: 'cat /run/secrets/deep_thought_answer_secure' secrets: - deep_thought_answer_secure secrets: deep_thought_answer_secure: external: true 4. 部署访问 secrets Docker Compose Stack的部署使用Docker CLI。作为部署的一部分，堆栈将配置为对秘密的访问。使用以下命令部署任务: docker stack deploy -c docker-compose.yml secrets1 docker logs $(docker ps -aqn1 -f status=exited) 如果命令错误与“docker日志”需要精确的1个参数。这意味着容器还没有启动并返回秘密。 5. File Based Secret 另一种创建秘密的方法是通过文件。既然如此，我们有个秘密。需要从容器中访问的CRT文件。 echo \"my-super-secure-cert\" > secret.crt 更新docker-compose Stack以使用基于机密的文件 version: '3.1' services: test: image: 'alpine' command: 'cat /run/secrets/secretcert' secrets: - secretcert secrets: secretcert: file: ./secret.crt 6. 使用Compose部署和访问secrets 和前面一样，部署Docker Compose stack docker stack deploy -c docker-compose.yml secrets2 下面的命令将获取为新创建的服务退出的最后一个容器的日志文件 docker logs $(docker ps -aqn1 -f name=secrets2 -f status=exited) 参考： Manage sensitive data with Docker secrets The Complete Guide to Docker Secrets Managing Secrets in Docker Swarm Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_6_healthcheck.html":{"url":"Docker-Swarm/docker_swarm_6_healthcheck.html","title":"docker swarm 健康检查","keywords":"","body":"Docker Swarm 健康检查1. 容器添加健康检查2. 崩溃命令3. 验证状态4. 状态修复5. swarm 运用 HealthchecksDocker Swarm 健康检查 1. 容器添加健康检查 新的Healthcheck功能是作为Dockerfile的扩展创建的，并在构建Docker映像时定义。 下面的Dockerfile扩展了现有的HTTP服务并添加了健康检查。 healthcheck将每秒钟对HTTP服务器进行curl操作以确保其正常运行。如果服务器以非200请求响应，curl将失败，并返回退出码1。在三次失败后，Docker将标记容器为不健康。 说明格式为HEALTHCHECK [OPTIONS] CMD命令。 $ vim Dockerfile FROM katacoda/docker-http-server:health HEALTHCHECK --timeout=1s --interval=1s --retries=3 \\ CMD curl -s --fail http://localhost:80/ || exit 1 目前，Healthcheck支持三种不同的选项: interval=DURATION (default: 30s).这是执行健康检查之间的时间间隔。 timeout=DURATION (default: 30s) 如果检查在超时前没有完成，则认为它失败了。 retries=N (default: 3) 在将容器标记为不健康之前需要重新检查多少次。 执行的命令必须作为容器部署的一部分安装。在幕后，Docker将使用Docker exec来执行该命令。 在继续之前，构建并运行HTTP服务。 $ docker build -t http . Sending build context to Docker daemon 2.048kB Step 1/2 : FROM katacoda/docker-http-server:health health: Pulling from katacoda/docker-http-server 12b41071e6ce: Pull complete fb1cef6edba2: Pull complete 1061ea2815dd: Pull complete Digest: sha256:fee2132b14b4148ded82aacd8f06bdcb9efa535b4dfd2f1d88518996f4b2fb1d Status: Downloaded newer image for katacoda/docker-http-server:health ---> 7f16ea0c8bd8 Step 2/2 : HEALTHCHECK --timeout=1s --interval=1s --retries=3 CMD curl -s --fail http://localhost:80/ || exit 1 ---> Running in fcfcf750b855 Removing intermediate container fcfcf750b855 ---> 333a8ac14b75 Successfully built 333a8ac14b75 Successfully tagged http:latest $ docker run -d -p 80:80 --name srv http 6702ab36060fc2efb112081ab0ebf96f8a14f1139417fac7dc0dad04e6d33b86 在接下来的步骤中，我们将导致HTTP服务器开始抛出错误。当HTTP服务器作为容器运行时，Docker守护进程将根据选项自动检查健康检查。例如，当您列出所有正在运行的容器时，它将返回状态. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6702ab36060f http \"/app\" About a minute ago Up About a minute (healthy) 0.0.0.0:80->80/tcp srv HTTP服务器有一个特殊的端点，它将导致它开始报告错误。 2. 崩溃命令 $ curl http://docker/unhealthy 服务现在将进入错误模式。在下一步中，我们将看看Docker是如何处理这个问题的。 由于HTTP服务器处于错误状态，健康检查应该失败。Docker会将此报告为元数据的一部分。 3. 验证状态 Docker会报告不同地方的健康状况。要获取原始文本流(在自动化过程中非常有用)，请使用Docker Inspect提取健康状态字段。 $ docker inspect --format \"{{json .State.Health.Status }}\" srv \"unhealthy\" 运行状况状态存储所有失败和命令的任何输出的日志。这对于调试为什么认为容器不健康非常有用。 $ docker inspect --format \"{{json .State.Health }}\" srv {\"Status\":\"unhealthy\",\"FailingStreak\":75,\"Log\":[{\"Start\":\"2021-10-11T02:19:07.166147032Z\",\"End\":\"2021-10-11T02:19:07.233788055Z\",\"ExitCode\":1,\"Output\":\"\"},{\"Start\":\"2021-10-11T02:19:08.253738183Z\",\"End\":\"2021-10-11T02:19:08.333500914Z\",\"ExitCode\":1,\"Output\":\"\"},{\"Start\":\"2021-10-11T02:19:09.354272859Z\",\"End\":\"2021-10-11T02:19:09.436257557Z\",\"ExitCode\":1,\"Output\":\"\"},{\"Start\":\"2021-10-11T02:19:10.508515556Z\",\"End\":\"2021-10-11T02:19:10.571064095Z\",\"ExitCode\":1,\"Output\":\"\"},{\"Start\":\"2021-10-11T02:19:11.590597522Z\",\"End\":\"2021-10-11T02:19:11.663932528Z\",\"ExitCode\":1,\"Output\":\"\"}]} $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6702ab36060f http \"/app\" 3 minutes ago Up 3 minutes (unhealthy) 0.0.0.0:80->80/tcp srv 4. 状态修复 $ curl http://docker/healthy $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6702ab36060f http \"/app\" 5 minutes ago Up 5 minutes (healthy) 0.0.0.0:80->80/tcp srv $ docker inspect --format \"{{json .State.Health.Status }}\" srv \"healthy\" 5. swarm 运用 Healthchecks Docker Swarm可以使用这些运行状况检查来了解何时需要重新启动/重新创建服务。 初始化一个Swarm集群，并将新创建的映像部署为具有两个副本的服务。 docker rm -f $(docker ps -qa); docker swarm init docker service create --name http --replicas 2 -p 80:80 http 您应该看到两个容器在响应 $ curl host01 A healthy request was processed by host: f3da8c49a948 随机导致其中一个节点不正常, $ curl host01/unhealthy 您应该只看到一个节点处理请求，因为Swarm已经自动从负载均衡器中删除了它 $ curl host01 A healthy request was processed by host: 01c394673bc0 $ curl host01 A healthy request was processed by host: 01c394673bc0 $ curl host01 A healthy request was processed by host: 01c394673bc0 群集现在将自动重启不健康的服务 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c2ba9356b47e http:latest \"/app\" 36 seconds ago Up 30 seconds (healthy) 80/tcp http.2.kg7bky9gy65r77ym7dkoxke84 01c394673bc0 http:latest \"/app\" About a minute ago Up About a minute (healthy) 80/tcp http.1.jit13g50cohbsv3qa1ht3o9h1 在Swarm重启服务后，你应该再次看到两个节点 $ curl host01 A healthy request was processed by host: c2ba9356b47e $ curl host01 A healthy request was processed by host: 01c394673bc0 参考： Docker Swarm Health Check Healthchecks in a Docker Swarm How To Successfully Implement A Healthcheck In Docker Compose Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_7_update.html":{"url":"Docker-Swarm/docker_swarm_7_update.html","title":"dcoker swarm 更新","keywords":"","body":"Dcoker Swarm 更新1. Update Limits2. Update Replicas3. Update Image4. 滚动更新Dcoker Swarm 更新 服务可以动态更新，以控制各种设置和选项。在内部，Docker管理如何应用更新。对于某些命令，Docker将停止、删除和重新创建容器。对于管理连接和正常运行时间来说，让所有容器一次性停止是一个重要的考虑因素。 有各种各样的设置你可以控制，通过查看帮助 $ docker service update --help 要启动，需要部署HTTP服务。我们将使用它来更新/修改容器设置。 $ docker swarm init && docker service create --name http --replicas 2 -p 80:80 katacoda/docker-http-server:v1 Swarm initialized: current node (vemt8upto85rpq2c0iv26hfjb) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-3ql2gc1k1clslqhyce6ww09fshgvslde4fpkt5wabgc6gewkid-96vytrgsrua2qrgfujq8bdo08 172.17.0.40:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 2838c9f2d95l8qxzdin1zn7pa overall progress: 2 out of 2 tasks 1/2: running [==================================================>] 2/2: running [==================================================>] verify: Service converged 1. Update Limits 一旦启动，就可以更新各种属性。例如，向容器添加一个新的环境变量。 $ docker service update --env-add KEY=VALUE http http overall progress: 0 out of 2 tasks overall progress: 2 out of 2 tasks 1/2: running [==================================================>] 2/2: running [==================================================>] verify: Service converged 或者，更新CPU和内存限制。 $ docker service update --limit-cpu 2 --limit-memory 512mb http http overall progress: 2 out of 2 tasks 1/2: running [==================================================>] 2/2: running [==================================================>] verify: Service converged 一旦执行，当您检查服务时，结果将是可见的. $ docker service inspect --pretty http ID: 2838c9f2d95l8qxzdin1zn7pa Name: http Service Mode: Replicated Replicas: 2 UpdateStatus: State: completed Started: 38 seconds ago Completed: 23 seconds ago Message: update completed Placement: UpdateConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: katacoda/docker-http-server:v1@sha256:4d7bfcb1e38912d286c5cda63aeddc850a4be16127094ffacbb7abfc6298c5fa Env: KEY=VALUE Resources: Limits: CPU: 2 Memory: 512MiB Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress 但是，在列出所有容器时，您将看到它们在每次更新时都被重新创建. $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26e2064259ff katacoda/docker-http-server:v1 \"/app\" About a minute ago Up About a minute 80/tcp http.1.vsv080fuwlsgcqo16m0d3czz9 74c412f02426 katacoda/docker-http-server:v1 \"/app\" About a minute ago Up About a minute 80/tcp http.2.z6sp9scii6kj4ponoz0wrmv05 deda0102a7e2 katacoda/docker-http-server:v1 \"/app\" 2 minutes ago Exited (2) About a minute ago http.2.ugilx2q6zkkkw0pwo2f6ydrmi 9b9d3addcc2f katacoda/docker-http-server:v1 \"/app\" 2 minutes ago Exited (2) About a minute ago http.1.lvhnd9zywl7ioax9t5wi3y9c8 9806903d549f katacoda/docker-http-server:v1 \"/app\" 2 minutes ago Exited (2) 2 minutes ago http.1.9m8l4jz9cxicel0ff8w3sq8k7 cdcd87d9487d katacoda/docker-http-server:v1 \"/app\" 2 minutes ago Exited (2) 2 minutes ago http.2.ahrvwdkcd9kkkxp868ppx14v5 2. Update Replicas 并非所有更新都需要重新创建每个容器。例如，扩展副本的数量不会影响现有容器。docker service scale作为一种替代docker服务规模的方法，可以使用更新来定义应该运行多少个副本。下面将把副本从2更新为6。然后Docker将重新安排要部署的另外四个容器。 $ docker service update --replicas=6 http http overall progress: 6 out of 6 tasks 1/6: running [==================================================>] 2/6: running [==================================================>] 3/6: running [==================================================>] 4/6: running [==================================================>] 5/6: running [==================================================>] 6/6: running [==================================================>] verify: Service converged 副本的数量在检查服务时是可见的 $ docker service inspect --pretty http ID: 2838c9f2d95l8qxzdin1zn7pa Name: http Service Mode: Replicated Replicas: 6 3. Update Image 使用更新的最常见场景是通过更新的Docker Image发布应用程序的新版本。由于Docker Image是容器的属性，所以可以像前面的步骤一样对其进行更新。 下面的命令将使用Docker镜像的v2标记重新创建HTTP服务的实例。 $ docker service update --image katacoda/docker-http-server:v2 http http overall progress: 6 out of 6 tasks 1/6: running [==================================================>] 2/6: running [==================================================>] 3/6: running [==================================================>] 4/6: running [==================================================>] 5/6: running [==================================================>] 6/6: running [==================================================>] verify: Service converged 如果你打开一个新的终端窗口，你会注意到Swarm正在执行滚动更新。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 35d342b07967 katacoda/docker-http-server:v2 \"/app\" 49 seconds ago Up 44 seconds 80/tcp http.5.bjdoq1c85l2q2hdv1xkx0q8tb 18da1f1606ef katacoda/docker-http-server:v2 \"/app\" 54 seconds ago Up 49 seconds 80/tcp http.2.gx464snpuedj2dm0iq1rzgad6 725fdaafe45b katacoda/docker-http-server:v2 \"/app\" About a minute ago Up 54 seconds 80/tcp http.1.4n0ub4wo24zi8v7ptigbahtsz 5987cf3c84e8 katacoda/docker-http-server:v2 \"/app\" About a minute ago Up About a minute 80/tcp http.3.rm9ehbhf8gaay83dkb44frd3o 7c68ffd7eb76 katacoda/docker-http-server:v2 \"/app\" About a minute ago Up About a minute 80/tcp http.6.u9yzhw2reneuipj5na0vkwp8r 2c1b365d7601 katacoda/docker-http-server:v2 \"/app\" About a minute ago Up About a minute 80/tcp http.4.i8r9ulpxfcldjorqucfryjtgl 通过使用多个副本进行滚动更新，应用程序永远不会宕机，并且可以执行零停机部署。 $ curl http://docker New Release! Now v2! This request was processed by host: 35d342b07967 $ curl http://docker New Release! Now v2! This request was processed by host: 18da1f1606ef $ curl http://docker New Release! Now v2! This request was processed by host: 725fdaafe45b $ curl http://docker New Release! Now v2! This request was processed by host: 5987cf3c84e8 $ curl http://docker New Release! Now v2! This request was processed by host: 7c68ffd7eb76 $ curl http://docker New Release! Now v2! This request was processed by host: 2c1b365d7601 $ curl http://docker New Release! Now v2! This request was processed by host: 35d342b07967 $ 下一步将讨论如何控制推出和零停机部署。 4. 滚动更新 其目的是部署一个新的Docker镜像而不引起任何停机时间。通过设置并行性和延迟，可以实现零停机时间。Docker可以批处理更新，并将其作为跨集群的rollout执行。 update-parallelism：定义了Docker一次应该更新多少个容器。副本的数量取决于您将对请求进行批量处理的大小。 update-delay：定义在每个更新批之间等待多长时间。如果应用程序有预热时间(例如启动JVM或CLR)，那么延迟是有用的。通过指定延迟，可以确保在流程启动时仍然可以处理请求。 这两个参数在运行docker服务更新时应用。在这个例子中，它一次更新一个容器，在每次更新之间等待10秒。该更新将影响所使用的Docker映像，但参数可以应用于任何可能的更新值 $ docker service update --update-delay=10s --update-parallelism=1 --image katacoda/docker-http-server:v3 http 启动之后，您会慢慢地看到容器的新v3版本启动并替换现有的v2版本。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES bf72245972b2 katacoda/docker-http-server:v3 \"/app\" 8 seconds ago Up 2 seconds 80/tcp http.5.nk6451tjyjof7bmhrbz8mwa8t db63c2989df4 katacoda/docker-http-server:v3 \"/app\" 24 seconds ago Up 18 seconds 80/tcp http.2.ymml1p2qcyndx7wdpzt4fgnmd b5e45490f79b katacoda/docker-http-server:v3 \"/app\" 39 seconds ago Up 34 seconds 80/tcp http.1.l6la8w8cqu87ndenxico2fx4q 5987cf3c84e8 katacoda/docker-http-server:v2 \"/app\" 7 minutes ago Up 6 minutes 80/tcp http.3.rm9ehbhf8gaay83dkb44frd3o 7c68ffd7eb76 katacoda/docker-http-server:v2 \"/app\" 7 minutes ago Up 7 minutes 80/tcp http.6.u9yzhw2reneuipj5na0vkwp8r 2c1b365d7601 katacoda/docker-http-server:v2 \"/app\" 7 minutes ago Up 7 minutes 80/tcp http.4.i8r9ulpxfcldjorqucfryjtgl 向负载均衡器发出HTTP请求将请求v2和v3容器处理它们，从而产生不同的输出。 $ curl http://docker New Release! Now v2! This request was processed by host: 5987cf3c84e8 $ curl http://docker New Another Release! Now v3! This request was processed by host: 8349dd465a7e $ curl http://docker New Another Release! Now v3! This request was processed by host: 09860150d9b1 $ curl http://docker New Another Release! Now v3! This request was processed by host: bf72245972b2 $ curl http://docker New Another Release! Now v3! This request was processed by host: db63c2989df4 $ curl http://docker New Another Release! Now v3! This request was processed by host: b5e45490f79b 应用程序必须考虑到这一点，并同时处理两个不同的版本。 参考： docker swarm update Updating Services in a Docker Swarm Mode Cluster Rolling updates with Docker Swarm Updating Docker Swarm Configs and Secrets Without Downtime Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_8_UI_Portainer.html":{"url":"Docker-Swarm/docker_swarm_8_UI_Portainer.html","title":"docker swarm 部署界面 ui portainer","keywords":"","body":"Docker Swarm 部署界面 UI Portainer1. 创建集群2. Deploy Portainer3. 访问 Portainer Dashboard4. 部署模板5. 管理容器Docker Swarm 部署界面 UI Portainer 1. 创建集群 第一台机器： docker swarm init 第二台机器： token=$(ssh -o StrictHostKeyChecking=no 172.17.0.46 \"docker swarm join-token -q worker\") && echo $token docker swarm join 172.17.0.46:2377 --token $token 第一台机器： docker node ls 2. Deploy Portainer 配置了集群后，下一阶段是部署Portainer。Portainer作为运行在Docker集群或Docker主机上的容器部署。 要完成这个场景，需要将Portainer部署为Docker服务。通过部署Docker服务，Swarm将确保该服务始终在管理器上运行，即使主机宕机。 该服务对外公开9000端口，并将内部Portainer数据保存在“/host/data”目录下。当Portainer启动时，它使用docker进行连接。sock文件到Docker Swarm manager。 还有一个附加的约束，即容器只能在管理器节点上运行 第一台执行： docker service create \\ --name portainer \\ --publish 9000:9000 \\ --constraint 'node.role == manager' \\ --mount type=bind,src=/host/data,dst=/data \\ --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\ portainer/portainer \\ -H unix:///var/run/docker.sock 另一种运行Portainer的方法是直接在主机上运行。在本例中，该命令在端口9000上暴露Portainer仪表板，将数据持久化到主机，并通过Docker连接到它正在运行的Docker主机。袜子文件。 docker run -d -p 9000:9000 --name=portainer \\ -v \"/var/run/docker.sock:/var/run/docker.sock\" \\ -v /host/data:/data \\ portainer/portainer 3. 访问 Portainer Dashboard 随着Portainer的运行，现在可以通过UI访问仪表板并管理集群。仪表板运行在Port 9000上，可以通过这个链接访问 第一个屏幕要求您为admin用户创建一个密码 配置完成后，第二个屏幕将要求您使用定义的密码登录到仪表板。 4. 部署模板 Portainer的众多特性之一是，它可以基于预定义的容器部署服务。 在这种情况下，您将部署nginx模板。 通过“应用模板”页签查看可用的模板。 选择nginx模板 例如，为容器输入一个友好的名称nginx-web 勾选“显示高级选项”，将80端口绑定到主机端口80 创建容器对象 访问容器通过80端口 5. 管理容器 将部署一个Nginx实例。使用指示板，您将看到状态并能够控制集群。 参考： Swarmpit web user interface for your Docker Swarm cluster Docker UI SWIRL Docker Portainer Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_9_docker-compose_deploy_app.html":{"url":"Docker-Swarm/docker_swarm_9_docker-compose_deploy_app.html","title":"docker swarm 通过 docker compose 部署应用","keywords":"","body":"Docker swarm 通过 docker-compose 部署应用1 初始化swarm2. 创建 Docker Compose 文件3. 部署服务Docker swarm 通过 docker-compose 部署应用 1 初始化swarm 默认情况下，Docker作为一个隔离的单节点工作。所有容器仅部署在引擎上。群模式将它变成了一个多主机集群感知引擎。 为了使用秘密功能，Docker必须处于“群模式”。这是通过 $ docker swarm init Swarm initialized: current node (ean4r3wx8dutbj2hlkp4lsfu0) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-2t5z3yzsmq4xlhilsuh04ltiprnqt1h1cv8gmmaq6eip3day99-advgpojck21b1hlh3v6vgpncq 172.17.0.86:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 在第二台主机上执行下面的命令，将它作为一个worker添加到集群中。 $ token=$(ssh -o StrictHostKeyChecking=no 172.17.0.86 \"docker swarm join-token -q worker\") && docker swarm join 172.17.0.86:2377 --token $token Warning: Permanently added '172.17.0.86' (ECDSA) to the list of known hosts. This node joined a swarm as a worker. 2. 创建 Docker Compose 文件 使用Docker Compose v3，可以定义一个Docker部署以及生产细节。这为管理可以部署到集群模式集群中的应用程序部署提供了一个中央位置。 一个Docker Compose文件已经创建，它定义了使用web前端部署Redis服务器。使用以下命令查看文件. $ cat docker-compose.yml version: \"3\" services: redis: image: redis:alpine volumes: - db-data:/data networks: appnet1: aliases: - db deploy: placement: constraints: [node.role == manager] web: image: katacoda/redis-node-docker-example networks: - appnet1 depends_on: - redis deploy: mode: replicated replicas: 2 labels: [APP=WEB] resources: limits: cpus: '0.25' memory: 512M reservations: cpus: '0.25' memory: 256M restart_policy: condition: on-failure delay: 5s max_attempts: 3 window: 120s update_config: parallelism: 1 delay: 10s failure_action: continue monitor: 60s max_failure_ratio: 0.3 placement: constraints: [node.role == worker] networks: appnet1: 该文件已扩展到利用群集部署选项。 第一个配置选项使用depends_on。这意味着Redis必须在网络之前部署，并允许我们控制服务启动的顺序。 下一个配置选项定义应该如何使用新的部署选项部署应用程序。 首先，mode: replicated和replicas: 2决定服务应该启动多少个replicas。 其次，定义资源。限制是应用程序不能超过的硬限制，预留是Docker Swarm指示应用程序需要的资源的指南。 第三，restart_policy指出进程崩溃时应该如何操作。 第四，update_config定义如何应用和推出更新。 最后，位置允许我们添加约束，以确定服务应该部署在哪里。 更多docker-compose file文件配置细节请参考 3. 部署服务 Docker Compose文件被称为Docker Compose Stack。堆叠可以通过CLI部署到Swarm。 docker stack命令用于通过Swarm部署docker Compose stack。在本例中，它将以myapp作为服务的前缀。 $ docker stack deploy --compose-file docker-compose.yml myapp Creating network myapp_appnet1 一旦部署完毕，就可以使用CLI检查状态。 $ docker stack ls NAME SERVICES ORCHESTRATOR myapp 2 Swarm 可以通过以下方式发现内部服务的详细信息 $ docker stack services myapp ID NAME MODE REPLICAS IMAGE PORTS l8c8oztncboc myapp_web replicated 2/2 katacoda/redis-node-docker-example:latest 注意，该命令指示服务的Desired / Running状态。如果不能部署服务，那么这将是不同的。 每个服务容器的详细信息可以使用 $ docker stack ps myapp ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS x90dytk7svj7 myapp_web.1 katacoda/redis-node-docker-example:latest host02 Running Running 3 minutes ago 92a3t08318ne myapp_redis.1 redis:alpine host01 Running Running 3 minutes ago ikmrz4pufzyd myapp_web.2 katacoda/redis-node-docker-example:latest host02 Running Running 3 minutes ago 所有这些信息仍然可以被发现使用 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 21bbc04a9e8d redis:alpine \"docker-entrypoint.s…\" 3 minutes ago Up 3 minutes 6379/tcp myapp_redis.1.92a3t08318neq5g5sd1akrwzo Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker-Swarm/docker_swarm_10_maintenance_mode.html":{"url":"Docker-Swarm/docker_swarm_10_maintenance_mode.html","title":"docker swarm 维护模式","keywords":"","body":"Docker Swarm 维护模式1. 创建集群2. 部署服务3. 开启维护模式4. 关闭维护模式Docker Swarm 维护模式 1. 创建集群 第一台执行： $ docker swarm init Swarm initialized: current node (qgowushrnwa87hul908vicsph) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5kk5jzz8rnjh0wctnsdm19yuvnvayw6eik9k3udyg4pupkklbx-0opd5tey2uxdplbzibg9uw104 172.17.0.14:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 在第二台主机上执行下面的命令，将它作为一个worker添加到集群中。 token=$(ssh -o StrictHostKeyChecking=no 172.17.0.14 \"docker swarm join-token -q worker\") && docker swarm join 172.17.0.14:2377 --token $token 2. 部署服务 首先在两个集群模式节点上部署一个带有两个副本的HTTP服务器。部署将导致在每个节点上部署一个容器。 $ docker service create --name lbapp1 --replicas 2 -p 80:80 katacoda/docker-http-server $ docker service ls ID NAME MODE REPLICAS IMAGE PORTS ud5scjjdmsu2 lbapp1 replicated 2/2 katacoda/docker-http-server:latest *:80->80/tcp $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES d584747866f1 katacoda/docker-http-server:latest \"/app\" 45 3. 开启维护模式 当需要维护时，正确管理流程以确保可靠性是很重要的。第一个操作是从负载平衡器中删除节点，并让所有活动会话都完成。这将确保没有请求被发送到主机。其次，需要重新部署系统上的工作负载，以确保容量得到维护。 Docker Swarm将在设置节点的可用性时为你管理这一点,设置可用性需要知道集群模式的IP。 $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION qgowushrnwa87hul908vicsph * host01 Ready Active Leader 19.03.13 ksjozt8473y0vwpvsg2uxxyue host02 Ready Active 19.03.13 $ worker=$(docker node ls | grep -v \"Leader\" | awk '{print $1}' | tail -n1); echo $worker ksjozt8473y0vwpvsg2uxxyue 通过更新节点来设置可用性 $ docker node update $worker --availability=drain ksjozt8473y0vwpvsg2uxxyue 容器现在应该都运行在单个管理器节点上。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 342a28429e8c katacoda/docker-http-server:latest \"/app\" 33 seconds ago Up 27 seconds 80/tcp lbapp1.2.c1rpw4q1lnohsdi12eclvzn9f d584747866f1 katacoda/docker-http-server:latest \"/app\" 7 minutes ago Up 7 minutes 80/tcp lbapp1.1.qevzgg0osytfsp0l6jfqryrq1 当查看所有节点时，可用性将发生变化 $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION qgowushrnwa87hul908vicsph * host01 Ready Active Leader 19.03.13 ksjozt8473y0vwpvsg2uxxyue host02 Ready Drain 19.03.13 4. 关闭维护模式 一旦完成了工作，节点应该可以用于未来的工作负载。这是通过设置可用性为活动。 $ docker node update $worker --availability=active ksjozt8473y0vwpvsg2uxxyue 现在可用性又变回来了 $ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION qgowushrnwa87hul908vicsph * host01 Ready Active Leader 19.03.13 ksjozt8473y0vwpvsg2uxxyue host02 Ready Active 19.03.13 值得注意的是，Docker不会重新安排现有的工作负载。查看这些容器，您将看到它们仍然运行在单个主机上。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 342a28429e8c katacoda/docker-http-server:latest \"/app\" 2 minutes ago Up 2 minutes 80/tcp lbapp1.2.c1rpw4q1lnohsdi12eclvzn9f d584747866f1 katacoda/docker-http-server:latest \"/app\" 10 minutes ago Up 9 minutes 80/tcp lbapp1.1.qevzgg0osytfsp0l6jfqryrq1 相反，Swarm只会将新的工作负载安排到新可用的主机上。这可以通过扩展所需的副本数量进行测试。 $ docker service scale lbapp1=3 lbapp1 scaled to 3 overall progress: 3 out of 3 tasks 1/3: running [==================================================>] 2/3: running [==================================================>] 3/3: running [==================================================>] verify: Service converged 新容器将被调度到第二个节点。 参考： Drain a node on the swarm Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/":{"url":"Docker/","title":"Docker","keywords":"","body":"Docker Overview1. Docker 简介2. Docker 起源3. Docker 开源4. Docker 平台5. Docker 特性6. Docker 架构7. Docker 守护进程8. Docker 客户端9. Docker 桌面10. Docker 仓库11. Docker 镜像12. Docker 容器13. Docker 底层技术Docker Overview 1. Docker 简介 Docker 是一个用于开发、发布和运行应用程序的开放平台。Docker 使您能够将应用程序与基础架构分离，以便您可以快速交付软件。使用 Docker，您可以像管理应用程序一样管理基础设施。通过利用 Docker 快速交付、测试和部署代码的方法，您可以显着减少编写代码和在生产环境中运行之间的延迟。 2. Docker 起源 Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。 3. Docker 开源 Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目 已经超过 6 万 3 千个星标和一万多个 fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。 4. Docker 平台 Docker 提供了在称为容器的松散隔离环境中打包和运行应用程序的能力。隔离和安全性允许您在给定主机上同时运行多个容器。容器是轻量级的，包含运行应用程序所需的一切，因此您无需依赖主机上当前安装的内容。您可以在工作时轻松共享容器，并确保与您共享的每个人都获得以相同方式工作的同一个容器。 Docker 提供工具和平台来管理容器的生命周期： 使用容器开发您的应用程序及其支持组件。 容器成为分发和测试应用程序的单元。 准备就绪后，将应用程序部署到生产环境中，作为容器或编排的服务。无论您的生产环境是本地数据中心、云提供商还是两者的混合，这都是一样的。 5. Docker 特性 快速、一致地交付您的应用程序 Docker 通过允许开发人员使用提供应用程序和服务的本地容器在标准化环境中工作来简化开发生命周期。容器非常适合持续集成和持续交付 (CI/CD) 工作流程。 考虑以下示例场景： 您的开发人员在本地编写代码并使用 Docker 容器与同事分享他们的工作。 他们使用 Docker 将他们的应用程序推送到测试环境中并执行自动化和手动测试。 当开发者发现bug时，可以在开发环境中修复，重新部署到测试环境中进行测试和验证。 测试完成后，将修复程序提供给客户就像将更新的镜像推送到生产环境一样简单。 响应式部署和扩展 Docker 基于容器的平台允许高度可移植的工作负载。Docker 容器可以在开发人员的本地笔记本电脑、数据中心的物理或虚拟机、云提供商或混合环境中运行。 Docker 的可移植性和轻量级特性还使得动态管理工作负载、根据业务需求近乎实时地扩展或拆除应用程序和服务变得容易。 在相同硬件上运行更多工作负载 Docker 是轻量级和快速的。它为基于管理程序的虚拟机提供了一种可行且经济高效的替代方案，因此您可以使用更多计算容量来实现业务目标。Docker 非常适合高密度环境以及需要用更少资源完成更多工作的中小型部署。 6. Docker 架构 Docker 使用客户端-服务器架构。Docker客户端与 Docker守护进程对话，后者负责构建、运行和分发 Docker 容器的繁重工作。Docker 客户端和守护程序可以 在同一系统上运行，或者您可以将 Docker 客户端连接到远程 Docker 守护程序。Docker 客户端和守护程序使用 REST API，通过 UNIX 套接字或网络接口进行通信。另一个 Docker 客户端是 Docker Compose，它允许您使用由一组容器组成的应用程序。 7. Docker 守护进程 Docker 守护程序 ( dockerd) 侦听 Docker API 请求并管理 Docker 对象，例如image、container、network和volume。守护进程还可以与其他守护进程通信以管理 Docker 服务。 8. Docker 客户端 Docker 客户端 ( docker client) 是许多 Docker 用户与 Docker 交互的主要方式。当您使用诸如docker run之类的命令时，客户端会将这些命令发送到dockerd执行它们。该docker命令使用 Docker API。Docker 客户端可以与多个守护进程通信。 9. Docker 桌面 Docker Desktop 是一个易于安装的应用程序，适用于您的 Mac 或 Windows 环境，使您能够构建和共享容器化应用程序和微服务。Docker Desktop 包括 Docker 守护程序 ( dockerd)、Docker 客户端 ( docker)、Docker Compose、Docker Content Trust、Kubernetes 和 Credential Helper。有关更多信息，请参阅Docker 桌面。 10. Docker 仓库 Docker仓库存储 Docker 镜像。Docker Hub 是一个任何人都可以使用的公共仓库，并且 Docker 默认配置为在 Docker Hub 上查找镜像。您甚至可以运行自己的私有仓库。 当您使用docker pull or docker run命令时，将从您配置的仓库中提取所需的镜像。当您使用该docker push命令时，您的镜像会被推送到您配置的仓库中。 11. Docker 镜像 镜像是一个只读模板，其中包含创建 Docker 容器的说明。通常，一个镜像基于另一个镜像，并带有一些额外的自定义。例如，您可以基于该镜像构建一个镜像ubuntu ，但安装 Apache Web 服务器和您的应用程序，以及使您的应用程序运行所需的配置详细信息。 您可以创建自己的镜像，也可以只使用其他人创建并在仓库中发布的镜像。要构建您自己的镜像，您需要使用简单的语法创建一个Dockerfile ，用于定义创建和运行镜像所需的步骤。Dockerfile 中的每条指令都会在镜像中创建一个层。当您更改 Dockerfile 并重建镜像时，仅重建那些已更改的层。与其他虚拟化技术相比，这是使镜像如此轻量、小巧和快速的部分原因。 12. Docker 容器 容器是图像的可运行实例。您可以使用 Docker API 或 CLI 创建、启动、停止、移动或删除容器。您可以将容器连接到一个或多个网络，将存储附加到它，甚至可以根据其当前状态创建新镜像。 默认情况下，一个容器与其他容器及其主机的隔离相对较好。您可以控制容器的网络、存储或其他底层子系统与其他容器或主机的隔离程度。 容器由其镜像以及您在创建或启动它时提供给它的任何配置选项定义。当容器被移除时，任何未存储在持久存储中的状态更改都会消失。 示例docker run命令 以下命令运行一个ubuntu容器，以交互方式附加到您的本地命令行会话，然后运行/bin/bash​​. docker run -i -t ubuntu /bin/bash 当您运行此命令时，会发生以下情况（假设您使用的是默认仓库配置）： 如果您在本地没有ubuntu镜像，Docker 会从您配置的仓库中提取它，就像您docker pull ubuntu手动运行一样。 Docker 会创建一个新容器，就像您docker container create 手动运行命令一样。 Docker 为容器分配一个读写文件系统，作为它的最后一层。这允许正在运行的容器在其本地文件系统中创建或修改文件和目录。 Docker 创建了一个网络接口来将容器连接到默认网络，因为您没有指定任何网络选项。这包括为容器分配 IP 地址。默认情况下，容器可以使用主机的网络连接连接到外部网络。 Docker 启动容器并执行/bin/bash. 因为容器以交互方式运行并附加到您的终端（由于-i and -t 标志），所以您可以在输出记录到终端时使用键盘提供输入。 当您键入exit终止/bin/bash命令时，容器会停止但不会被删除。您可以重新启动或删除它。 13. Docker 底层技术 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 版本开始，则进一步演进为使用 runC 和 containerd。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/CICD/":{"url":"Docker/CICD/","title":"CICD","keywords":"","body":"Docker CI/CD Overiview1. 前言2. 什么是持续集成？3. 什么是持续交付？4. 什么是持续部署？5. 什么是 Docker？6. Docker 如何在 CI/CD 中提供帮助？7. Docker Pipeline 阶段Docker CI/CD Overiview 1. 前言 根据2020 年 Jetbrains 开发人员调查，44% 的开发人员现在正在使用某种形式的 Docker 容器持续集成和部署。 Docker 已成为持续集成和持续部署的早期采用者。通过利用与 GIT 等源代码控制机制的正确集成，Jenkins 可以在开发人员每次提交代码时启动构建过程。此过程会生成一个新的 Docker 映像，该映像可在整个环境中立即可用。使用 Docker 镜像，团队可以快速构建、共享和部署他们的应用程序。 目的：根据开发/IT 需求，使用自助服务自动化工具自动配置环境/基础设施。 组织面临的挑战： 环境不可用 缺乏环境配置技能 环境配置的前置时间长 2. 什么是持续集成？ 这是一种开发实践，开发人员每天多次将代码集成到共享存储库中，支持将新功能与现有代码集成。此集成代码还确保运行时环境中没有错误，使我们能够检查它对其他更改的反应。 用于持续集成的最流行工具是“Jenkins”，而 GIT 用于源代码控制存储库。Jenkins 可以从 GIT 存储库中提取最新的代码修订版并生成可以部署到服务器的构建。 3. 什么是持续交付？ 持续交付是在任何给定时间将软件部署到任何环境的能力，包括二进制文件、配置和环境更改（如果有）。 4. 什么是持续部署？ 持续部署是开发团队在短周期内发布软件的一种方法。开发人员所做的任何更改都会一直部署到生产环境。 5. 什么是 Docker？ Docker 是一个容器化平台，它将应用程序及其所有依赖项以 Container 的形式打包在一起，以确保应用程序在任何环境中都能无缝运行。 6. Docker 如何在 CI/CD 中提供帮助？ Docker 帮助开发人员在任何环境中构建他们的代码并测试他们的代码，以便在应用程序开发生命周期的早期捕获错误。Docker 有助于简化流程，节省构建时间，并允许开发人员并行运行测试。 Docker 可以与 GitHub 等源代码控制管理工具和 Jenkins 等集成工具集成。开发人员将代码提交到 GitHub，使用 Jenkins 创建映像测试自动触发构建的代码。可以将此镜像添加到 Docker 注册表中，以处理不同环境类型之间的不一致。 7. Docker Pipeline 阶段 Build：此阶段的 Docker 容器将具有构建所需的所有工具，例如 SDK，它还可以缓存我们应用所需的依赖项。 Test： Docker 在集成和功能测试方面非常有帮助，我们可以在开始集成或功能测试之前创建应用程序所需的数据库、身份等服务的容器。这种方法还为我们提供了并行化测试的能力，因为在尝试创建应用程序的两个实例进行测试时不会出现端口冲突等问题。它非常具有成本效益并减少了环境管理所需的工作量，因为不需要保持相关服务始终运行。最后，我们还可以在数据库中加载测试数据，以便我们的集成/功能测试运行，这节省了大量时间，因为我们之前在测试设置中加载数据。 Deliver：在这个阶段，我们可以设置 docker 镜像以拥有打包和交付所需的工具。例如，我们可以在 docker 容器中使用 docker 来将我们的工件打包为 docker 镜像。 todo: How to build a CI/CD pipeline with Docker Run your CI/CD jobs in Docker containers Docker for your CI CD Pipeline Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/CICD/docker_makefile_CI_Pipeline.html":{"url":"Docker/CICD/docker_makefile_CI_Pipeline.html","title":"docker   makefile  ci pipeline","keywords":"","body":"docker + makefile =CI Pipeline1. 简单的docker&makefile1.1 编辑 dockerfile1.2 编辑 Makefile1.3 执行 make2. docker & Makefile 实战2.1 编写 Makefile2.2 处理 UID2.3 执行 make2.4 开发小技巧docker + makefile =CI Pipeline 1. 简单的docker&makefile 1.1 编辑 dockerfile $ cat Dockerfile FROM busybox CMD [\"date\"] 1.2 编辑 Makefile build: docker build -t benhall/docker-make-example . run: docker run benhall/docker-make-example default: build test 1.3 执行 make #通过dockerfile构建容器 $ make docker build -t benhall/docker-make-example . Sending build context to Docker daemon 116.7kB Step 1/2 : FROM busybox latest: Pulling from library/busybox 24fb2886d6f6: Pull complete Digest: sha256:f7ca5a32c10d51aeda3b4d01c61c6061f497893d7f6628b92f822f7117182a57 Status: Downloaded newer image for busybox:latest ---> 16ea53ea7c65 Step 2/2 : CMD [\"date\"] ---> Running in 26d6f36ad3b4 Removing intermediate container 26d6f36ad3b4 ---> 339b5a708dce Successfully built 339b5a708dce Successfully tagged benhall/docker-make-example:latest #查看构建效果 $ docker images |grep benha benhall/docker-make-example latest 339b5a708dce 2 minutes ago 1.24MB #运行容器 $ make run docker run benhall/docker-make-example Mon Sep 27 08:36:15 UTC 2021 $ docker ps |grep ben $ docker ps -a |grep ben c8cf9f9b7090 benhall/docker-make-example \"date\" 14 seconds ago Exited (0) 13 seconds ago amazing_buck #一次性构建部署 $ make build run docker build -t benhall/docker-make-example . Sending build context to Docker daemon 116.7kB Step 1/2 : FROM busybox ---> 16ea53ea7c65 Step 2/2 : CMD [\"date\"] ---> Using cache ---> 339b5a708dce Successfully built 339b5a708dce Successfully tagged benhall/docker-make-example:latest docker run benhall/docker-make-example Mon Sep 27 08:38:11 UTC 2021 我希望我所有的项目都能像这样工作： git pull && make test && make build && make deploy 2. docker & Makefile 实战 项目：https://github.com/Ghostwritten/dockerbuild 2.1 编写 Makefile 版权作者说明 # -------------------------------------------------------------------- # Copyright (c) 2019 LINKIT, The Netherlands. All Rights Reserved. # Author(s): Anthony Potappel # # This software may be modified and distributed under the terms of the # MIT license. See the LICENSE file for details. # -------------------------------------------------------------------- # If you see pwd_unknown showing up, this is why. Re-calibrate your system. #条件赋值 ( ?= ) 如果变量未定义，则使用符号中的值定义变量。如果该变量已经赋值，则该赋值语句无效。 PWD ?= pwd_unknown # PROJECT_NAME defaults to name of the current directory. # should not to be changed if you follow GitOps operating procedures. PROJECT_NAME = $(notdir $(PWD)) # Note. If you change this, you also need to update docker-compose.yml. # only useful in a setting with multiple services/ makefiles. #简单赋值 ( := ) 编程语言中常规理解的赋值方式，只对当前语句的变量有效。 SERVICE_TARGET := main # if vars not set specifially: try default to environment, else fixed value. # strip to ensure spaces are removed in future editorial mistakes. # tested to work consistently on popular Linux flavors and Mac. ifeq ($(user),) # USER retrieved from env, UID from shell. HOST_USER ?= $(strip $(if $(USER),$(USER),nodummy)) HOST_UID ?= $(strip $(if $(shell id -u),$(shell id -u),4000)) else # allow override by adding user= and/ or uid= (lowercase!). # uid= defaults to 0 if user= set (i.e. root). HOST_USER = $(user) HOST_UID = $(strip $(if $(uid),$(uid),0)) endif THIS_FILE := $(lastword $(MAKEFILE_LIST)) CMD_ARGUMENTS ?= $(cmd) # export such that its passed to shell functions for Docker to pick up. export PROJECT_NAME export HOST_USER export HOST_UID # all our targets are phony (no files to check). .PHONY: shell help build rebuild service login test clean prune # suppress makes own output #.SILENT: # shell is the first target. So instead of: make shell cmd=\"whoami\", we can type: make cmd=\"whoami\". # more examples: make shell cmd=\"whoami && env\", make shell cmd=\"echo hello container space\". # leave the double quotes to prevent commands overflowing in makefile (things like && would break) # special chars: '',\"\",|,&&,||,*,^,[], should all work. Except \"$\" and \"`\", if someone knows how, please let me know!). # escaping (\\) does work on most chars, except double quotes (if someone knows how, please let me know) # i.e. works on most cases. For everything else perhaps more useful to upload a script and execute that. shell: ifeq ($(CMD_ARGUMENTS),) # no command is given, default to shell docker-compose -p $(PROJECT_NAME)_$(HOST_UID) run --rm $(SERVICE_TARGET) sh else # run the command docker-compose -p $(PROJECT_NAME)_$(HOST_UID) run --rm $(SERVICE_TARGET) sh -c \"$(CMD_ARGUMENTS)\" endif # Regular Makefile part for buildpypi itself help: @echo '' @echo 'Usage: make [TARGET] [EXTRA_ARGUMENTS]' @echo 'Targets:' @echo ' build build docker --image-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo ' rebuild rebuild docker --image-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo ' test test docker --container-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo ' service run as service --container-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo ' login run as service and login --container-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo ' clean remove docker --image-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo ' prune shortcut for docker system prune -af. Cleanup inactive containers and cache.' @echo ' shell run docker --container-- for current user: $(HOST_USER)(uid=$(HOST_UID))' @echo '' @echo 'Extra arguments:' @echo 'cmd=: make cmd=\"whoami\"' @echo '# user= and uid= allows to override current user. Might require additional privileges.' @echo 'user=: make shell user=root (no need to set uid=0)' @echo 'uid=: make shell user=dummy uid=4000 (defaults to 0 if user= set)' rebuild: # force a rebuild by passing --no-cache docker-compose build --no-cache $(SERVICE_TARGET) service: # run as a (background) service docker-compose -p $(PROJECT_NAME)_$(HOST_UID) up -d $(SERVICE_TARGET) login: service # run as a service and attach to it docker exec -it $(PROJECT_NAME)_$(HOST_UID) sh build: # only build the container. Note, docker does this also if you apply other targets. docker-compose build $(SERVICE_TARGET) clean: # remove created images @docker-compose -p $(PROJECT_NAME)_$(HOST_UID) down --remove-orphans --rmi all 2>/dev/null \\ && echo 'Image(s) for \"$(PROJECT_NAME):$(HOST_USER)\" removed.' \\ || echo 'Image(s) for \"$(PROJECT_NAME):$(HOST_USER)\" already removed.' prune: # clean all that is not actively used docker system prune -af test: # here it is useful to add your own customised tests docker-compose -p $(PROJECT_NAME)_$(HOST_UID) run --rm $(SERVICE_TARGET) sh -c '\\ echo \"I am `whoami`. My uid is `id -u`.\" && echo \"Docker runs!\"' \\ && echo success 2.2 处理 UID 未配置 UserID (UID) 时，Docker 会将容器默认为用户 root。当您开始处理生产系统时，需要练习正确配置用户。你可以在Dockers 的最佳实践列表上阅读它，来自 K8s 的这篇文章也很好地涵盖了它。 当您在 Mac 上本地运行测试时，root 会映射到您自己的用户，因此一切正常。生产平台——应该——被配置为用户隔离，但这并不总是默认的。例如，如果您在 Linux 系统上运行测试（没有重新映射），您可能会偶然发现 Docker 生成的文件归 root 所有的问题。 当您在一个系统上有多个用户时，您也会遇到问题。即使您不共享一个系统，运行并行测试也同样需要分离。而且，如果没有正确的 UID 设置，您甚至如何传递凭据（例如，通过将卷链接为 ~/.ssh 和 ~/.aws）？后者是处理基础设施部署时的常见模式。 如果你开始成为一个更密集的 docker-consumer，你的团队会成长并且事情会变得复杂，最终你会想要或需要在你的所有项目中嵌入 UID 分离。 幸运的是，尽早配置 UID 是（相对）容易的。虽然我花了一些练习才能获得良好的设置，但我现在有一个模板（假设使用 Makefile）自动完成所有操作，成本几乎为零。 以下是要使用的docker-compose文件的副本： version: '3.4' services: main: # Makefile fills PROJECT_NAME to current directory name. # add UID to allow multiple users run this in parallel container_name: ${PROJECT_NAME}_${HOST_UID:-4000} hostname: ${PROJECT_NAME} # These variables are passed into the container. environment: - UID=${HOST_UID:-4000} # Run with user priviliges by default. user: ${HOST_USER:-nodummy} image: ${PROJECT_NAME}:${HOST_USER:-nodummy} build: context: . # Build for current user. target: user dockerfile: Dockerfile # These variables are passed to Dockerfile. args: - HOST_UID=${HOST_UID:-4000} - HOST_USER=${HOST_USER:-nodummy} # Run container as a service. Replace with something useful. command: [\"tail\", \"-f\", \"/dev/null\"] # Copy current (git-) project into container. volumes: - ${PWD:-.}:/home/${HOST_USER}/${PROJECT_NAME} 默认变量的一种写法 ${HOST_USER:-nodummy} ${HOST_UID:-4000} 这会从您的运行时复制变量，如果不存在，则分别默认为“nodummy”和“4000”。如果您不喜欢默认值，请执行以下操作： ${HOST_USER:?You forgot to set HOST_USER in .env!} ${HOST_UID:?You forgot to set HOST_UID in .env!} 注意“HOST_”前缀。我避免直接​​使用 USER 和 UID。不保证这些变量在运行时可用。USER 通常在 shell 中可用，但 UID 主要是 Docker 不会获取的环境变量。拥有单独的命名方案可以防止意外，并允许灵活配置自动化管道。 Dockerfile内容如下： FROM alpine as base RUN apk update \\ && apk add --no-cache \\ bash FROM scratch as user COPY --from=base . . ARG HOST_UID=${HOST_UID:-4000} ARG HOST_USER=${HOST_USER:-nodummy} RUN [ \"${HOST_USER}\" == \"root\" ] || \\ (adduser -h /home/${HOST_USER} -D -u ${HOST_UID} ${HOST_USER} \\ && chown -R \"${HOST_UID}:${HOST_UID}\" /home/${HOST_USER}) USER ${HOST_USER} WORKDIR /home/${HOST_USER} 在这里，我们构建了一个小型（只有 10MB，微服务 FTW！）Alpine 容器，添加了 bash 以供娱乐和练习。我们应用所谓的分阶段构建的概念来保持基础镜像（可重用构建组件）与用户镜像（为特定运行准备的镜像）分离。 使用 Makefile 时，所有变量都会自动设置。这个 Dockerfile 在没有 Makefile 的情况下也能正常工作，但用户仍然可以在自己的运行时或单独的env-file 中配置变量. 如果您处于开发模式，您可能会遇到某些障碍，需要您以 root 用户身份进行故障排除。 make shell user=root 2.3 执行 make # download our files git clone https://github.com/LINKIT-Group/dockerbuild # enter directory cd dockerbuild # build, test and run a command make build test shell cmd=\"whoami\" # my favorite for container exploration make shell # shell-target is the default (first item), so this also works: make cmd=\"whoami\" make cmd=\"ls /\" # force a rebuild, test and cleanup make rebuild test clean 2.4 开发小技巧 运行容器最好加上--rm 我们还可以加其他目标任务，比如将镜像推送到仓库。 参考： 如何学习Makefile docker & Makefile实战 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/CICD/docker_GitHub_Action_builds_image.html":{"url":"Docker/CICD/docker_GitHub_Action_builds_image.html","title":"github action 构建镜像","keywords":"","body":"GitHub Action 构建镜像1. 简介2. 功能3. 什么是 GitHub Action？3.1 Workflow3.2 Event3.3 Jobs3.4 Step4. 费用5. 创建 GitHub Action Workflow5.1 创建 build.yaml 文件5.2 创建 GitHub 仓库并推送5.3 创建 Docker Hub Secret5.4 创建 GitHub Action Secret5.5 触发 GitHub Action Workflow6. 总结GitHub Action 构建镜像 tagsstart $/backend:$ tagsstop 1. 简介 GitHub Packages 是一个用于托管和管理包的平台，包括容器和其他依赖项。 GitHub Packages 将源代码和包组合在一起，以提供集成的权限管理和计费，使你能够在 GitHub 上专注于软件开发。 GitHub 正在以托管代码仓库为切入点，逐步覆盖整个研发工具链，打造一站式 DevOps 平台。项目管理有 Issues 、Projects，包管理有 Packages，CI 有 Actions，知识管理有 Wiki ，覆盖面越来越广。 GitHub Packages 为常用的包管理器提供不同的包仓库，例如 npm、RubyGems、Apache Maven、Gradle、Docker 和 Nuget。 GitHub 的 Container registry 针对容器进行了优化，支持 Docker 和 OCI 映像。 今天，我们尝试 Github Container Registry 应用实践。 2. 功能 Container registry 为您和您的组织添加了 GitHub Packages 的功能，以便在您的开发中有效地使用容器。自推出测试版以来，我们逐步发布并共享了容器注册表的一些附加功能。这是您可以利用的所有内容的完整总结： 公共容器的匿名访问 容器的组织级所有权 细粒度的容器权限控制 充满有用信息的特定容器登陆页面 独立于存储库可见性的容器可见性 组织内容器的内部可见性设置 通过 Actions 工作流安全无缝地访问容器GITHUB_TOKEN 清理入门工作流以直接发布到 Container Registry，网址为ghcr.io 3. 什么是 GitHub Action？ 这张图中出现了几个基本概念：Workflow、Event、Job 和 Step，我们分开来讲解。 3.1 Workflow Workflow 也叫做工作流。其实，GitHub Action 本质上是一个是一个 CI/CD 工作流，要使用工作流，我们首先需要先定义它。和 K8s Manifest 一样，GitHub Action 工作流是通过 YAML 来描述的，你可以在任何 GitHub 仓库创建 .github/workflows 目录，并创建 YAML 文件来定义工作流。所有在 .github/workflows 目录创建的工作流文件，都将被 GitHub 自动扫描。在工作流中，通常我们会进一步定义 Event、Job 和 Step 字段，它们被用来定义工作流的触发时机和具体行为。 3.2 Event Event 从字面上的理解是“事件”的意思，你可以简单地把它理解为定义了“什么时候运行工作流”，也就是工作流的触发器。在定义自动化构建镜像的工作流时，我们通常会把 Event 的触发器配置成“当指定分支有新的提交时，自动触发镜像构建”。 3.3 Jobs Jobs 的字面意思是一个具体的任务，它是一个抽象概念。在工作流中，它并不能直接工作，而是需要通过 Step 来定义具体的行为。此外，你还可以为 Job 定义它的运行的环境，例如 ubuntu。在一个 Workflow 当中，你可以定义多个 Job，多个 Job 之间可以并行运行，也可以定义相互依赖关系。在自动构建镜像环节，通常我们只需要定义一个 Job 就够了，所以在上面的示意图中，我只画出了一个 Job。 3.4 Step Step 隶属于 Jobs，它是工作流中最小的粒度，也是最重要的部分。通常来说，Step 的具体行为是执行一段 Shell 来完成一个功能。在同一个 Job 里，一般我们需要定义多个 Step 才能完成一个完整的 Job，由于它们是在同一个环境下运行的，所以当它们运行时，就等同于在同一台设备上执行一段 Shell。 以自动构建镜像为例，我们可能需要在 1 个 Job 中定义 3 个 Step。 Step1，克隆仓库的源码。 Step2，运行 docker build 来构建镜像。 Step3，推送到镜像仓库。 4. 费用 GitHub Action 在使用上虽然很方便，但天下并没有免费的午餐。对于 GitHub 免费账户，每个月有 2000 分钟的 GitHub Action 时长可供使用（Linux 环境），超出时长则需要按量付费，你可以在这里查看详细的计费策略。 5. 创建 GitHub Action Workflow 我以 K8s 极简实战模块的示例应用为例，看看如何配置自动构建示例应用的前后端镜像工作流。在这个例子中，我们创建的工作流将实现以下这些步骤。 当 main 分支有新的提交时，触发工作流。 克隆代码。 初始化 Docker 构建工具链。 登录 Docker Hub。 构建前后端应用镜像， 并使用 commit id 作为镜像的 tag。 推送到 Docker Hub 镜像仓库。 下面，我们来为示例应用创建工作流。 5.1 创建 build.yaml 文件 首先，我们要将示例应用仓库克隆到本地。 $ git clone https://github.com/lyzhang1999/kubernetes-example.git 进入 kubernetes-example 目录。 $ cd kubernetes-example 然后，在当前目录下新建 .github/workflows 目录。 $ mkdir -p .github/workflows 接下来，将下面的内容保存到 .github/workflows/build.yaml 文件内。 name: build on: push: branches: - 'main' env: DOCKERHUB_USERNAME: ghostwritten jobs: docker: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Set outputs id: vars run: echo \"::set-output name=sha_short::$(git rev-parse --short HEAD)\" - name: Set up QEMU uses: docker/setup-qemu-action@v2 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v2 - name: Login to Docker Hub uses: docker/login-action@v2 with: username: ${{ env.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build backend and push uses: docker/build-push-action@v3 with: context: backend push: true platforms: linux/amd64,linux/arm64 tags: ${{ env.DOCKERHUB_USERNAME }}/backend:${{ steps.vars.outputs.sha_short }} - name: Build frontend and push uses: docker/build-push-action@v3 with: context: frontend push: true platforms: linux/amd64,linux/arm64 tags: ${{ env.DOCKERHUB_USERNAME }}/frontend:${{ steps.vars.outputs.sha_short }} 请注意，你需要将上面的 env.DOCKERHUB_USERNAME 环境变量替换为你的 Docker Hub 用户名。我简单介绍一下这个工作流。 我简单介绍一下这个工作流。 这里的 name 字段是工作流的名称，它会展示在 GitHub 网页上。 on.push.branches 字段的值为 main，这代表当 main 分支有新的提交之后，会触发工作流。--- env.DOCKERHUB_USERNAME 是我们为 Job 配置的全局环境变量，用作镜像 Tag 的前缀。 jobs.docker 字段定义了一个任务，它的运行环境是 ubuntu-latest，并且由 7 个 Step 组成。 jobs.docker.steps 字段定义了 7 个具体的执行阶段。 要特别注意的是，uses 字段代表使用 GitHub Action 的某个插件，例如 actions/checkout@v3 插件会帮助我们检出代码。在这个工作流中，这 7 个阶段会具体执行下面几件事。 “Checkout”阶段负责将代码检出到运行环境。 “Set outputs”阶段会输出 sha_short 环境变量，值为 short commit id，这可以方便在后续阶段引用。 “Set up QEMU”和“Set up Docker Buildx”阶段负责初始化 Docker 构建工具链。 “Login to Docker Hub”阶段通过 docker login 来登录到 Docker Hub，以便获得推送镜像的权限。要注意的是，with 字段是向插件传递参数的，在这里我们传递了 username 和 password，值的来源分别是我们定义的环境变量 DOCKERHUB_USERNAME 和 GitHub Action Secret，后者我们还会在稍后进行配置。 “Build backend and push”和“Build frontend and push”阶段负责构建前后端镜像，并且将镜像推送到 Docker Hub，在这个阶段中，我们传递了 context、push 和 tags 参数，context 和 tags 实际上就是 docker build 的参数。在 tags 参数中，我们通过表达式 $ 和 $ 分别读取了 在 YAML 中预定义的 Docker Hub 的用户名，以及在“Set outputs”阶段输出的 short commit id。 5.2 创建 GitHub 仓库并推送 创建完 build.yaml 文件后，接下来，我们要把示例应用推送到 GitHub 上。首先，你需要通过这个页面来为自己创建新的代码仓库，仓库名设置为 kubernetes-example。 创建完成后，将刚才克隆的 kubernetes-example 仓库的 remote url 配置为你刚才创建仓库的 Git 地址。 $ git remote set-url origin YOUR_GIT_URL 然后，将 kubernetes-example 推送到你的仓库。在这之前，你可能还需要配置 SSH Key，你可以参考这个链接来配置，这里就不再赘述了。 $ git add . $ git commit -a -m 'first commit' $ git branch -M main $ git push -u origin main 5.3 创建 Docker Hub Secret 创建完 build.yaml 文件后，接下来，我们需要创建 Docker Hub Secret，它将会为工作流提供推送镜像的权限。首先，使用你注册的账号密码登录 https://hub.docker.com/。然后，点击右上角的“用户名”，选择“Account Settings”，并进入左侧的“Security”菜单。 下一步点击右侧的“New Access Token”按钮，创建一个新的 Token。 输入描述，然后点击“Genarate”按钮生成 Token。 点击“Copy and Close”将 Token 复制到剪贴板。请注意，当窗口关闭后，Token 无法再次查看，所以请在其他地方先保存刚才生成的 Token。 5.4 创建 GitHub Action Secret 创建完 Docker Hub Token 之后，接下来我们就可以创建 GitHub Action Secret 了，也就是说我们要为 Workflow 提供 secrets.DOCKERHUB_TOKEN 变量值。 进入 kubernetes-example 仓库的 Settings 页面，点击左侧的“Secrets”，进入“Actions”菜单，然后点击右侧“New repository secret”创建新的 Secret。 在 Name 输入框中输入 DOCKERHUB_TOKEN，这样在 GitHub Action 的 Step 中，就可以通过 ${{ secrets.DOCKERHUB_TOKEN }} 表达式来获取它的值。 在 Secret 输入框中输入刚才我们复制的 Docker Hub Token，点击“Add secret”创建。 5.5 触发 GitHub Action Workflow 到这里，准备工作已经全部完成了，接下来我们尝试触发 GitHub Action 工作流。还记得我们在工作流配置的 on.push.branches 字段吗？它的值为 main，代表当有新的提交到 main 分支时触发工作流。 首先，我们向仓库提交一个空 commit。 $ git commit --allow-empty -m \"Trigger Build\" 然后，使用 git push 来推送到仓库，这将触发工作流。 $ git push origin main 接下来，进入 kubernetes-example 仓库的“Actions”页面，你将看到我们刚才触发的工作流。 在工作流的详情页面，我们能看到工作流的每一个 Step 的状态及其运行时输出的日志。 当工作流运行完成后，进入到 Docker Hub frontend 或者 backend 镜像的详情页，你将看到刚才 GitHub Action 自动构建并推送的新版本镜像。 到这里，我们便完成了使用 GitHub Action 自动构建镜像的全过程。最终实现效果是，当我们向 main 分支提交代码时，GitHub 工作流将自动构建 frontend 和 backend 镜像，并且每一个 commit id 对应一个镜像版本。 6. 总结 总结一下，这节课，我为你介绍了构成 GitOps 工作流的第一个自动化阶段：自动化构建镜像。为了实现自动化构建镜像，我们学习了 GitHub Action 工作流及其基本概念，例如 Workflow、Event、Jobs 和 Steps。 在介绍 GitHub Action 相关概念时，我故意精简了一部分概念，比如 Runner、多个 Jobs 以及 Jobs 相互依赖的情况。在现阶段，我们只需要掌握最简单的自动构建镜像的 YAML 写法以及相关概念就足够了。 在实战环节，我们创建了一个 build.yaml 文件用来定义 GitHub Action 工作流，总结来说，它定义了工作流的： 工作流名称 在什么时候触发 在什么环境下运行 具体执行的步骤是什么 需要注意的是，在创建 build.yaml 文件后，你需要创建自己的仓库，并将 kubernetes-example 的内容推送到你的仓库中，以便进行触发工作流的实验。其次，为了给 GitHub Action 工作流赋予镜像仓库的推送权限，我们还需要在 Docker Hub 中创建 Token，并将其配置到仓库的 Secrets 中。在配置时，需要注意 Secret Name 和工作流 Step 中的 ${{ secrets.DOCKERHUB_TOKEN }} 表达式相互对应，以便工作流能够获取到正确的 Secrets。 配置完成后，当我们向 Main 分支推送新的提交时，GitHub Action 工作流将会被自动触发，工作流会自动构建 frontend 和 backend 镜像，并且会使用当前的 short commit id 作为镜像的 Tag 推送到 Docker Hub 中。 这意味着，每一个提交都会生成一个 Docker 镜像，实现了代码和制品的对应关系。这种对应关系给我们带来了非常大的好处，例如当我们要回滚或更新应用时，只需要找到代码的 commit id 就能够找到对应的镜像版本。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 13:52:39 "},"Docker/CICD/docker_deploy_enterprise-level_harbor.html":{"url":"Docker/CICD/docker_deploy_enterprise-level_harbor.html","title":"自托管构建 如何使用 harbor 搭建企业级镜像仓库","keywords":"","body":"自托管构建：如何使用 Harbor 搭建企业级镜像仓库？1. 背景2. 安装 Helm3. 安装 Cert-manager4. 安装和配置 Harbor4.1 安装 Harbor4.2 配置 DNS 解析4.3 访问 Harbor Dashboard4.4 推送镜像测试5. 在 Tekton Pipeline 中使用 Harbor6. Harbor 生产建议6.1 确认 PVC 是否支持在线扩容6.2 推荐使用 S3 存储镜像6.3 使用外部数据库和 Redis开启自动镜像扫描和阻止漏洞镜像7. 总结8. 思考自托管构建：如何使用 Harbor 搭建企业级镜像仓库？ 1. 背景 但是，在之前的构建方案中，我们是将镜像推送到了 Docker Hub 镜像仓库。实际上，Docker Hub 也是一个收费的服务，对于免费用户来说，它限制每 6 小时最多拉取 200 次镜像，显然，这对团队来说是完全不够用的。 在这节课，我就带你学习如何使用 Harbor 来搭建企业级的镜像仓库，将它集成到我们上一节课创建的 Tekton Pipeline 流程中，最终替换 Docker Hub，进一步降低镜像存储的成本。此外，在安装 Harbor 的过程中，我还会首次介绍 Helm 工具的使用方法。 在开始今天的学习之前，你需要按照上一节课的内容准备好一个云厂商的 Kubernetes 集群，安装 Ingress-Nginx 和 Tekton，并配置好 Pipeline 和 GitHub Webook。此外，在生产环境下，Harbor 一般都会开启 TLS，所以你还需要准备一个可用的域名。 2. 安装 Helm 在我们之前的实践中，像是安装 Tekton 和 Ingress-Nginx 都是通过 Kubernetes Manifest 来完成的。实际上，安装 Kubernetes 应用并不只有一种方案，这里我们介绍第二种方案。Helm。 要使用 Helm，首先需要安装它，你可以通过这个链接查看不同平台的安装方法，这里我使用官方提供的脚本来安装。 curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash 安装好 Helm 之后，在正式使用之前，你还要确保本地 Kubectl 和集群的连通性，Helm 和 Kubectl 默认读取的 Kubeconfig 文件路径都是 ~/.kube/config。 3. 安装 Cert-manager 下来我们安装 Cert-manager，它会为我们自动签发免费的 Let’s Encrypt HTTPS 证书，并在过期前自动续期。首先，运行 helm repo add 命令添加官方 Helm 仓库。 $ helm repo add jetstack https://charts.jetstack.io \"jetstack\" has been added to your repositories 然后，运行 helm repo update 更新本地缓存。 $ helm repo update ...Successfully got an update from the \"jetstack\" chart repository 接下来，运行 helm install 来安装 Cert-manager。 $ helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.10.0 --set ingressShim.defaultIssuerName=letsencrypt-prod --set ingressShim.defaultIssuerKind=ClusterIssuer --set ingressShim.defaultIssuerGroup=cert-manager.io --set installCRDs=true NAME: cert-manager LAST DEPLOYED: Mon Oct 17 21:26:44 2022 NAMESPACE: cert-manager STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: cert-manager v1.10.0 has been deployed successfully! 此外，还需要为 Cert-manager 创建 ClusterIssuer，用来提供签发机构。将下面的内容保存为 cluster-issuer.yaml。 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: \"1zoxun1@gmail.com\" privateKeySecretRef: name: letsencrypt-prod solvers: - http01: ingress: class: nginx 注意，这里你需要将 spec.acme.email 替换为你真实的邮箱地址。然后运行 kubectl apply 提交到集群内。 $ kubectl apply -f cluster-issuer.yaml clusterissuer.cert-manager.io/letsencrypt-prod created 到这里，Cert-manager 就已经配置好了。 4. 安装和配置 Harbor 4.1 安装 Harbor 现在，我们同样使用 Helm 来安装 Harbor，首先添加 Harbor 官方仓库。 helm repo add harbor https://helm.goharbor.io 然后，更新本地 Helm 缓存。 helm repo update 接下来，由于我们需要定制化安装 Harbor，所以需要修改 Harbor 的安装参数，将下面的内容保存为 values.yaml。 expose: type: ingress tls: enabled: true certSource: secret secret: secretName: \"harbor-secret-tls\" notarySecretName: \"notary-secret-tls\" ingress: hosts: core: harbor.geekcloudnative.com notary: notary.geekcloudnative.com className: nginx annotations: kubernetes.io/tls-acme: \"true\" persistence: persistentVolumeClaim: registry: size: 20Gi chartmuseum: size: 10Gi jobservice: jobLog: size: 10Gi scanDataExports: size: 10Gi database: size: 10Gi redis: size: 10Gi trivy: size: 10Gi 注意，由于腾讯云的 PVC 最小 10G 起售，所以，除了 registry 的容量以外，我将安装参数中其他的持久化卷容量都配置为了 10G，你可以根据安装集群的实际情况做调整。 另外，我还为 Harbor 配置了 ingress 访问域名，分别是 harbor.geekcloudnative.com 和 notary.geekcloudnative.com，你需要将它们分别替换成你的真实域名。 然后，再通过 helm install 命令来安装 Harbor，并指定参数配置文件 values.yaml。 $ helm install harbor harbor/harbor -f values.yaml --namespace harbor --create-namespace NAME: harbor LAST DEPLOYED: Mon Oct 17 21:53:28 2022 NAMESPACE: harbor STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Please wait for several minutes for Harbor deployment to complete. Then you should be able to visit the Harbor portal at https://core.harbor.domain For more details, please visit https://github.com/goharbor/harbor 等待所有 Pod 处于就绪状态。 $ kubectl wait --for=condition=Ready pods --all -n harbor --timeout 600s pod/cm-acme-http-solver-4v4zz condition met pod/harbor-chartmuseum-79f49f5b4-8f4mb condition met pod/harbor-core-6c6bc7fb4f-4822f condition met pod/harbor-database-0 condition met pod/harbor-jobservice-85d448d5c9-gn5pk condition met pod/harbor-notary-server-848bcc7ccd-m5m5v condition met pod/harbor-notary-signer-6897444589-6vssq condition met pod/harbor-portal-588b64cbdb-gqlbn condition met pod/harbor-redis-0 condition met pod/harbor-registry-5c7d58c87c-6bgsj condition met pod/harbor-trivy-0 condition met 到这里，Harbor 就已经安装完成了。 4.2 配置 DNS 解析 接下来，我们为域名配置 DNS 解析。首先，获取 Ingress-Nginx Loadbalancer 的外网 IP。 $ kubectl get services --namespace ingress-nginx ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' 43.135.82.249 然后，为域名配置 DNS 解析。在这个例子中，我需要分别为 harbor.geekcloudnative.com 和 notary.geekcloudnative.com 配置 A 记录，并指向 43.135.82.249。 4.3 访问 Harbor Dashboard 在访问 Harbor Dashboard 之前，首先我们要确认 Cert-manager 是否已经成功签发了 HTTPS 证书，你可以通过 kubectl get certificate 命令来确认。 $ kubectl get certificate -A NAMESPACE NAME READY SECRET AGE harbor harbor-secret-tls True harbor-secret-tls 8s harbor notary-secret-tls True notary-secret-tls 8s 由于我们在部署 Harbor 的时候需要配置两个域名，所以这里会出现两个证书。当这两个证书的 Ready 状态都为 True 时，说明 HTTPS 证书已经签发成功了。此外，Cert-manager 自动从 Ingress 对象中读取了 tls 配置，还自动创建了名为 harbor-secret-tls 和 notary-secret-tls 两个包含证书信息的 Secret。 接下来，打开 harbor.geekcloudnative.com 进入 Harbor Dashboard，使用默认账号 admin 和 Harbor12345 即可登录控制台。 Harbor 已经自动为我们创建了 library 项目，我们在后续的阶段将直接使用它。 4.4 推送镜像测试 现在，让我们来尝试将本地的镜像推送到 Harbor 仓库。首先，在本地拉取 busybox 镜像。 $ docker pull busybox Using default tag: latest latest: Pulling from library/busybox f5b7ce95afea: Pull complete Digest: sha256:9810966b5f712084ea05bf28fc8ba2c8fb110baa2531a10e2da52c1efc504698 Status: Downloaded newer image for busybox:latest docker.io/library/busybox:latest 然后，运行 docker login 命令登录到 Harbor 仓库，使用默认的账号密码。 $ docker login harbor.geekcloudnative.com username: admin password: Harbor12345 Login Succeeded 接下来，重新给 busybox 镜像打标签，指向 Harbor 镜像仓库。 $ docker tag busybox:latest harbor.geekcloudnative.com/library/busybox:latest 和推送到 Docker Hub 的 Tag 相比，推送到 Harbor 需要指定完整的镜像仓库地址、项目名和镜像名。在这里，我使用了默认的 library 项目，当然你也可以新建一个项目，并将 library 替换为新的项目名。 最后，将镜像推送到仓库。 $ docker push harbor.geekcloudnative.com/library/busybox:latest The push refers to repository [harbor.geekcloudnative.com/library/busybox] 0b16ab2571f4: Pushed latest: digest: sha256:7bd0c945d7e4cc2ce5c21d449ba07eb89c8e6c28085edbcf6f5fa4bf90e7eedc size: 527 镜像推送成功后，访问 Harbor 控制台，进入 library 项目详情，你将看到我们刚才推送的镜像。 到这里，Harbor 镜像仓库就已经配置好了。 5. 在 Tekton Pipeline 中使用 Harbor 要在 Tekton Pipeline 中使用 Harbor，我们需要将 Pipeline 中的 spec.params.registry_url 变量值由 docker.io 修改为 harbor.geekcloudnative.com，并且将 spec.params.registry_mirror 变量值修改为 library。你可以使用 kubectl edit 命令来修改。 $ kubectl edit Pipeline github-trigger-pipeline ...... params: - default: harbor.n7t.dev # 修改为 harbor.n7t.dev name: registry_url type: string - default: \"library\" # 修改为 library name: registry_mirror type: string pipeline.tekton.dev/github-trigger-pipeline edited 然后，再修改镜像仓库的凭据，也就是 registry-auth Secret。 $ kubectl edit secret registry-auth apiVersion: v1 data: password: SGFyYm9yMTIzNDUK # 修改为 Base64 编码：Harbor12345 username: YWRtaW4K # 修改为 Base64 编码：admin kind: Secret secret/registry-auth edited 保存后生效。现在，回到本地示例应用 kubernetes-example 目录，向仓库推送一个空的 commit 来触发 Tekton 流水线。 $ git commit --allow-empty -m \"Trigger Build\" [main e42ac45] Trigger Build $ git push origin main 进入 Tekton Dashboard http://tekton.k8s.local/，并查看流水线运行状态。 当流水线运行结束后，我们进入 Harbor Dashboard 会看到刚才 Tekton 推送的新镜像。 到这里，Harbor 的安装和配置就完成了。在最开始，我们是通过最小化的配置安装 Harbor，在生产环境下你可能需要注意一些额外的配置。 6. Harbor 生产建议 Harbor 的安装配置相对较多，这里我也提供几点生产建议供你参考。 确认 PVC 是否支持在线扩容。 尽量使用 S3 作为镜像存储系统。 使用外部数据库和 Redis。 开启自动镜像扫描和阻止漏洞镜像。 6.1 确认 PVC 是否支持在线扩容 如果你是按照这节课的安装方式使用 PVC 持久卷来存储镜像的，那么随着镜像数量的增加，你需要额外注意 Harbor 仓库存储容量的问题。 一个简单的方案是在 Harbor 安装前，提前确认 StorageClass 是否支持在线扩容，以便后续对存储镜像的持久卷进行动态扩容。你可以使用 kubectl get storageclass 命令来确认。 $ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cbs (default) com.tencent.cloud.csi.cbs Delete Immediate true 在返回内容中，如果 ALLOWVOLUMEEXPANSION 为 true，就说明支持在线扩容。否则，你需要手动为 StorageClass 添加 AllowVolumeExpansion 字段。 $ kubectl patch storageclass cbs -p '{\"allowVolumeExpansion\": true}' 6.2 推荐使用 S3 存储镜像 除了使用持久卷来存储镜像以外，Harbor 还支持外部存储。如果你希望大规模使用 Harbor 又不想关注存储问题，那么使用外部存储是一个非常的选择。例如使用 AWS S3 存储桶来存储镜像。 S3 存储方案的优势是，它能为我们提供接近无限存储容量的存储系统，并且按量计费的方式成本也相对可控，同时它还具备高可用性和容灾能力。 要使用 S3 来存储镜像，你需要在安装时修改 Harbor 的安装配置 values.yaml。 expose: type: ingress tls: enabled: true certSource: secret secret: secretName: \"harbor-secret-tls\" notarySecretName: \"notary-secret-tls\" ingress: hosts: core: harbor.n7t.dev notary: notary.n7t.dev className: nginx annotations: kubernetes.io/tls-acme: \"true\" persistence: imageChartStorage: type: s3 s3: region: us-west-1 bucket: bucketname accesskey: AWS_ACCESS_KEY_ID secretkey: AWS_SECRET_ACCESS_KEY rootdirectory: /harbor persistentVolumeClaim: chartmuseum: size: 10Gi jobservice: jobLog: size: 10Gi scanDataExports: size: 10Gi ...... 注意，要将 S3 相关配置 region、bucket、accesskey、secretkey 和 rootdirectory 字段修改为实际的值。然后，再使用 helm install -f values.yaml 来安装。 6.3 使用外部数据库和 Redis 在安装 Harbor 时，会默认自动安装数据库和 Redis。但是为了保证稳定性和高可用，我建议你使用云厂商提供的 Postgres 和 Redis 托管服务。要使用外部数据库和 Redis，你同样可以在 values.yaml 文件中直接指定。 expose: type: ingress tls: enabled: true certSource: secret secret: secretName: \"harbor-secret-tls\" notarySecretName: \"notary-secret-tls\" ingress: hosts: core: harbor.n7t.dev notary: notary.n7t.dev className: nginx annotations: kubernetes.io/tls-acme: \"true\" database: type: external external: host: \"192.168.0.1\" port: \"5432\" username: \"user\" password: \"password\" coreDatabase: \"registry\" notaryServerDatabase: \"notary_server\" notarySignerDatabase: \"notary_signer\" redis: type: external external: addr: \"192.168.0.2:6379\" password: \"\" persistence: ...... 注意，要将 database 和 redis 字段的连接信息修改为实际的内容，并提前创建好 registry、notary_server 和 notary_signer 数据库。 开启自动镜像扫描和阻止漏洞镜像 最后一个建议。由于 Harbor 自带 Trivy 镜像扫描功能，可以帮助我们发现镜像的漏洞，并提供修复建议。因此在生产环境下，我推荐你开启“自动镜像扫描”和“阻止潜在漏洞镜像”功能，你可以进入项目的“配置管理”菜单开启它们。 开启“自动扫描镜像”功能后，所有推送到 Harbor 的镜像都会自动执行扫描，你可以进入镜像详情查看镜像漏洞数量。 进入“Artifacts”详情可以查看漏洞详情和修复建议。 开启“阻止潜在漏洞镜像”功能之后，接下来我们尝试在本地拉取 frontend 镜像，会发现 Harbor 阻止了这个行为。 $ docker pull harbor.n7t.dev/library/frontend:8d64515 Error response from daemon: unknown: current image with 14 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Low\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE allowlist. 7. 总结 在这节课，我向你介绍了如何使用 Harbor 搭建企业级镜像仓库，在安装 Harbor 的过程中，我还简单介绍了 Helm 工具的使用方法，包括添加 Helm 仓库和安装应用。 在生产环境下，我们一般会使用 HTTPS 协议来加密访问请求，所以在安装 Harbor 之前，我向你介绍了 Cert-manager 组件，它可以帮助我们自动签发 Let’s Encrypt HTTPS 证书，并按照 Ingress 的配置生成 Secret。需要注意的是，Let’s Encrypt 证书的有效期是 90 天，不过 Cert-manager 在到期前会自动帮助我们续期，使用起来非常方便。 其次，我还介绍了如何在 Tekton Pipeline 中使用 Harbor，这里的重点是要修改 Pipeline 的仓库地址以及在 Secret 中配置的镜像仓库用户名和密码，以便 Tekton 能顺利地将镜像推送到 Harbor 仓库中。这里还有一个小细节，当我们将镜像推送到 Docker Hub 时，只需要在镜像 Tag 前面加上用户名前缀，例如 ghostwritten/frontend:latest 即可。但当我们需要将镜像推送到其他镜像仓库时，则需要把 Tag 配置为完整的地址，例如harbor.geekcloudnative.com/library/frontend:latest 才能够推送。 最后，我还给你提了 4 个 Harbor 的生产建议，你可以结合项目的实际情况来选择性地采用。到这里，自动化镜像构建这个章节就全部结束了。在接下来的课程中，我会为你介绍 Manifest 以外其他两种更高级的应用定义格式：Kustomize 和 Helm Chart。显然，这节课提到的 Cert-manager 和 Harbor 就是以 Helm Chart 的方式来定义的。同时，我也会将示例应用以这两种方式进行封装改造，带你深入了解 Kubernetes 的应用定义。 8. 思考 改造 GitHub Action Workflow 实现将镜像推送到 Harbor 中。提示：为 docker/login-action@v2 插件增加 registry 参数，并将 docker/build-push-action@v3 插件的 Tag 字段修改为包含完整 Harbor 仓库的 URL。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 14:00:39 "},"Docker/CICD/docker_Tekton_builds_image.html":{"url":"Docker/CICD/docker_Tekton_builds_image.html","title":"自托管构建 如何使用 tekton 构建镜像","keywords":"","body":"自托管构建：如何使用 Tekton 构建镜像？1. 背景2. 准备 Kubernetes 集群3. 安装组件3.1 Tekton3.2 Ingress-Nginx3.3 暴露 Tekton Dashboard4. Tekton 简介4.1 EventListener4.2 TriggerTemplate4.3 Step4.4 Task4.5 Pipeline4.6 PipelineRun5. 创建 Tekton Pipeline6. 创建 Task7. 创建 Pipeline8. 创建 EventListener9. 暴露 EventListener10. 创建 TriggerTemplate11. 创建 Service Account 和 PVC12. 设置 Secret13. 创建 GitHub Webhook14. 触发 Pipeline自托管构建：如何使用 Tekton 构建镜像？ 1. 背景 对于构建次数较少的团队来说，在免费额度范围内使用它们是一个非常好的选择。但是对于构建次数非常频繁的中大型团队来说，综合考虑费用、可控和定制化等各方面因素，他们可能会考虑使用其他自托管的方案。这节课，我们就来介绍其中一种自动构建镜像的自托管方案：使用 Tekton 来自动构建镜像。Tekton 是一款基于 Kubernetes 的 CI/CD 开源产品，如果你已经有一个 Kubernetes 集群，那么利用 Tekton 直接在 Kubernetes 上构建镜像是一个不错的选择。 我会首先带你了解 Tekton 的基本概念，然后我们仍然以示例应用为例，从零开始为示例应用配置构建镜像的流水线，并结合 GitHub 为 Tekton 配置 Webhook 触发器，实现提交代码之后触发 Tekton 流水线并构建镜像，最后推送到镜像仓库的过程。在学完这节课之后，你将基本掌握 Tekton 的流水线以及触发器的用法，并具备独立配置它们的能力。 在开始今天的学习之前，你需要具备以下前提条件。 在本地安装了 kubectl。 将 kubernetes-example 示例应用代码推送到了自己的 GitHub 仓库中。 2. 准备 Kubernetes 集群 由于我们在实践的过程中需要 Kubernetes 集群的 Loadbalancer 能力，所以，首先你需要准备一个云厂商的 Kubernetes 集群，你可以使用 AWS、阿里云或腾讯云等任何云厂商。这里我以开通腾讯云 TKE 集群为例演示一下，这部分内容比较基础，如果你已经有云厂商 Kubernetes 集群，或者熟悉开通过程，都可以跳过这个步骤。 首先，登录腾讯云并在这个页面打开 TKE 控制台，点击“新建”按钮，选择“标准集群”。 在创建集群页面输入“集群名称”，“所在地域”项中选择“中国香港”，集群网络选择“Default”，其他信息保持默认，点击下一步。 创建 集群网络 接下来进入到 Worker 节点配置阶段。在“机型”一栏中选择一个 2 核 8G 的节点，在“公网带宽”一栏中将带宽调整为 100Mbps，并且按量计费。 点击集群名称“kubernetes-1”进入集群详情页，在“集群 APIServer 信息”一栏找到“外网访问”，点击开关来开启集群的外网访问。 在弹出的新窗口中，选择“Default”安全组并选择“按使用流量”计费，访问方式选择“公网 IP”，然后点击“保存”开通集群外网访问。 等待“外网访问”开关转变为启用状态。接下来，在“Kubeconfig”一栏点击“复制”，复制集群 Kubeconfig 信息。 接下来，将集群证书信息内容写入到本地的 ~/.kube/config 文件内，这是 kubectl 默认读取 kubeconfig 的文件位置。为了避免覆盖已有的 kubeconfig，首先你需要备份 ~/.kube/config 文件。 $ mv ~/.kube/config ~/.kube/config-bak 然后新建 ~/.kube/config 文件，将刚才复制的 Kubeconfig 内容写入到该文件内。最后，执行 kubectl get node 来验证 kubectl 与集群的联通性。 $ kubectl get node NAME STATUS ROLES AGE VERSION 172.19.0.107 Ready 5m21s v1.22.5-tke.5 待 Node 信息返回后，我们的 Kubernetes 集群也就准备好了。 3. 安装组件 准备好云厂商 Kubernetes 集群之后，接下来我们需要安装两个组件，分别是 Tekton 相关的组件以及 Ingress-Nginx。 3.1 Tekton 首先，安装 Tekton Operator。 $ kubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml namespace/tekton-pipelines created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access created role.rbac.authorization.k8s.io/tekton-pipelines-controller created role.rbac.authorization.k8s.io/tekton-pipelines-webhook created role.rbac.authorization.k8s.io/tekton-pipelines-leader-election created role.rbac.authorization.k8s.io/tekton-pipelines-info created serviceaccount/tekton-pipelines-controller created serviceaccount/tekton-pipelines-webhook created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-cluster-access created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-tenant-access created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-cluster-access created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-controller-leaderelection created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-webhook-leaderelection created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-info created customresourcedefinition.apiextensions.k8s.io/clustertasks.tekton.dev created customresourcedefinition.apiextensions.k8s.io/customruns.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelines.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelineruns.tekton.dev created customresourcedefinition.apiextensions.k8s.io/resolutionrequests.resolution.tekton.dev created customresourcedefinition.apiextensions.k8s.io/pipelineresources.tekton.dev created customresourcedefinition.apiextensions.k8s.io/runs.tekton.dev created customresourcedefinition.apiextensions.k8s.io/tasks.tekton.dev created customresourcedefinition.apiextensions.k8s.io/taskruns.tekton.dev created customresourcedefinition.apiextensions.k8s.io/verificationpolicies.tekton.dev created secret/webhook-certs created validatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.pipeline.tekton.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.pipeline.tekton.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.pipeline.tekton.dev created clusterrole.rbac.authorization.k8s.io/tekton-aggregate-edit created clusterrole.rbac.authorization.k8s.io/tekton-aggregate-view created configmap/config-artifact-bucket created configmap/config-artifact-pvc created configmap/config-defaults created configmap/feature-flags created configmap/pipelines-info created configmap/config-leader-election created configmap/config-logging created configmap/config-observability created configmap/config-registry-cert created configmap/config-trusted-resources created deployment.apps/tekton-pipelines-controller created service/tekton-pipelines-controller created namespace/tekton-pipelines-resolvers created clusterrole.rbac.authorization.k8s.io/tekton-pipelines-resolvers-resolution-request-updates created role.rbac.authorization.k8s.io/tekton-pipelines-resolvers-namespace-rbac created serviceaccount/tekton-pipelines-resolvers created clusterrolebinding.rbac.authorization.k8s.io/tekton-pipelines-resolvers created rolebinding.rbac.authorization.k8s.io/tekton-pipelines-resolvers-namespace-rbac created configmap/bundleresolver-config created configmap/cluster-resolver-config created configmap/resolvers-feature-flags created configmap/config-leader-election created configmap/config-logging created configmap/config-observability created configmap/git-resolver-config created configmap/hubresolver-config created deployment.apps/tekton-pipelines-remote-resolvers created horizontalpodautoscaler.autoscaling/tekton-pipelines-webhook created deployment.apps/tekton-pipelines-webhook created service/tekton-pipelines-webhook created 等待 Tekton 所有的 Pod 就绪 $ kubectl wait --for=condition=Ready pods --all -n tekton-pipelines --timeout=300s pod/tekton-pipelines-controller-799f9f989b-hxmlx condition met pod/tekton-pipelines-webhook-556f9f7476-sgx2n condition met 接下来，安装 Tekton Dashboard。 $ kubectl apply --filename https://storage.googleapis.com/tekton-releases/dashboard/latest/release.yaml customresourcedefinition.apiextensions.k8s.io/extensions.dashboard.tekton.dev created serviceaccount/tekton-dashboard created role.rbac.authorization.k8s.io/tekton-dashboard-info created clusterrole.rbac.authorization.k8s.io/tekton-dashboard-backend created clusterrole.rbac.authorization.k8s.io/tekton-dashboard-tenant created rolebinding.rbac.authorization.k8s.io/tekton-dashboard-info created clusterrolebinding.rbac.authorization.k8s.io/tekton-dashboard-backend created configmap/dashboard-info created service/tekton-dashboard created deployment.apps/tekton-dashboard created clusterrolebinding.rbac.authorization.k8s.io/tekton-dashboard-tenant created 然后，分别安装 Tekton Trigger 和 Tekton Interceptors 组件。 $ kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml clusterrole.rbac.authorization.k8s.io/tekton-triggers-admin created clusterrole.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created clusterrole.rbac.authorization.k8s.io/tekton-triggers-core-interceptors-secrets created clusterrole.rbac.authorization.k8s.io/tekton-triggers-eventlistener-roles created clusterrole.rbac.authorization.k8s.io/tekton-triggers-eventlistener-clusterroles created role.rbac.authorization.k8s.io/tekton-triggers-admin-webhook created role.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created role.rbac.authorization.k8s.io/tekton-triggers-info created serviceaccount/tekton-triggers-controller created serviceaccount/tekton-triggers-webhook created serviceaccount/tekton-triggers-core-interceptors created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-controller-admin created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-webhook-admin created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors-secrets created rolebinding.rbac.authorization.k8s.io/tekton-triggers-webhook-admin created rolebinding.rbac.authorization.k8s.io/tekton-triggers-core-interceptors created rolebinding.rbac.authorization.k8s.io/tekton-triggers-info created customresourcedefinition.apiextensions.k8s.io/clusterinterceptors.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/clustertriggerbindings.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/eventlisteners.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/interceptors.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggers.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggerbindings.triggers.tekton.dev created customresourcedefinition.apiextensions.k8s.io/triggertemplates.triggers.tekton.dev created secret/triggers-webhook-certs created validatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.triggers.tekton.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.triggers.tekton.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.triggers.tekton.dev created clusterrole.rbac.authorization.k8s.io/tekton-triggers-aggregate-edit created clusterrole.rbac.authorization.k8s.io/tekton-triggers-aggregate-view created configmap/config-defaults-triggers created configmap/feature-flags-triggers created configmap/triggers-info created configmap/config-logging-triggers created configmap/config-observability-triggers created service/tekton-triggers-controller created deployment.apps/tekton-triggers-controller created service/tekton-triggers-webhook created deployment.apps/tekton-triggers-webhook created $ kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml deployment.apps/tekton-triggers-core-interceptors created service/tekton-triggers-core-interceptors created clusterinterceptor.triggers.tekton.dev/cel created clusterinterceptor.triggers.tekton.dev/bitbucket created clusterinterceptor.triggers.tekton.dev/github created clusterinterceptor.triggers.tekton.dev/gitlab created secret/tekton-triggers-core-interceptors-certs created 等待所有 Tekton 的所有组件的 Pod 都处于就绪状态，Tekton 就部署完成了。 $ kubectl wait --for=condition=Ready pods --all -n tekton-pipelines --timeout=300s pod/tekton-dashboard-5d94c7f687-8t6p2 condition met pod/tekton-pipelines-controller-799f9f989b-hxmlx condition met pod/tekton-pipelines-webhook-556f9f7476-sgx2n condition met pod/tekton-triggers-controller-bffdd47cf-cw7sv condition met pod/tekton-triggers-core-interceptors-5485b8bd66-n9n2m condition met pod/tekton-triggers-webhook-79ddd8d6c9-f79tg condition met 3.2 Ingress-Nginx 安装完 Tekton 之后，我们再来安装 Ingress-Nginx。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.4.0/deploy/static/provider/cloud/deploy.yaml serviceaccount/ingress-nginx created serviceaccount/ingress-nginx-admission created role.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created configmap/ingress-nginx-controller created service/ingress-nginx-controller created service/ingress-nginx-controller-admission created deployment.apps/ingress-nginx-controller created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created ingressclass.networking.k8s.io/nginx created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created 等待所有 Ingress-Nginx Pod 处于就绪状态，Ingress-Nginx 就部署完成了。 $ kubectl wait --for=condition=AVAILABLE deployment/ingress-nginx-controller --all -n ingress-nginx deployment.apps/ingress-nginx-controller condition met 查看所有pod kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE default kubernetes-proxy-6fdb4f8968-9wl6r 1/1 Running 0 22m default kubernetes-proxy-6fdb4f8968-phz5p 1/1 Running 0 22m ingress-nginx ingress-nginx-admission-create-ff577 0/1 Completed 0 8m38s ingress-nginx ingress-nginx-admission-patch-m96ww 0/1 Completed 0 8m38s ingress-nginx ingress-nginx-controller-598d7c8b88-4pdrg 1/1 Running 0 8m38s kube-system coredns-67778c7987-dcvks 0/1 Pending 0 30m kube-system coredns-67778c7987-t9f4m 1/1 Running 0 30m kube-system csi-cbs-controller-5c5699748c-p8bbl 6/6 Running 0 29m kube-system csi-cbs-node-fvc78 2/2 Running 0 27m kube-system ip-masq-agent-kknln 1/1 Running 0 27m kube-system kube-proxy-5h97x 1/1 Running 0 27m kube-system l7-lb-controller-684767cf57-rldtl 1/1 Running 0 30m kube-system tke-bridge-agent-gmw7v 1/1 Running 1 (27m ago) 27m kube-system tke-cni-agent-dspl2 1/1 Running 0 27m kube-system tke-monitor-agent-w75lq 1/1 Running 0 27m tekton-pipelines-resolvers tekton-pipelines-remote-resolvers-9b9cfd554-wclj4 1/1 Running 0 14m tekton-pipelines tekton-dashboard-7f4d9fdf85-2sndc 1/1 Running 0 11m tekton-pipelines tekton-pipelines-controller-549f96d645-v7gb2 1/1 Running 0 15m tekton-pipelines tekton-pipelines-webhook-76df49fcc9-chnhk 1/1 Running 0 14m tekton-pipelines tekton-triggers-controller-6586b9d989-r4nd2 1/1 Running 0 10m tekton-pipelines tekton-triggers-core-interceptors-fbbdd8bdd-tkrm6 1/1 Running 0 10m tekton-pipelines tekton-triggers-webhook-cd76d447f-4ddsw 1/1 Running 0 10m 3.3 暴露 Tekton Dashboard 配置好 Tekton 和 Ingress-Nginx 之后，为了方便访问 Tekton Dashboard，我们要通过 Ingress 的方式暴露它，将下列内容保存为 tekton-dashboard.yaml。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-resource namespace: tekton-pipelines annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: rules: - host: tekton.k8s.local http: paths: - path: / pathType: Prefix backend: service: name: tekton-dashboard port: number: 9097 然后，执行 kubectl apply 将它应用到集群内。 $ kubectl apply -f tekton-dashboard.yaml ingress.networking.k8s.io/ingress-resource created 接下来，获取 Ingress-Nginx Loadbalancer 的外网 IP 地址。你可以使用 kubectl get service 来获取。 $ kubectl get services --namespace ingress-nginx ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' 43.135.82.249 由于之前我在 Ingress 策略中配置的是一个虚拟域名，所以需要在本地配置 Hosts。当然你也可以将 Ingress 的 host 修改为实际的域名，并且为域名添加 DNS 解析，也能达到同样的效果。 以 Linux 系统为例，要修改 Hosts，你需要将下面的内容添加到 /etc/hosts 文件内。 43.135.82.249 tekton.k8s.local 然后，你就可以通过域名 http://tekton.k8s.local 来访问 Tekton Dashboard 了。 4. Tekton 简介 在正式创建 Tekton 流水线之前，你需要先了解一些基本概念，你可以结合下面这张图来理解一下。 在上面这张图中，从左往右依次出现了这几个概念：EventListener、TriggerTemplate、PipelineRun、Pipeline、Task 以及 Step，接下来，我就简单介绍一下它们。 4.1 EventListener EventListener 顾名思义是一个事件监听器，它是外部事件的入口。EventListener 通常以 HTTP 的方式对外暴露，在我们这节课的例子中，我们会在 GitHub 创建 WebHook 来调用 Tekton 的 EventListener，使它能接收到仓库推送事件。 4.2 TriggerTemplate 当 EventListener 接收到外部事件之后，它会调用 Trigger 也就是触发器，而 TriggerTemplate 是用来定义接收到事件之后需要创建的 Tekton 资源的，例如创建一个 PipelineRun 对象来运行流水线。这节课，我们会使用 TriggerTemplate 来创建 PipelineRun 资源。 4.3 Step Step 是流水线中的一个具体的操作，例如构建和推送镜像操作。Step 接收镜像和需要运行的 Shell 脚本作为参数，Tekton 将会启动镜像并执行 Shell 脚本。 4.4 Task Task 是一组有序的 Step 集合，每一个 Task 都会在独立的 Pod 中运行，Task 中不同的 Step 将在同一个 Pod 不同的容器内运行。 4.5 Pipeline Pipeline 是 Tekton 中的一个核心组件，它是一组 Task 的集合，Task 将组成一组有向无环图（DAG），Pipeline 会按照 DAG 的顺序来执行。 4.6 PipelineRun PipelineRun 实际上是 Pipeline 的实例化，它负责为 Pipeline 提供输入参数，并运行 Pipeline。例如，两次不同的镜像构建操作对应的就是两个不同的 PipelineRun 资源。 5. 创建 Tekton Pipeline 可以看出，Tekton 的概念确实比较多，抽象也不好理解。别担心，接下来我们就实际创建一下流水线，在这个过程中不断加深理解。 创建的 Tekton 流水线最终可以实现的效果，如下图所示。 简单来说，当我们向 GitHub 推送代码时，GitHub 将以 HTTP 请求的方式通知集群内的 Tekton 触发器，触发器通过 Ingress-Nginx 对外暴露，当触发器接收到来自 GitHub 的事件推送时，将通过 TriggerTemplate 来创建 PipelineRun 运行 Pipeline，最终实现镜像的自动构建和推送。 6. 创建 Task 好，下面我们正式开始实战。在创建 Pipeline 之前，我们需要先创建两个 Task，这两个 Task 分别负责“检出代码”还有“构建和推送镜像”。 首先创建检出代码的 Task。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/task/git-clone.yaml task.tekton.dev/git-clone created 这个 Task 是 Tekton 官方提供的插件，它和 GitHub Action 的 checkout 插件有一点类似，主要作用是检出代码。在这里，我们不需要理解它具体的细节。 然后，创建构建和推送镜像的 Task。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/task/docker-build.yaml task.tekton.dev/docker-socket configured 简单介绍一个这个 Task，关键内容如下。 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: docker-socket spec: workspaces: - name: source params: - name: image description: Reference of the image docker will produce. ...... steps: - name: docker-build image: docker:stable env: ...... - name: IMAGE value: $(params.image) - name: DOCKER_PASSWORD valueFrom: secretKeyRef: name: registry-auth key: password - name: DOCKER_USERNAME valueFrom: secretKeyRef: name: registry-auth key: username workingDir: $(workspaces.source.path) script: | cd $SUBDIRECTORY docker login $REGISTRY_URL -u $DOCKER_USERNAME -p $DOCKER_PASSWORD if [ \"${REGISTRY_URL}\" = \"docker.io\" ] ; then docker build --no-cache -f $CONTEXT/$DOCKERFILE_PATH -t $DOCKER_USERNAME/$IMAGE:$TAG $CONTEXT docker push $DOCKER_USERNAME/$IMAGE:$TAG exit fi docker build --no-cache -f $CONTEXT/$DOCKERFILE_PATH -t $REGISTRY_URL/$REGISTRY_MIRROR/$IMAGE:$TAG $CONTEXT docker push $REGISTRY_URL/$REGISTRY_MIRROR/$IMAGE:$TAG volumeMounts: # 共享 docker.socket - mountPath: /var/run/ name: dind-socket sidecars: #sidecar 提供 docker daemon - image: docker:dind ...... spec.params 字段用来定义变量，并最终由 PipelineRun 提供具体的值。 spec.steps 字段用来定义具体的执行步骤，例如，我们在这里使用 docker:stable 镜像创建了容器，并将 spec.params 定义的变量以 ENV 的方式传递到容器内部，其中 DOCKER_PASSWORD 和 DOCKER_USERNAME 两个变量来源于 Secret，我们将在后续创建。 spce.steps[0].script 字段定义了具体执行的命令，这里执行了 docker login 登录到 Docker Hub，并且使用了 docker build 和 docker push 来构建和推送镜像。我们对 Docker Hub 和其他镜像仓库做了区分，以便使用不同的 TAG 命名规则。 spce.sidecars 字段为容器提供 Docker daemon，它使用的镜像是 docker:dind。 仔细回想一下上节课的内容你会发现，这个 Task 定义的具体行为和 GitLab CI 定义的流水线非常类似，它们都是指定一个镜像，然后运行一段脚本，并且都是用 DiND 的方式来构建和推送镜像的。 7. 创建 Pipeline 创建完 Task 之后，由于它们实现的具体功能是独立的，所以我们需要将他们联系起来。也就是说，我们希望 Pipeline 先克隆代码，再构建和推送镜像。所以，下面我们需要创建 Pipeline 来引用这两个 Task。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/pipeline/pipeline.yaml pipeline.tekton.dev/github-trigger-pipeline created 这里我也简单介绍一下这个 Pipeline。 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: github-trigger-pipeline spec: workspaces: - name: pipeline-pvc ...... params: - name: subdirectory # 为每一个 Pipeline 配置一个 workspace，防止并发错误 type: string default: \"\" - name: git_url ...... tasks: - name: clone taskRef: name: git-clone workspaces: - name: output workspace: pipeline-pvc - name: ssh-directory workspace: git-credentials params: - name: subdirectory value: $(params.subdirectory) - name: url value: $(params.git_url) - name: build-and-push-frontend taskRef: name: docker-socket runAfter: - clone workspaces: - name: source workspace: pipeline-pvc params: - name: image value: \"frontend\" ...... - name: build-and-push-backend taskRef: name: docker-socket runAfter: - clone workspaces: - name: source workspace: pipeline-pvc params: - name: image value: \"backend\" ...... 首先，spec.workspaces 定义了一个工作空间。还记得我们提到的每一个 Task 都会在独立的 Pod 中运行吗？那么不同的 Task 如何共享上下文呢？答案就是 workspaces。实际上它是一个 PVC 持久化卷，这个 PVC 将会在 Pod 之间复用，这就让下游 Task 可以读取到上游 Task 写入的数据（比如克隆的代码）。 spce.params 定义了 Pipeline 的参数，参数的传递顺序是：PipelineRun->Pipeline->Task。 spce.tasks 定义了 Pipeline 引用的 Task，例如在这里分别引用了 git-clone 和 docker-socket 两个 Task，并且都指定了同一个 workspaces pipeline-pvc，然后指定了 params 向 Task 传递了参数值。 在 build-and-push-frontend 和 build-and-push-backend Task 中，都指定了 runAfter 字段，它的含义是等待 clone Task 执行完毕后再运行。 所以，Pipeline 对 Task 的引用就形成了一个有向无环图（DAG），在这个 Pipeline 中，首先会检出源码，然后以并行的方式同时构建前后端镜像。 8. 创建 EventListener 创建完 Pipeline 之后，工作流实际上就已经定义好了。但是我们并不希望手动来运行它，我们希望通过 GitHub 来自动触发它。所以，接下来需要创建 EventListener 来获得一个能够监听外部事件的服务。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/trigger/github-event-listener.yaml eventlistener.triggers.tekton.dev/github-listener created EventListener 的具体作用是：接收来自 GitHub 的 Webhook 调用，并将 Webhook 的参数和 TriggerTemplate 定义的参数对应起来，以便将参数值从 Webhook 一直传递到 PipelineRun。 9. 暴露 EventListener 在 EventListener 创建完成之后，Tekton 将会拉起一个 Deployment 用来处理 Webhook 请求，你可以通过 kubectl get deployment 命令来查看。 $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE el-github-listener 1/1 1 1 22m 同时，Tekton 也会为 el-github-listener Deployment 创建 Service，以便接受来自外部的 HTTP 请求。 $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE el-github-listener ClusterIP 172.16.253.54 8080/TCP,9000/TCP 4h23m 为了能够让 GitHub 将事件推送到 Tekton 中，我们需要暴露 el-github-listener Service。我使用了 Ingress-Nginx 来对外暴露它。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/ingress/github-listener.yaml ingress.networking.k8s.io/ingress-resource created 这个 Ingress 的内容比较简单，具体如下。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-resource annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-redirect: \"false\" spec: rules: - http: paths: - path: /hooks pathType: Exact backend: service: name: el-github-listener port: number: 8080 在 Ingress 策略中，我们没有使用 host 定义域名，而使用了 path 来匹配路径。这样，Tekton 接收外部 Webhook 的入口也就是 Ingress-Nginx 的负载均衡器 IP 地址了，具体的地址为 http://43.135.82.249/hooks。 10. 创建 TriggerTemplate 不过 EventListener 并不能独立工作，它还需要一个助手，那就是 TriggerTemplate。TriggerTemplate 是真正控制 Pipeline 启动的组件，它负责创建 PipelineRun。 我们可以通过下面的命令来创建 TriggerTemplate。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/trigger/github-trigger-template.yaml triggertemplate.triggers.tekton.dev/github-template created 11. 创建 Service Account 和 PVC 下一步，由于触发器并没有具体的执行用户，所以我们还需要为触发器配置权限，也就是创建 Service Account。同时，我们也可以一并创建用于共享 Task 之间上下文的 PVC 。 $ kubectl apply -f https://ghproxy.com/https://raw.githubusercontent.com/lyzhang1999/gitops/main/ci/18/tekton/other/service-account.yaml serviceaccount/tekton-build-sa created clusterrolebinding.rbac.authorization.k8s.io/tekton-clusterrole-binding created persistentvolumeclaim/pipeline-pvc created role.rbac.authorization.k8s.io/tekton-triggers-github-minimal created rolebinding.rbac.authorization.k8s.io/tekton-triggers-github-binding created clusterrole.rbac.authorization.k8s.io/tekton-triggers-github-clusterrole created clusterrolebinding.rbac.authorization.k8s.io/tekton-triggers-github-clusterbinding created 12. 设置 Secret 最后，我们还需要为 Tekton 提供一些凭据信息，例如 Docker Hub Token、GitHub Webhook Secret 以及用于检出私有仓库的私钥信息。 将下面的内容保存为 secret.yaml，并修改相应的内容。 apiVersion: v1 kind: Secret metadata: name: registry-auth annotations: tekton.dev/docker-0: https://docker.io type: kubernetes.io/basic-auth stringData: username: \"\" # docker username password: \"\" # docker hub token --- apiVersion: v1 kind: Secret metadata: name: github-secret type: Opaque stringData: secretToken: \"webhooksecret\" --- apiVersion: v1 kind: Secret metadata: name: git-credentials data: id_rsa: LS0tLS...... known_hosts: Z2l0aHViLm...... config: SG9zd...... 解释一下，你主要需要修改的是下面这几个字段。 将 stringData.username 替换为你的 Docker Hub 的用户名。 将 stringData.password 替换为你的 Docker Hub Token 将 data.id_rsa 替换为你本地 ~/.ssh/id_rsa 文件的 base64 编码内容，这将会为 Tekton 提供检出私有仓库的权限，你可以使用 $ cat ~/.ssh/id_rsa | base64 命令来获取。 将 data.known_hosts 替换为你本地 ~/.ssh/known_hosts 文件的 base64 编码内容，你可以通过 $ cat ~/.ssh/known_hosts | grep \"github\" | base64 命令来获取。 将 data.config 替换为你本地 ~/.ssh/config 文件的 base64 编码内容，你可以通过 $ cat ~/.ssh/config | base64 命令来获取。 然后，运行 kuebctl apply，同时将这 3 个 Secret 应用到集群内。 $ kubectl apply -f secret.yaml secret/registry-auth created secret/github-secret created secret/git-credentials created 13. 创建 GitHub Webhook 到这里，Tekton 的配置就已经完成了。接下来还剩最后一步：创建 GitHub Webhook。 打开你在 GitHub 创建的 kubernetes-example 仓库，进入“Settings”页面，点击左侧的“Webhooks”菜单，在右侧的页面中按照下图进行配置。 输入 Webhook 地址，也就是“ Ingress-Nginx 网关的负载均衡器地址 + hooks 路径”，并且将“Content type”配置为“application/json”。点击“Add webhook”创建。 14. 触发 Pipeline 到这里，所有的准备工作就已经完成了。现在我们向仓库提交一个空的 commit 来触发 Pipeline $ git commit --allow-empty -m \"Trigger Build\" [main 79ca67e] Trigger Build $ git push origin main 完成推送后，打开 http://tekton.k8s.local/ 进入 Tekton 控制台，点击左侧的“PipelineRun”，你会看到刚才触发的 Pipeline。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 14:01:23 "},"Docker/CICD/docker_GitLab_CI_builds_image.html":{"url":"Docker/CICD/docker_GitLab_CI_builds_image.html","title":"如何使用 gitlab ci 构建镜像","keywords":"","body":"如何使用 GitLab CI 构建镜像？1. 前言2. GitLab CI 简介2.1 Pipeline2.2 Stage2.3 Job3. 费用4. 创建 GitLab CI Pipeline4.1 创建 .gitlab-ci.yml 文件4.2 创建 GitLab 仓库并推送4.3 创建 Docker Hub Secret4.4 创建 GitLab CI Variables4.5 触发 GitLab CI Pipeline5. 构建 linux/amd64 和 linux/arm64 两个平台的镜像报错6. 总结如何使用 GitLab CI 构建镜像？ 1. 前言 在上一节课，我们学习了如何使用 GitHub Action 自动构建镜像，我们通过为示例应用配置 GitHub Action 工作流，实现了自动构建，并将镜像推送到了 Docker Hub 镜像仓库。但是，要使用 GitHub Action 构建镜像，前提条件是你需要使用 GitHub 作为代码仓库，那么，如果我所在的团队使用的是 GitLab 要怎么做呢？这节课，我会带你学习如何使用 GitLab CI 来自动构建镜像。我还是以示例应用为例，使用 SaaS 版的 GitLab 从零配置 CI 流水线。需要注意的是，有些团队是以自托管的方式来使用 GitLab 的，也就是我们常说的私有部署的方式，它和 SaaS 版本的差异不大。如果你用的是私有化部署版本，同样可以按照这节课的流程来实践。 2. GitLab CI 简介 在正式使用 GitLab CI 之前，你需要先了解一些基本概念，你可以结合下面这张图来理解。 这张图中出现了 Pipeline、Stage 和 Job 这几个概念，接下来我们分别了解一下。 2.1 Pipeline Pipeline 指的是流水线，在 GitLab 中，当有新提交推送到仓库中时，会自动触发流水线。流水线包含一组 Stage 和 Job 的定义，它们负责执行具体的逻辑。在 GitLab 中，Pipeline 是通过仓库根目录下的 .gitlab-ci.yml 文件来定义的。此外，Pipeline 在全局也可以配置运行镜像、全局变量和额外服务镜像。 2.2 Stage Stage 字面上的意思是“阶段”。在 GitLab CI 中，至少需要包含一个 Stage，上面这张图中有三个 Stage，分别是 Stage1、Stage2 和 Stage3，不同的 Stage 是按照定义的顺序依次执行的。如果其中一个 Stage 失败，则整个 Pipeline 都将失败，后续的 Stage 也都不会再继续执行。 2.3 Job Job 字面上的意思是“任务”。实际上，Job 的作用是定义具体需要执行的 Shell 脚本，同时，Job 可以被关联到一个 Stage 上。当 Stage 执行时，它所关联的 Job 也会并行执行。以自动构建镜像为例，我们可能需要在 1 个 Job 中定义 2 个 Shell 脚本步骤，它们分别是： 运行 docker build 构建镜像 运行 docker push 来推送镜像 3. 费用 和 GitHub Action 一样，GitLab 也不能无限免费使用。对于 GitLab 免费账户，每个月有 400 分钟的 GitLab CI/CD 时长可供使用，超出时长则需要按量付费，你可以在这里查看详细的计费策略。 4. 创建 GitLab CI Pipeline 在这个例子中，我们创建的流水线将实现以下这些步骤。 运行 docker login 登录到 Docker Hub。 运行 docker build 来构建前后端应用的镜像。 运行 docker push 推送镜像。 接下来，我们开始创建 GitLab CI Pipeline。 4.1 创建 .gitlab-ci.yml 文件 首先，将示例应用仓库克隆到本地。 $ git clone https://github.com/lyzhang1999/kubernetes-example.git 进入 kubernetes-example 目录。 $ cd kubernetes-example 然后，将下面的内容保存到 .gitlab-ci.yml 文件内。 stages: - build image: docker:20.10.16 variables: DOCKER_TLS_CERTDIR: \"/certs\" DOCKERHUB_USERNAME: \"ghostwritten\" services: - docker:20.10.16-dind before_script: - docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_TOKEN build_and_push: stage: build script: - docker build -t $DOCKERHUB_USERNAME/frontend:$CI_COMMIT_SHORT_SHA ./frontend - docker push $DOCKERHUB_USERNAME/frontend:$CI_COMMIT_SHORT_SHA - docker build -t $DOCKERHUB_USERNAME/backend:$CI_COMMIT_SHORT_SHA ./backend - docker push $DOCKERHUB_USERNAME/backend:$CI_COMMIT_SHORT_SHA 请注意，你需要将上面的 variables.DOCKERHUB_USERNAME 环境变量替换为你的 Docker Hub 用户名。 下来，结合上面提到的概念，我简单介绍一下这个 Pipeline。 stages 字段定义了阶段，在这个 Pipeline 中，我们定义了一个 build 阶段。 image 字段定义了运行镜像，也就是说，GitLab CI 将会使用 docker:20.10.16 镜像来启动一个容器，并在容器内运行 Pipeline。 variables 字段定义了全局变量，其中， DOCKER_TLS_CERTDIR 变量是用来共享 Docker 证书的。DOCKERHUB_USERNAME 变量是 Docker Hub 的用户名。 services 字段定义了一个额外的镜像，你可以把它理解成一个额外的容器，它将和 image 字段定义的容器相互协作，这两个容器可以相互访问。 before_script 定义了 Pipeline 最开始的 Shell 脚本， 它将会在 Job 运行之前执行。在这里，我们运行了 docker login 命令来登录到 Docker Hub，以便获得推送镜像的权限。请注意，$DOCKERHUB_USERNAME 变量的值来源于我们在 variables 定义的值，$DOCKERHUB_TOKEN 是一个在 GitLab UI 界面定义的变量，我们稍后会在 GitLab 平台添加。 build_and_push 字段定义了一个 Job，“build_and_push” 实际上是 Job 的名称，你也可以更改这个名称。build_and_push.stage 字段定义了 Job 所属的 Stage，也就是 build 阶段。build_and_push.script 字段定义了执行的具体的 Shell 脚本，它们是按顺序执行的。在这里，我们分别构建了 frontend 和 backend 的镜像，并将它们推送到 Docker Hub 仓库。其中，$CI_COMMIT_SHORT_SHA 是一个内置变量，它可以获取到当前的 short commit id。 4.2 创建 GitLab 仓库并推送 创建完 .gitlab-ci.yml 文件后，接下来我们将示例应用推送到 GitLab 上。首先，你需要通过这个页面来为你自己创建新的代码仓库，仓库名设置为 kubernetes-example。 创建完成后，将刚才克隆的 kubernetes-example 仓库的 remote url 配置为你刚才创建仓库的 Git 地址。 $ git remote set-url origin YOUR_GITLAB_REPO_URL 然后，将 kubernetes-example 推送到你的 GitLab 仓库中。在这之前，你可能需要配置 SSH Key，你可以参考这个链接来配置，这里就不再赘述了 $ git add . $ git commit -a -m 'first commit' $ git branch -M main $ git push -u origin main 获取 gitlab acess token 4.3 创建 Docker Hub Secret 创建完 .gitlab-ci.yml 文件后，接下来我们需要创建 Docker Hub Secret，它将会为工作流提供推送镜像的权限。 首先，使用你注册的账号密码登录 https://hub.docker.com/。然后，点击右上角的“用户名”，选择“Account Settings”，并进入左侧的“Security”菜单。 然后，点击右侧的“New Access Token”按钮创建一个新的 Token。 输入描述，然后点击“Genarate”按钮生成 Token。 点击“Copy and Close”将 Token 复制到剪贴板。请注意，一旦窗口关闭，我们就无法再次查看这个 Token 了，所以请务必复制并在其他地方保存下来。 4.4 创建 GitLab CI Variables 创建完 Docker Hub Token 之后，我们就可以创建 GitLab CI Variables 了，也就是要为 Pipeline 提供 DOCKERHUB_TOKEN 变量值。 进入 kubernetes-example 仓库的 Settings 页面，点击左侧的“CI/CD”，然后点击右侧的“Variables”展开菜单，接着点击“Add variable”来创建新的 Variables。如下图所示。 在弹出的输入框中，将 Key 填写为 DOCKERHUB_TOKEN。 将 Value 填写为刚才我们复制的 Docker Hub Token，其他选项保持默认，点击“Add variable”创建变量值，如下图所示。 4.5 触发 GitLab CI Pipeline 到这里，准备工作已经全部完成了。请注意，如果你使用的是 GitLab SaaS 版，那么你需要先绑定信用卡才能使用 CI/CD 的免费额度。接下来我们尝试触发 GitLab CI Pipeline。首先，向仓库提交一个空 commit。 git commit --allow-empty -m \"Trigger Build\" 然后，使用 git push 来推送到仓库，这将触发 Pipeline。 git push origin main 接下来，进入 kubernetes-example 仓库的“CI/CD”页面，你会看到我们刚才触发的流水线。 需要visa 验证身份 重新运行 pipeline 你可以点击流水线的状态（running）进入流水线详情页面。 在流水线的详情页面，我们能看到流水线的每一个 Job 的状态还有运行时输出的日志。当工作流运行完成后，进入到 Docker Hub frontend 或者 backend 镜像的详情页，你将看到刚才 GitLab CI 自动构建并推送的新版本镜像。 到这里，我们便完成了使用 GitLab CI 自动构建镜像。最终实现的效果是，当我们向仓库推送新的提交时，GitLab 流水线将自动构建 frontend 和 backend 镜像，并且每一个 commit id 都会对应一个镜像版本。 5. 构建 linux/amd64 和 linux/arm64 两个平台的镜像 编辑 .gitlab-ci.yml stages: - build image: docker:20.10.16 variables: DOCKER_TLS_CERTDIR: \"/certs\" DOCKERHUB_USERNAME: \"ghostwritten\" PLATFORM: \"linux/amd64,linux/arm64\" services: - docker:20.10.16-dind before_script: - docker context create builder - docker buildx create --name builder --use builder - docker buildx use builder - docker buildx inspect --bootstrap - docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_TOKEN build_and_push: stage: build script: - docker buildx build --platform $PLATFORM -t $DOCKERHUB_USERNAME/frontend:$CI_COMMIT_SHORT_SHA ./frontend --push - docker buildx build --platform $PLATFORM -t $DOCKERHUB_USERNAME/backend:$CI_COMMIT_SHORT_SHA ./backend --push 提交至gitlab git add . git commit -m \"build linux/amd64 and linux/arm64 images\" git push origin main 查看推送效果 报错 报错1 $ docker buildx create --name builder error: could not create a builder instance with TLS data loaded from environment. Please use `docker context create ` to create a context for current environment and then create a builder instance with `docker buildx create ` Cleaning up project directory and file based variables 00:01 ERROR: Job failed: exit code 1 解决方法： - docker context create builder - docker buildx create --name builder --use builder 报错2 $ docker push $DOCKERHUB_USERNAME/frontend:$CI_COMMIT_SHORT_SHA The push refers to repository [docker.io/ghostwritten/frontend] An image does not exist locally with the tag: ghostwritten/frontend Cleaning up project directory and file based variables 00:01 ERROR: Job failed: exit code 1 解决方法： - docker buildx build --platform $PLATFORM -t $DOCKERHUB_USERNAME/frontend:$CI_COMMIT_SHORT_SHA ./frontend --push 6. 总结 在这节课，我为你介绍了如何使用 GitLab CI 来自动构建镜像，并讲解了 Pipeline、Stage 和 Job 几个重要概念。 GitLab CI 是通过在仓库根目录创建 .gitlab-ci.yml 文件来定义流水线的，这和 GitHub 有明显的差异。在这节课的例子中，.gitlab-ci.yml 文件定义的内容也相对简单，它基本上和我们在本地构建镜像所运行的命令以及顺序是一致的。 此外，相比较 GitHub Action Workflow，GitLab CI 省略了触发器和检出代码的配置步骤，并且，在 GitLab CI 中我们是通过 DiND 的方式来运行流水线的，也就是在容器的运行环境下启动另一个容器来运行流水线，而 GitHub Action 则是通过虚拟机的方式来运行流水线。 和 GitHub Action 相比较，它们除了流水线文件内容不一样以外，其他的操作例如创建 GitLab 仓库、创建 Docker Hub Secret 以及创建 GitLab CI Variables 等步骤都是差不多的。 最终，当我们有新的推送到仓库时，GitLab CI 将运行自动构建镜像的流水线，并且每次提交的 commit id 都会对应一个镜像版本，和 GitHub Action Workflow 一样，也实现了代码版本和制品版本的对应关系。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 14:01:37 "},"Docker/Command/":{"url":"Docker/Command/","title":"Command","keywords":"","body":"docker 命令1. 镜像1.1 拉取 (docker pull)1.2 查看配置信息 (docker inspect)1.3 修改tag (docker tag)1.4 打包与解包 (docker save|load)1.5 推送 (docker push)将本地的镜像推送至公共镜像源的自己仓库，一般区别官方镜像，为私人定制。ghostwritten为我的仓库名1.6 删除 (docker rmi)1.7 构建（docker build）2 容器2.1 创建容器 (docker run)2.2 查看容器状态 (docker ps)2.3 查看容器日志 (docker logs)2.4 查看容器配置信息 (docker inspect)2.5 查看容器内文件操作（docker diff）2.5 重命名容器 (docker rename)2.6 将容器构建镜像 (docker commit)2.6 打包与解包 (docker export / import)2.7 启动停止容器 (docker start / stop)2.8 删除容器(docker rm)2.9 更新容器运行参数（docker update）3.0 容器逻辑卷管理 (docker volume)3.1 容器网络管理 (docker network)docker servicedocker 命令 1. 镜像 dockerhub官网：https://hub.docker.com/ prometheus镜像为示例 1.1 拉取 (docker pull) $ docker search prom/prometheus #搜索镜像 $ docker pull prom/prometheus #默认版本latest $ docker pull docker.io/prom/prometheus #默认版本latest $ docker pull docker.io/prom/prometheus:2.3.1 # 指定版本 $ docker images #查看镜像列表 $ docker images -a #标签得镜像也能被展示 1.2 查看配置信息 (docker inspect) $ docker inspect prom/prometheus:latest 1.3 修改tag (docker tag) $ docker tag prom/prometheus:latest prom/prometheus:v1.0 #修改版本 $ docker tag prom/prometheus:latest docker.registry.localhost/prometheus:latest #修改仓库名，修改自己的私有仓库docker.registry.localhost（自定义） 1.4 打包与解包 (docker save|load) 打包 $ docker save -o prometheus.tar prom/prometheus:latest #第一种方式打包 $ docker save > prometheus.tar prom/prometheus:latest #第二种方式打包 $ docker save -o monitor.tar prom/prometheus:latest prom/alertmanager:latest # 多个镜像打包 $ docker save prom/prometheus:latest | gzip -> prometheus.tar.gz 解包 $ docker load -i prometheus.tar #第一种方式解包 $ docker load 1.5 推送 (docker push) 将本地的镜像推送至公共镜像源的自己仓库，一般区别官方镜像，为私人定制。ghostwritten为我的仓库名 $ docker push docker.io/ghostwritten/prometheus:latest 将本地的镜像推送至本地搭建的私有仓库，一般为内网集群环境公用。搭建私有仓库请点击 $ docker push docker.registry.localhost/prometheus:latest $ docker push docker.registry.localhost/monitor/prometheus:latest #加monitor tag方便区分镜像类型 1.6 删除 (docker rmi) $ docker rmi prom/prometheus:latest $ docker rmi -f prom/prometheus:latest #-f 为强制删除 1.7 构建（docker build） docker build命令会根据Dockerfile文件及上下文构建新Docker镜像。构建上下文是指Dockerfile所在的本地路径或一个URL（Git仓库地址）。构建上下文环境会被递归处理，所以，构建所指定的路径还包括了子目录，而URL还包括了其中指定的子模块。 OPTIONS说明： --build-arg=[] :设置镜像创建时的变量； --cpu-shares :设置 cpu 使用权重； --cpu-period :限制 CPU CFS周期； --cpu-quota :限制 CPU CFS配额； --cpuset-cpus :指定使用的CPU id； --cpuset-mems :指定使用的内存 id； --disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； --force-rm :设置镜像过程中删除中间容器； --isolation :使用容器隔离技术； --label=[] :设置镜像使用的元数据； -m :设置内存最大值； --memory-swap :设置Swap的最大值为内存+swap，\"-1\"表示不限swap； --no-cache :创建镜像的过程不使用缓存； --pull :尝试去更新镜像的新版本； --quiet, -q :安静模式，成功后只输出镜像 ID； --rm :设置镜像成功后删除中间容器； --shm-size :设置/dev/shm的大小，默认值是64M； --ulimit :Ulimit配置。 --tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 --network: 默认 default。在构建期间设置RUN指令的网络模式 常用命令 $ docker build . #默认使用当前目录下Dockerfile $ docker build . -f centosdockerfile #其他名称dockerfile，需要指定 $ docker build -f /path/to/a/Dockerfile . #递归目录下的dockerfile $ docker build -t ghostwritten/app . #指定镜像名 $ docker build -t ghostwritten/app:1.0.2 -t ghostwritten/app:latest . #指定多个tag #Dockerfile文件中的每条指令会被独立执行，并会创建一个新镜像，Docker 会重用已生成的中间镜像，以加速docker build的构建速度，也可以通过--cache-from指定 $ docker build -t ghostwritten/app --cache-from 31f630c65071 . $ docker build -t ghostwritten/app --no-cache . #不使用缓存 详细请参考docker官网 2 容器 2.1 创建容器 (docker run) 更多细节请点击 参数细节 -d --detach #后台运行容器 -i --interactive #保持标准输入流（stdin）对容器开发 -t --tty #为容器分配一个虚拟终端 -e --env username=\"ritchie\": #为容器配置环境变量 --env-file=[]: #从指定文件读入环境变量； --volume , -v : #绑定一个卷 --volume-from #挂载另一个容器的存储卷与本容器 -P: #随机端口映射，容器内部端口随机映射到主机的端口 -p: #指定端口映射，格式为：主机(宿主)端口:容器端口 --restart=always #自动重启 --rm #停止即删除 --entrypoint #运行指定程序,入口点 --expose=[]: 开放一个端口或一组端口； --name=\"nginx-lb\": #为容器指定一个名称； --cpuset=\"0-2\" or --cpuset=\"0,1,2\": #绑定容器到指定CPU运行； -m :#设置容器使用内存最大值； --net=\"bridge\": #指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； --dns 8.8.8.8: #指定容器使用的DNS服务器，默认和宿主一致； --dns-search example.com: #指定容器DNS搜索域名，默认和宿主一致； --add-host #添加域名解析 -h --hostname \"mars\": #指定容器的hostname --link :alias #name和id是源容器的name和id，alias是源容器在link下的别名 添加链接到另一个容器，是单向的网络依赖。 --read-only #只读容器 --privileged #root拥有真正的root权限,可以看到很多host上的设备，并且可以执行mount,甚至允许在docker容器中启动docker容器 docker run -d prom/prometheus:latest #后台模式启动一个容器，容器名随机 docker run -d --name prometheus prom/prometheus:latest #后台模式启动一个容器，容器名定义为prometheus docker run -d -P --name prometheus prom/prometheus:latest #后台模式启动一个容器，容器名定义为prometheus，容器的80端口映射到主机随机端口 限制内存使用上限 $ docker run -it -m 300M --memory-swap -1 --name con1 u-stress /bin/bash 正常情况下， --memory-swap 的值包含容器可用内存和可用 swap。所以 --memory=\"300m\" --memory-swap=\"1g\" 的含义为： 容器可以使用 300M 的物理内存，并且可以使用 700M(1G -300M) 的 swap。--memory-swap 居然是容器可以使用的物理内存和可以使用的 swap 之和！ 把 --memory-swap 设置为 0 和不设置是一样的，此时如果设置了 --memory，容器可以使用的 swap 大小为 --memory 值的两倍。 如果 --memory-swap 的值和 --memory 相同，则容器不能使用 swap 限制可用的 CPU 个数 $ docker run -it --rm --cpus=2 u-stress:latest /bin/bash 指定固定的 CPU $ docker run -it --rm --cpuset-cpus=\"1\" u-stress:latest /bin/bash 设置使用 CPU 的权重 $ docker run -it --rm --cpuset-cpus=\"0\" --cpu-shares=512 u-stress:latest /bin/bash $ docker run -it --rm --cpuset-cpus=\"0\" --cpu-shares=1024 u-stress:latest /bin/bash 注意： 当 CPU 资源充足时，设置 CPU 的权重是没有意义的。只有在容器争用 CPU 资源的情况下， CPU 的权重才能让不同的容器分到不同的 CPU 用量。--cpu-shares 选项用来设置 CPU 权重，它的默认值为 1024。我们可以把它设置为 2 表示很低的权重，但是设置为 0 表示使用默认值 1024。 2.2 查看容器状态 (docker ps) 更多细节请点击 $ docker ps #查看启动的容器 $ docker ps -n 3 #查看前三个容器 $ docker ps -q #查看启动的容器ID $ docker ps -a #查看全部容器，包括停止的 $ docker ps -a -q #查看全部容器ID $ docker ps --filter status=running #查看状态为启动的容器 $ docker top prometheus #查看容器进程 $ docker exec prometheus ps #查看容器进程 2.3 查看容器日志 (docker logs) 更多细节请点击 $ docker logs prometheus $ docker logs -f prometheus $ docker logs -f --tail 200 prometheus #查看日志后200行 2.4 查看容器配置信息 (docker inspect) 更多细节请点击 $ docker inspect prometheus docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' prometheus #指定查看具体网卡地址ip配置信息 2.5 查看容器内文件操作（docker diff） A开头：表示文件被添加 C开头：表示文件被修改 D开头：表示文件被删除$ docker diff prometheus 2.5 重命名容器 (docker rename) $ docker rename prometheus prometheus-new 注意：不影响运行状态 2.6 将容器构建镜像 (docker commit) $ docker commit $ docker commit prometheus prometheus:v2 $ docker commit -a \"@author\" -m \"added somthing\" prometheus prometheus:v3 注意：当通过容器构建的镜像，也许你再用新镜像创建容器时发现无法启动，这是因为创建镜像的容器附带了/bin/bash命令，当你通过这个容器构建的镜像创建容器时，启动一个shell命令它会停止它，如果你想启动某一个命令，你可以以某个命令通过--entrypoint作为一个入口点。 $ docker run -tid --name git-cmd --entrypoint git ubuntu:latest $ docker -a \"@ghostwritten\" -m \"action git \" git-cmd ubuntu-git:v1 $ docker run -tid --name git-cmd2 git-cmd ubuntu-git:v1 version 2.6 打包与解包 (docker export / import) 打包 $ docker export -o prometheus.tar prometheus $ docker export > prometheus.tar prometheus 解包 $ docker import prometheus.tar prometheus-new #默认tag为latest $ docker import prometheus.tar prometheus-new:v1.0 #为新镜像指定name和tag 2.7 启动停止容器 (docker start / stop) $ docker start prometheus #启动 $ docker stop prometheus #停止 $ docker restart prometheus #重启 $ docker kill prometheus #强制停止 2.8 删除容器(docker rm) $ docker rm prometheus $ docker rm -f prometheus 2.9 更新容器运行参数（docker update） $ docker update --restart=always prometheus $ docker update --cpu-period=100000 --cpu-quota=20000 prometheus 3.0 容器逻辑卷管理 (docker volume) docker volume ls #查看容器卷列表 docker volume inspect #查看容器卷配置信息 docker volume rm #删除容器卷 3.1 容器网络管理 (docker network) docker network ls #容器网络列表 docker network inspect #查看容器网络配置信息 docker network rm #删除容器网络 docker network rm -f #强制删除容器网络 docker network disconnect #断开容器网络连接 docker network disconnect --force #强制断开容器网络连接 docker service docker swarm init #Docker Swarm 启动了两个 Nginx 容器实例。其中，第一条 create 命令创建了这两个容器 #第二条 update 命令则把它们“滚动更新”成了一个新的镜像。 $ docker service create --name nginx --replicas 2 nginx $ docker service update --image nginx:1.7.9 nginx 参考： docker command 11 Docker Commands: A Guide of Docker Commands with Examples Top 15 Docker Commands – Docker Commands Tutorial ✈推荐阅读： docker 命令 podman 命令 crictl 命令 kubectl 命令 operator-sdk 命令 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Command/docker_command_inspect.html":{"url":"Docker/Command/docker_command_inspect.html","title":"docker inspect   format","keywords":"","body":"docker inspect --format1. 什么是模板?2. Go模板常用语法3. 变量3.1 系统变量 {{.}}3.2 自定义变量3.3 遍历（循环）：range4. index5. 判断：if … else … end5.1 not5.2 or6. 判断条件7. 判断的使用8. 打印信息9. 管道docker inspect --format Docker --format 参数提供了基于 Go模板 的日志格式化输出辅助功能，并提供了一些内置的增强函数。 1. 什么是模板? 上图是大家熟悉的 MVC 框架（Model View Controller）： Model（模型，通常在服务端）用于处理数据、View（视图，客户端代码）用于展现结果、Controller（控制器）用于控制数据流，确保 M 和 V 的同步，即一旦 M 改变，V 也应该同步更新。 而对于 View 端的处理，在很多动态语言中是通过在静态 HTML 代码中插入动态数据来实现的。例如 JSP 的 和 PHP 的 语法。 由于最终展示给用户的信息大部分是静态不变的，只有少部分数据会根据用户的不同而动态生成。比如，对于 docker ls 的输出信息会根据附加参数的不同而不同，但其表头信息是固定的。所以，将静态信息固化为模板可以复用代码，提高展示效率。 2. Go模板常用语法 docker network inspect --format='{{/*查看容器的默认网关*/}}{{range .IPAM.Config}}{{.Gateway}}{{end}}' $INSTANCE_ID 3. 变量 3.1 系统变量 {{.}} 点号表示当前对象及上下文，和 Java、C++ 中的 this 类似。可以直接通过{{.}}获取当前对象。 另外，如果返回结果也是一个 Struct 对象（Json 中以花括号/大括号包含），则可以直接通过点号级联调用，获取子对象的指定属性值。 #可以通过级联调用直接读取子对象 State 的 Status 属性，以获取容器的状态信息： docker inspect --format '{{/*读取容器状态*/}}{{.State.Status}}' $INSTANCE_ID 注意： 如果需要获取的属性名称包含点号（比如下列示例数据）或者以数字开头，则不能直接通过级联调用获取信息。因为属性名称中的点号会被解析成级联信息，进而导致返回错误结果。即便使用引号将其包含也会提示语法格式错误。此时，需要通过 index 来读取指定属性信息。 \"Options\": { \"com.docker.network.bridge.default_bridge\": \"true\", \"com.docker.network.bridge.enable_icc\": \"true\", \"com.docker.network.bridge.enable_ip_masquerade\": \"true\", \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\", \"com.docker.network.bridge.name\": \"docker0\", \"com.docker.network.driver.mtu\": \"1500\" }, # 直接级联调用会提示找不到数据： docker inspect --format '{{.Options.com.docker.network.driver.mtu}}' bridge # 用引号括起来会提示语法错误： docker inspect --format '{{.Options.\"com.docker.network.driver.mtu\"}}' bridge Template parsing error: template: :1: bad character U+0022 '\"' # 正确的用法，必须用 index 读取指定属性名称的属性值： docker inspect --format '{{/*读取网络在hosts上的名称*/}}{{index .Options \"com.docker.network.bridge.name\"}}' bridge docker0 实例： [root@monitor1 ~]# docker inspect -f '{{.Id}}' prometheus 9094fdeb64edf75d52189e1b985d0926cf4e6d53880a8f09ab30fc2d6c8a0908 [root@monitor1 ~]# docker inspect -f '{{.State.Status}}' prometheus running [root@monitor1 ~]# docker inspect -f '{{\"status:\"}}{{.State.Status}}' prometheus status:running [root@monitor1 ~]# docker inspect -f '{{\"status:\"}}{{index .State.Status}}' prometheus status:running 3.2 自定义变量 可以在处理过程中设置自定义变量，然后结合自定义变量做更复杂的处理。 如果自定义变量的返回值是对象，则可以通过点号进一步级联访问其属性。比如 {{$Myvar.Field1}}。 # 结合变量的使用，对输出结果进行组装展现，以输出容器的所有绑定端口列表： docker inspect --format '{{/*通过变量组合展示容器绑定端口列表*/}}已绑定端口列表：{{println}}{{range $p,$conf := .NetworkSettings.Ports}}{{$p}} -> {{(index $conf 0).HostPort}}{{println}}{{end}}' Web_web_1 # 示例输出信息 已绑定端口列表： 80/tcp -> 32770 8081/tcp -> 8081 3.3 遍历（循环）：range 格式： {{range pipeline}}{{.}}{{end}} {{range pipeline}}{{.}}{{else}}{{.}}{{end}} range 用于遍历结构内返回值的所有数据。支持的类型包括 array, slice, map 和 channel。使用要点： 对应的值长度为 0 时，range 不会执行。 结构内部如要使用外部的变量，需要在前面加 引用，比如Var2。 range 也支持 else 操作。效果是：当返回值为空或长度为 0 时执行 else 内的内容。 查看容器网络下已挂载的所有容器名称，如果没有挂载任何容器，则输出 \"With No Containers\" \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"a4ae52dfb055599b4b582d4c91f5a38d297bbed30f3e1d698f1c199be572c1e3\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": { \"9090/tcp\": [ { \"HostIp\": \"0.0.0.0\", \"HostPort\": \"9090\" } ] }, \"SandboxKey\": \"/var/run/docker/netns/a4ae52dfb055\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"16dbaea3aef149379a83673284c9adb843dab363246e6eb3255e40b149003580\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"2045baaf0ce3f404298a1a57035b6cc6c91ebd4ee05ce219b818f255c198b44b\", \"EndpointID\": \"16dbaea3aef149379a83673284c9adb843dab363246e6eb3255e40b149003580\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\" } } } } ] [root@monitor1 ~]# docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' prometheus 172.17.0.2 [root@monitor1 ~]# docker inspect --format='{{range $p, $conf := .NetworkSettings.Ports}} {{$p}} -> {{(index $conf 0).HostPort}} {{end}}' prometheus 9090/tcp -> 9090 4. index 如果返回结果是一个 map, slice, array 或 string，则可以使用 index 加索引序号（从零开始计数）来读取属性值。 [root@monitor1 ~]# docker inspect --format='{{(index (index .NetworkSettings.Ports \"9090/tcp\") 0).HostPort}}' prometheus 9090 5. 判断：if … else … end 基本判断 5.1 not 返回单一参数的布尔否定值，即返回输入参数的否定值。 示例: # 如果容器的 restarting 设置为 false，则返回信息“容器没有配置重启策略” docker inspect --format '{{if not .State.Restarting}}容器没有配置重启策略{{end}}' $(docker ps -q) 容器没有配置重启策略 5.2 or {{or x y}}: 表示如果 x 为真返回 x，否则返回 y。 {{or x y z}}：后面跟多个参数时会逐一判断每个参数，并返回第一个非空的参数。如果都为 false，则返回最后一个参数。 除了 null（空）和 false 被识别为 false，其它值（字符串、数字、对象等）均被识别为 true。 示例: [root@monitor1 ~]# docker inspect --format '{{or .State.Status .State.Restarting}}' $(docker ps -q) running 6. 判断条件 判断语句通常需要结合判断条件一起使用，使用格式基本相同： {{if 判断条件 .Var1 .Var2}}{{end}} go模板支持如下判断方式： 1) eq: 相等，即 arg1 == arg2。比较特殊的是，它支持多个参数进行与比较，此时，它会将第一个参数和其余参数依次比较，返回下式的结果： {{if eq true .Var1 .Var2 .Var3}}{{end}} 效果等同于： arg1==arg2 || arg1==arg3 || arg1==arg4 ... 2) ne: 不等，即 arg1 != arg2。 3) lt: 小于，即 arg1 arg2。 6) ge: 大于等于，即 arg1 >= arg2。 7. 判断的使用 {{if pipeline}}{{end}} {{if pipeline}}{{else}}{{if pipeline}}{{end}}{{end}} {{if pipeline}}{{else if pipeline}}{{else}}{{end}} # 输出所有已停止的容器名称： docker inspect --format '{{if ne 0.0 .State.ExitCode}}{{.Name}}{{end}}' $(docker ps -aq) docker inspect --format '{{if ne 0.0 .State.ExitCode}}{{.Name}}{{else}}该容器还在运行{{end}}' $(docker ps -aq) docker inspect --format '{{if ne 0.0 .State.ExitCode}}{{.Name}}{{else if .}}该容器还在运行{{end}}' $(docker ps -aq) # 输出所有已停止或配置了 Restarting 策略的容器名称 docker inspect --format '{{if ne 0.0 .State.ExitCode}}{{.Name}}{{else if eq .State.Restarting true}}容器{{.Name}}配置了Restarting策略.{{else}}{{end}}' $(docker ps -aq) 8. 打印信息 docker --format 默认调用 go语言的 print 函数对模板中的字符串进行输出。而 go语言还有另外 2 种相似的内置函数，对比说明如下： print： 将传入的对象转换为字符串并写入到标准输出中。如果后跟多个参数，输出结果之间会自动填充空格进行分隔。 println: 功能和 print 类似，但会在结尾添加一个换行符。也可以直接使用 来换行。 printf: 与 shell 等环境一致，可配合占位符用于格式化输出。 docker inspect --format '{{.State.Pid}}{{.State.ExitCode}}' $INSTANCE_ID 240390 docker inspect --format '{{print .State.Pid .State.ExitCode}}' $INSTANCE_ID 24039 0 docker inspect --format '{{.State.Pid}}{{println \" 从这换行\"}}{{.State.ExitCode}}' $INSTANCE_ID 24039 从这换行 0 docker inspect --format '{{printf \"Pid:%d ExitCode:%d\" .State.Pid .State.ExitCode}}' $INSTANCE_ID Pid:24039 ExitCode:0 9. 管道 管道 即 pipeline ，与 shell 中类似，可以是上下文的变量输出，也可以是函数通过管道传递的返回值。 {{.Con | markdown | addlinks}} {{.Name | printf \"%s\"}} $ docker inspect --format '{{len .Name}}' prometheus 11 $ docker inspect nginx -f '{{json .State}}' | jq { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 23773, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2021-08-04T09:27:06.018089509Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" } 参考： What to Inspect When You're Inspecting Docker格式化输出命令:\"docker inspect --format\" 学习笔记 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Dockerfile/":{"url":"Docker/Dockerfile/","title":"Dockerfile","keywords":"","body":"Docker Dockerfile 编排1. FROM2. COPY3. ADD4. RUN5. CMD6. ENTRYPOINT7. LABEL8. EXPOSE9. ENV10. VOLUME11. USER12. WORKDIR13. ARG14. ONBUILD15. STOPSIGNAL16. SHELLDocker Dockerfile 编排 1. FROM FROM指令用于指定其后构建新镜像所使用的基础镜像。 FROM FROM : FROM : FROM必须是Dockerfile中第一条非注释命令 在一个Dockerfile文件中创建多个镜像时，docker17.05版本以后，FROM可以多次出现。只需在每个新命令FROM之前，记录提交上次的镜像ID。 tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 多次使用FROM的情况时构建与运行分离的场景 基础镜像golang:1.10.3是非常庞大的，因为其中包含了所有的Go语言编译工具和库，而运行时候我们仅仅需要编译后的server程序 就行了，不需要编译时的编译工具，最后生成的大体积镜像就是一种浪费。 scratch 是内置关键词，并不是一个真实存在的镜像。 FROM scratch 会使用一个完全干净的文件系统，不包含任何文件。因为Go语言编译后不需要运行时，也就不需要安装任何的运行库。FROM scratch可以使得最后生成的镜像最小化，其中只包含了server 程序。 当然，你也可以FROM一个你熟悉的并且空间占用小的镜像，比如：centos、ubuntu、busybox等。 # 编译阶段 FROM golang:1.10.3 COPY server.go /build/ WORKDIR /build RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 GOARM=6 go build -ldflags '-w -s' -o server # 运行阶段 FROM scratch # 从编译阶段的中拷贝编译结果到当前镜像中 COPY --from=0 /build/server / ENTRYPOINT [\"/server\"] COPY 指令的--from=0 参数，从前边的阶段中拷贝文件到当前阶段中，多个FROM语句时，0代表第一个阶段。除了使用数字，我们还可以给阶段命名，比如： # 编译阶段 命名为 builder FROM golang:1.10.3 as builder # ... 省略 # 运行阶段 FROM scratch # 从编译阶段的中拷贝编译结果到当前镜像中 COPY --from=builder /build/server / COPY --from 不但可以从前置阶段中拷贝，还可以直接从一个已经存在的镜像中拷贝。比如， FROM ubuntu:16.04 COPY --from=quay.io/coreos/etcd:v3.3.9 /usr/local/bin/etcd /usr/local/bin/ 2. COPY COPY同样用于复制构建环境中的文件或目录到镜像中。 COPY ... COPY [\"\",... \"\"] COPY --from=builder /build/server / COPY --from=quay.io/coreos/etcd:v3.3.9 /usr/local/bin/etcd /usr/local/bin/ COPY指令非常类似于ADD，不同点在于COPY只会复制构建目录下的文件，不能使用URL也不会进行解压操作。 3. ADD ADD用于复制构建环境中的文件或目录到镜像中。 ADD ... ADD [\"\",... \"\"] 指定源文件位置，来指定目标位置。 可以是一个构建上下文中的文件或目录，也可以是一个URL，但不能访问构建上下文之外的文件或目录。 ADD复制一个网络文件： ADD http://wordpress.org/test.zip $WORKER_PATH 另外，如果使用的是本地归档文件（gzip、bzip2、xz）时，Docker会自动进行解包操作，类似使用tar -x 4. RUN RUN用于在镜像容器中执行命令，其有以下两种命令执行方式： shell执行 在这种方式会在shell中执行命令，Linux下默认使用/bin/sh -c，Windows下使用cmd /S /C。 注意：通过SHELL命令修改RUN所使用的默认shell RUN RUN /bin/bash -c 'source $HOME/.bashrc; \\ echo $HOME' #通过RUN执行多条命令时，可以通过\\换行执行 RUN /bin/bash -c 'source $HOME/.bashrc; echo $HOME' #同一行中，通过分号分隔命令 exec执行 RUN [\"executable\", \"param1\", \"param2\"] RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache。 5. CMD CMD用于指定在容器启动时所要执行的命令。CMD有以下三种格式： CMD [\"executable\",\"param1\",\"param2\"] CMD [\"param1\",\"param2\"] CMD command param1 param2 CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。 CMD与RUN在功能实现上也有相似之处。如： docker run -t -i ghostwritten/web_server /bin/true 等价于： cmd [\"/bin/true\"] CMD在Dockerfile文件中仅可指定一次，指定多次时，会覆盖前的指令。 docker run命令也会覆盖Dockerfile中CMD命令。 6. ENTRYPOINT ENTRYPOINT用于给容器配置一个可执行程序。也就是说，每次使用镜像创建容器时，通过ENTRYPOINT指定的程序都会被设置为默认程序。ENTRYPOINT有以下两种形式： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] ENTRYPOINT command param1 param2 ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖 ENTRYPOINT docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。 Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 docker run运行容器时指定的参数都会被传递给ENTRYPOINT，且会覆盖CMD命令指定的参数。如，执行docker run -d时，-d参数将被传递给入口点。 也可以通过docker run --entrypoint重写ENTRYPOINT入口点。 示例： ENTRYPOINT [\"/usr/bin/nginx\"] Dockerfile FROM ubuntu:16.04 RUN apt-get update RUN apt-get install -y nginx RUN echo 'Hello World, 我是个容器' > /var/www/html/index.html EXPOSE 80 ENTRYPOINT [\"/usr/sbin/nginx\"] 构建 docker build -t=\"ghostwritten/app\" . 创建容器 docker run -i -t ghostwritten/app -g \"daemon off;\" -g \"daemon off;\"参数将会被传递给ENTRYPOINT，最终在容器中执行的命令为/usr/sbin/nginx -g \"daemon off;\"。 7. LABEL LABEL用于为镜像添加元数据，元数以键值对的形式指定： LABEL = = = ... 使用LABEL指定元数据时，一条LABEL指定可以指定一或多条元数据，指定多条元数据时不同元数据之间通过空格分隔。 LABEL version=\"1.0\" description=\"这是一个Web服务器\" by=\"ghostwritten\" 也可以换行 LABEL multi.label1=\"value1\" \\ multi.label2=\"value2\" \\ other=\"value3\" docker inspect查看： $ docker inspect itbilu/test \"Labels\": { \"version\": \"1.0\", \"description\": \"这是一个Web服务器\", \"by\": \"ghostwritten\" }, 注意:Dockerfile中还有个MAINTAINER命令，该命令用于指定镜像作者。但MAINTAINER并不推荐使用，更推荐使用LABEL来指定镜像作者。如： LABEL maintainer=\"ghostwritten\" 8. EXPOSE EXPOSE用于指定容器在运行时监听的端口： EXPOSE [...] EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口。 9. ENV ENV用于设置环境变量，其有以下两种设置形式： ENV ENV = ... 示例： ENV $WORKER_PATH /myapp 设置后，这个环境变量在ENV命令后都可以使用。如： WORKERDIR $WORKER_PATH docker run可以通过 -e 新添环境变量或者覆盖环境变量。 docker run -tid --name test -e $WORKER_PATH /web -e IP=192.168.1.2 ghostwritten/web:v1.0 -e $WORKER_PATH /web 为覆盖ENV $WORKER_PATH /myapp -e IP=192.168.1.2为新添 10. VOLUME VOLUME用于创建挂载点，即向基于所构建镜像创始的容器添加卷： VOLUME [\"/data\"] 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能： 卷可以容器间共享和重用 容器并不一定要和其它容器共享卷 修改卷后会立即生效 对卷的修改不会对镜像产生影响 VOLUME创建一个挂载点： ENV WORKER_PATH /web VOLUME [$WORKER_PATH] 运行容器时，需-v参将能本地目录绑定到容器的卷（挂载点）上，以使容器可以访问宿主机的数据。 docker run -itd --name web -v ~/data:/web/ ghostwritten/web:v1.0 11. USER USER用于指定运行镜像所使用的用户 可以使用用户名、UID或GID，或是两者的组合。 USER user USER user:group USER uid USER uid:gid USER user:gid USER uid:group docker run运行容器时，可以通过-u参数来覆盖所指定的用户。 12. WORKDIR WORKDIR用于在容器内设置一个工作目录： WORKDIR /path/to/workdir 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行。 WORKDIR /a WORKDIR b WORKDIR c RUN pwd pwd最终将会在/a/b/c目录中执行。 docker run运行容器时，可以通过-w参数覆盖构建时所设置的工作目录。 13. ARG ARG用于指定传递给构建运行时的变量： ARG [=] 示例： ARG site ARG build_user=ghostwritten 以上我们指定了site和build_user两个变量，其中build_user指定了默认值。在使用docker build构建镜像时，可以通过--build-arg =参数来指定或重设置这些变量的值。 $ docker build --build-arg site=ghostwritten -t ghostwritten/test . 14. ONBUILD ONBUILD用于设置镜像触发器： ONBUILD [INSTRUCTION] ONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src 当所构建的镜像被用做其它镜像的基础镜像，该镜像中的触发器将会被钥触发。 示例： 第一个构建镜像的Dockerfile，文件名为base.df FROM busybox:latest WORKDIR /app RUN touch /app/base-evidence ONBUILD RUN ls -al /app 构建 docker build -t ghostwritten/onbuild:v1.0 -f base.df . 第二个构建镜像的Dockerfile，文件名downstream.df FROM ghostwritten/onbuild:v1.0 RUN touch downstream-evidence RUN ls -al . 构建 docker build -t ghostwritten/onbuild_down:v1.0 -f downstream.df . onbuild指令在第一次构建时不会执行，在第二次被引用时会首先执行。 15. STOPSIGNAL STOPSIGNAL用于设置停止容器所要发送的系统调用信号： STOPSIGNAL signal 所使用的信号必须是内核系统调用表中的合法的值，如：9、SIGKILL。 16. SHELL SHELL用于设置执行命令（shell式）所使用的的默认shell类型： SHELL [\"executable\", \"parameters\"] SHELL在Windows环境下比较有用，Windows下通常会有cmd和powershell两种shell，可能还会有sh。这时就可以通过SHELL来指定所使用的shell类型。 参考： Docker Dockerfile 使用 Dockerfile 定制镜像 Dockerfile reference Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Image/":{"url":"Docker/Image/","title":"Image","keywords":"","body":"Docker 镜像 overview1. 什么是 Docker 镜像？2. Docker 镜像 demo3. Docker 容器与 Docker 镜像4. Docker 镜像剖析Docker 镜像 overview 1. 什么是 Docker 镜像？ Docker 镜像是用于在 Docker 容器中执行代码的文件。Docker 镜像充当构建 Docker容器的一组指令，就像模板一样。使用 Docker 时，Docker 镜像也可以作为起点。 镜像相当于虚拟机 (VM) 环境中的快照。 Docker 用于在容器中创建、运行和部署应用程序。Docker 镜像包含应用程序运行所需的应用程序代码、库、工具、依赖项和其他文件。当用户运行一个镜像时，它可以成为一个容器的一个或多个实例。 Docker 镜像有多个层，每一层都源自上一层，但又有所不同。这些层加速了 Docker 构建，同时提高了可重用性并减少了磁盘使用。图像层也是只读文件。创建容器后，在不可更改的图像之上添加一个可写层，允许用户进行更改。 对 Docker 镜像和容器中的磁盘空间的引用可能会令人困惑。区分大小和虚拟大小很重要。大小是指容器的可写层使用的磁盘空间，而虚拟大小是指容器和可写层使用的磁盘空间。 镜像的只读层可以在从同一 镜像启动的任何容器之间共享。 2. Docker 镜像 demo Docker 镜像包含运行容器化应用程序所需的一切，包括代码、配置文件、环境变量、库和运行时。当镜像部署到 Docker 环境时，它可以作为 Docker 容器执行。docker run 命令从一个特定的镜像创建一个容器。 Docker 镜像是可重用的资产——可部署在任何主机上。开发人员可以从一个项目中获取静态图像层并在另一个项目中使用它们。这节省了用户时间，因为他们不必从头开始重新创建图像。 3. Docker 容器与 Docker 镜像 Docker 容器是用于应用程序开发的虚拟化运行时环境。它用于创建、运行和部署与底层硬件隔离的应用程序。一个 Docker 容器可以使用一台机器，共享其内核并虚拟化操作系统以运行更多独立的进程。因此，Docker 容器是轻量级的。 Docker 镜像就像其他类型的 VM 环境中的快照。它是 Docker 容器在特定时间点的记录。Docker 镜像也是不可变的。虽然它们无法更改，但可以复制、共享或删除它们。该功能对于测试新软件或配置很有用，因为无论发生什么，图像都保持不变。 容器需要一个可运行的镜像才能存在。容器依赖于镜像，因为它们用于构建运行时环境并且是运行应用程序所必需的。 4. Docker 镜像剖析 一个 Docker 镜像有很多层，每个镜像都包含配置容器环境所需的一切——系统库、工具、依赖项和其他文件。图像的一些部分包括： Base image：用户可以使用 build 命令完全从头开始构建第一层。 Parent image：作为基础镜像的替代方案，父镜像可以是 Docker 镜像中的第一层。它是一个重复使用的图像，作为所有其他层的基础。 Layers：层被添加到基础镜像中，使用代码使其能够在容器中运行。Docker 映像的每一层都可以在 /var/lib/docker/aufs/diff 下查看，或者通过命令行界面 (CLI) 中的 Docker history 命令查看。Docker 的默认状态是显示所有顶层镜像，包括存储库、标签和文件大小。中间层被缓存，使顶层更容易查看。Docker 具有处理镜像层内容管理的存储驱动器。 Container layer：一个 Docker 镜像不仅会创建一个新的容器，还会创建一个可写或容器层。该层托管对正在运行的容器所做的更改，并存储新写入和删除的文件，以及对现有文件的更改。该层还用于自定义容器。 Docker manifest：Docker 映像的这一部分是一个附加文件。它使用JSON格式来描述图像，使用图像标签和数字签名等信息。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Image/build_images_with_buildah.html":{"url":"Docker/Image/build_images_with_buildah.html","title":"构建镜像开源工具 buildah","keywords":"","body":"构建镜像开源工具 buildah1. 简介2. 特点3. Buildah 和 Podman4. 安装4.1 CentOS4.2 Ubuntu4.3 RHEL74.4 Fedora5. 命令6. 示例6.1 命令行构建一个 httpd 镜像6.2 Dockerfile 构建6.3 构建镜像脚本（代替 Dockerfile）构建镜像开源工具 buildah tagsstart images tagsstop 1. 简介 Buildah 是一种基于 Linux 的开源工具，用于构建与开放容器倡议 (OCI) 兼容的容器，这意味着容器也与Docker和Kubernetes兼容。借助 Buildah，您可以使用自己喜欢的工具从现有基础镜像或使用空镜像从头开始创建高效的容器镜像。这是一种更灵活、更安全的构建容器镜像的方式。 Buildah由 Daniel Walsh 和他在 Red Hat 的团队于 2017 年创建。他们着手创建容器镜像的“coreutils”——一种可以与现有容器主机工具一起使用来构建 OCI 和 Docker 兼容容器镜像的工具。然后，这些镜像可以存储在容器仓库中，并在多个运行时环境中使用。 2. 特点 使用或不使用 Dockerfiles（包含用户可以调用以组装镜像的所有命令的文本文档）构建容器镜像 从头开始或从现有容器镜像起点创建容器镜像； 不在镜像本身中包含构建工具，减少构建镜像的大小，提高安全性，并允许使用更少的资源更容易地传输 ； 与 Dockerfiles 兼容，允许从 Docker 轻松转换； 创建特定于用户的镜像，以便镜像可以按创建它们的用户进行排序； 检查、验证和修改镜像； 将容器和镜像从本地存储推送到公共或私有仓库或存储库； 从 Docker Hub 推送或拉取镜像； 移除本地存储的容器镜像； 挂载和卸载工作容器的根文件系统； 使用容器根文件系统的更新内容作为新镜像的文件系统层。 3. Buildah 和 Podman Buildah 和Podman都是互补的开源项目和命令行工具，使用并构建 OCI 镜像和容器。首先创建了 Buildah，Podman 使用与 Buildah 相同的代码进行构建。但是，Buildah 的命令比 Podman 的命令详细得多，允许对镜像进行更细粒度的控制并允许创建更精细的镜像层。Podman 的“构建”命令使用了 Buildah 功能的一个子集。 Buildah 专注于构建容器镜像，复制在没有守护程序套接字组件的 Dockerfile 中找到的所有命令，而 Podman 专注于维护和修改容器中的这些镜像所需的东西。使用 Podman，您可以创建一个容器——使用 Buildah 提供容器镜像——然后使用熟悉的命令行界面 (CLI) 命令（如果您可以运行一个Docker CLI 中的命令，您可以在 Podman CLI 中运行相同的命令）。 Podman 和 Buildah 的另一个不同之处是：Buildah 的容器主要是临时创建的，以允许将内容传输到正在创建的容器镜像中，而使用 Podman，用户创建传统容器，旨在使用和维护更长时间. Buildah 的容器用于短期目的，而 Podman 的容器用于长期目的。 Buildah 和 Podman 各自创建的容器是互相看不到的。 4. 安装 4.1 CentOS sudo yum -y install buildah 4.2 Ubuntu # Ubuntu 20.10 and newer sudo apt-get -y update sudo apt-get -y install buildah 4.3 RHEL7 sudo subscription-manager repos --enable=rhel-7-server-extras-rpms sudo yum -y install buildah 4.4 Fedora sudo dnf -y install buildah 或者 $ sudo rpm-ostree install buildah 更多安装方式请参考这里 5. 命令 Command Description buildah-add(1) 将文件、URL 或目录的内容添加到容器中。 buildah-build(1) 使用 Containerfiles 或 Dockerfiles 中的指令构建镜像。 buildah-commit(1) 从运行的容器创建镜像。 buildah-config(1) 更新镜像配置设置。 buildah-containers(1) 列出工作容器及其基础镜像。 buildah-copy(1) 将文件、URL 或目录的内容复制到容器的工作目录中。 buildah-from(1) 从头开始或使用指定镜像作为起点创建一个新的工作容器。 buildah-images(1) 列出本地存储中的镜像。 buildah-info(1) 显示 Buildah 系统信息。 buildah-inspect(1) 检查容器或镜像的配置。 buildah-mount(1) 挂载工作容器的根文件系统。 buildah-pull(1) 从指定位置拉取镜像。 buildah-push(1) 将镜像从本地存储推送到其他地方。 buildah-rename(1) 重命名本地容器 buildah-rm(1) 删除一个或多个工作容器。 buildah-rmi(1) 删除一个或多个镜像. buildah-run(1) 在容器内运行命令。 buildah-tag(1) 为本地镜像添加一个额外的名称。 buildah-umount(1) 卸载工作容器的根文件系统。 buildah-unshare(1) 在具有修改后的 ID 映射的用户命名空间中启动命令。 buildah-version(1) 显示 Buildah 版本信息 6. 示例 配置别名 $ vim /root/.bashrc alias p='podman' alias b='buildah' alias s='skopeo' 6.1 命令行构建一个 httpd 镜像 第一步是提取基本映像并创建工作容器 $ buildah from fedora fedora-working-container $ b ps CONTAINER ID BUILDER IMAGE ID IMAGE NAME CONTAINER NAME 89704476b76a * 885d2b38b819 registry.fedoraproject.org/fe... fedora-working-container 将包添加到工作容器 buildah run fedora-working-container dnf install httpd -y 为Web服务器创建包含某些内容的工作目录： mkdir demo-httpd && cd demo-httpd && echo 'sample container' > index.html 将本地文件复制到工作容器 buildah copy fedora-working-container index.html /var/www/html/index.html 定义容器入口点以启动应用程序 buildah config --entrypoint \"/usr/sbin/httpd -DFOREGROUND\" fedora-working-container 配置完成后，保存镜像： buildah commit fedora-working-container fedora-myhttpd 列出本地镜像 $ buildah images REPOSITORY TAG IMAGE ID CREATED SIZE localhost/fedora-myhttpd latest e1fb00a4662b 43 seconds ago 434 MB 现在可以使用podman在本地利用新生成的镜像运行容器： podman run -tid fedora-myhttpd 测试 $ p exec -ti heuristic_solomon curl http://localhost sample container 要将映像推送到本地Docker仓库，请执行以下操作： #登陆仓库 $ buildah login -u registryuser -p registryuserpassword 192.168.10.80:5000 Login Succeeded! #推送 $ buildah push fedora-myhttpd docker://192.168.10.80:5000/testuser/fedora-myhttpd:latest Getting image source signatures Copying blob d4222651a196 done Copying blob cc6656265656 done Copying config e1fb00a466 done Writing manifest to image destination Storing signatures 也可以这样执行： buildah push --creds registryuser:registryuserpassword fedora-myhttpd docker://192.168.10.80:5000/testuser/fedora-myhttpd:latest Skopeo检查结果 $ skopeo inspect docker://192.168.10.80:5000/testuser/fedora-myhttpd:latest 6.2 Dockerfile 构建 $ mkdir fedora-http-server && cd fedora-http-server $ nano Dockerfile # Base on the most recently released Fedora FROM fedora:latest MAINTAINER ipbabble email buildahboy@redhat.com # not a real email # Install updates and httpd RUN echo \"Updating all fedora packages\"; dnf -y update; dnf -y clean all RUN echo \"Installing httpd\"; dnf -y install httpd && dnf -y clean all # Expose the default httpd port 80 EXPOSE 80 # Run the httpd CMD [\"/usr/sbin/httpd\", \"-DFOREGROUND\"] 按CTRL+X退出，按Y保存，按Enter退出nano 构建 buildah bud -t fedora-http-server 运行容器 podman run -p 8080:80 -tid fedora-http-server podman ps 测试访问 curl localhost:8080 6.3 构建镜像脚本（代替 Dockerfile） build_buildah_upstream.sh #!/usr/bin/env bash # build_buildah_upstream.sh # ctr=$(buildah from fedora) buildah config --env GOPATH=/root/buildah $ctr buildah run $ctr /bin/sh -c 'dnf -y install --enablerepo=updates-testing \\ make \\ golang \\ bats \\ btrfs-progs-devel \\ device-mapper-devel \\ glib2-devel \\ gpgme-devel \\ libassuan-devel \\ libseccomp-devel \\ git \\ bzip2 \\ go-md2man \\ runc \\ fuse-overlayfs \\ fuse3 \\ containers-common; \\ mkdir -p /root/buildah; \\ git clone https://github.com/containers/buildah /root/buildah/src/github.com/containers/buildah; \\ cd /root/buildah/src/github.com/containers/buildah; \\ make; \\ make install; \\ rm -rf /root/buildah/*; \\ dnf -y remove bats git golang go-md2man make; \\ dnf clean all' buildah run $ctr -- sed -i -e 's|^#mount_program|mount_program|g' -e '/additionalimage.*/a \"/var/lib/shared\",' /etc/containers/storage.conf buildah run $ctr /bin/sh -c 'mkdir -p /var/lib/shared/overlay-images /var/lib/shared/overlay-layers; touch /var/lib/shared/overlay-images/images.lock; touch /var/lib/shared/overlay-layers/layers.lock' buildah config --env _BUILDAH_STARTED_IN_USERNS=\"\" --env BUILDAH_ISOLATION=chroot $ctr buildah commit $ctr buildahupstream 构建镜像： chmod 755 build_buildah_upstream.sh ./build_buildah_upstream.sh 运行容器： $ podman run buildahupstream buildah version $ podman run buildahupstream bash -c \"buildah from busybox; buildah images\" 参考： Building Images With Buildah Building with Buildah: Dockerfiles, command line, or scripts Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Image/docker_Build_multi-platform_image.html":{"url":"Docker/Image/docker_Build_multi-platform_image.html","title":"如何为不同语言快速构建多平台镜像","keywords":"","body":"如何为不同语言快速构建多平台镜像1. Java 应用容器化1.1 启动 Jar 包的构建方式1.2 Spring Boot 插件的构建方式2. Golang 应用容器化3. Node.js 应用容器化4. Vue 应用容器化4.1 Http-server 构建方式4.2 Nginx 构建方式5. 构建多平台镜像5.1 初始化5.2 构建多平台镜像6. 总结如何为不同语言快速构建多平台镜像 1. Java 应用容器化 常见的 Java 应用启动方式有两种，这也就意味着镜像构建方式也有两种。 一种是将应用打包成 Jar 包，在镜像内直接启动应用 Jar 包来构建镜像; 另一种是在容器里通过 Spring Boot 插件直接启动应用。接下来，我分别介绍这两种镜像构建方式。 1.1 启动 Jar 包的构建方式 以 Spring Boot 和 Maven 为例，我已经提前创建好了一个 Demo 应用，我们以它为例子介绍如何使用 Jar 包构建镜像。在将示例应用克隆到本地后，进入 Spring Boot Demo 目录并列出所有文件。 $ cd gitops/docker/13/spring-boot $ ls -al total 80 drwxr-xr-x 12 weiwang staff 384 10 5 11:17 . drwxr-xr-x 4 weiwang staff 128 10 5 11:17 .. -rw-r--r-- 1 weiwang staff 6 10 5 10:30 .dockerignore -rw-r--r-- 1 weiwang staff 374 10 5 11:05 Dockerfile drwxr-xr-x 4 weiwang staff 128 10 5 11:17 src ...... 在这里，我们重点关注 src 目录、Dockerfile 文件和 .dockerignore 文件。 首先，src 目录下的 src/main/java/com/example/demo/DemoApplication.java 文件的内容是 Demo 应用的主体文件，它包含一个 /hello 接口，使用 Get 请求访问后会返回 “Hello World”。 package com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @GetMapping(\"/hello\") public String hello(@RequestParam(value = \"name\", defaultValue = \"World\") String name) { return String.format(\"Hello %s!\", name); } } Demo 应用的主体内容虽然很简单，但它代表了 Spring Boot + Maven 的典型组合，只要符合这两种技术选型，你都可以直接参考这里的例子来容器化你的业务应用。接下来，是构建镜像的核心内容 Dockerfile 文件。 # syntax=docker/dockerfile:1 FROM eclipse-temurin:17-jdk-jammy as builder WORKDIR /opt/app COPY .mvn/ .mvn COPY mvnw pom.xml ./ RUN ./mvnw dependency:go-offline COPY ./src ./src RUN ./mvnw clean install FROM eclipse-temurin:17-jre-jammy WORKDIR /opt/app EXPOSE 8080 COPY --from=builder /opt/app/target/*.jar /opt/app/*.jar CMD [\"java\", \"-jar\", \"/opt/app/*.jar\" ] 刚开始学习 Dockerfile 的同学可能会感到疑惑，为什么这里有两个 FROM 语句呢？实际上，这里使用了多阶段构建的方式，你可以理解为，第一个阶段的构建产物可以作为下一个阶段的输入。 我们来看第一阶段的构建，也就是从第 3 行到第 9 行。第 3 行 FROM 表示把 eclipse-temurin:17-jdk-jammy 作为 build 阶段的基础镜像，然后使用 WORKDIR 关键字指定了工作目录为 /opt/app，后续的文件操作都会在这个工作目录下展开。 接下来，第 5 和第 6 行通过 COPY 关键字将 .mvn 目录和 mvnw、pom.xml 文件复制到了工作目录下，第 7 行通过 RUN 关键字运行 ./mvnw dependency:go-offline 来安装依赖。然后，第 8 行将 src 目录复制到了镜像中，第 9 行使用 RUN 关键字执行 ./mvnw clean install 进行编译。 第 12 行到 16 行是第二个构建阶段。第 12 行表示使用 eclipse-temurin:17-jre-jammy 作为基础镜像，第 13 行同样指定了工作目录为 /opt/app，第 14 行的 EXPOSE 关键字之前我们有提到过，它是一个备注功能，并不是要暴露真实容器端口的意思。 第 15 行的 COPY 语句比较复杂，它指的是从 builder 阶段也就是将第一个阶段位于 /opt/app/target/ 目录下所有的 .jar 文件都拷贝到当前构建阶段镜像的 /opt/app/ 目录下。第 16 行使用 CMD 关键字定义了启动命令，也就是通过 java -jar 的方式启动应用。 最后，.dockerignore 的功能和我们熟悉的 .gitignore 文件功能类似，它指的是在构建过程中需要忽略的文件或目录，合理的文件忽略策略将有助于提高构建镜像的速度。在这个例子中，因为我们要在容器里重新编译应用，所以我们忽略了本地的 target 目录。接下来，我们就可以使用 docker build 命令来构建镜像了。 $ docker build -t spring-boot . 当镜像构建完成后，我们要使用 docker run 命令启动镜像，并通过 --publish 暴露端口。 $ docker run --publish 8080:8080 spring-boot ...... 2022-10-05 03:59:48.746 INFO 1 --- [ main] com.example.demo.DemoApplication : Starting DemoApplication v0.0.1-SNAPSHOT using Java 17.0.4.1 on da50d0bb2460 with PID 1 (/opt/app/*.jar started by root in /opt/app) 2022-10-05 03:59:48.748 INFO 1 --- [ main] com.example.demo.DemoApplication : No active profile set, falling back to 1 default profile: \"default\" 2022-10-05 03:59:49.643 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2022-10-05 03:59:49.655 INFO 1 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2022-10-05 03:59:49.656 INFO 1 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.65] 2022-10-05 03:59:49.754 INFO 1 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2022-10-05 03:59:49.755 INFO 1 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 933 ms 2022-10-05 03:59:50.105 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2022-10-05 03:59:50.117 INFO 1 --- [ main] com.example.demo.DemoApplication : Started DemoApplication in 1.796 seconds (JVM running for 2.221) 打开一个新的命令行终端，并使用 curl 访问 hello 接口验证返回内容。 $ curl localhost:8080/hello Hello World! 如果要终止 spring-boot 应用，你可以回到执行 docker run 的命令行终端，并使用 ctrll+c 来停止容器。如果你跟着我操作到了这里，说明你也已经成功以 Jar 包的方式将 Spring Boot 应用构建为 Docker 镜像了。 1.2 Spring Boot 插件的构建方式 除了使用 Jar 包，Spring Boot 应用还可以通过 ./mvnw spring-boot:run 的方式启动，这意味着我们也可以把它作为镜像的启动命令。我还是以 Spring Boot 示例应用为例，我在示例应用 Dockerfile 文件同级目录下已经提前准备好了 Dockerfile-Boot 文件，下面是该文件的内容。 # syntax=docker/dockerfile:1 FROM eclipse-temurin:17-jdk-jammy WORKDIR /app COPY .mvn/ .mvn COPY mvnw pom.xml ./ RUN ./mvnw dependency:resolve COPY src ./src CMD [\"./mvnw\", \"spring-boot:run\"] 相比较 Jar 的启动方式，Spring Boot 插件的启动方式显得更加简单。在构建过程中，我们实际上还用了一个小技巧：第 7 和第 8 行代表单独复制了依赖清单文件 pom.xml 而不是复制整个根目录，目的是在依赖不变的情况下充分利用 Docker 构建缓存。 在这个 Dockerfile 文件中有两条关键的命令，一个 mvnw dependency:resolve 用于安装依赖，另一个 mvnw spring-boot:run 命令用来启动应用。接下来，我们使用 docker build 命令构建镜像，这里要注意增加 -f 参数指定新的 Dockerfile 文件。 $ docker build -t spring-boot . -f Dockerfile-Boot $ docker run --publish 8080:8080 spring-boot 最后，你可以尝试用 curl 访问 localhost:8080/hello 接口，将会得到 Hello World 返回结果。Spring Boot 插件的启动方式虽然比较简单，但它将构建过程延迟到了启动阶段，并且依赖镜像的 JDK 工具，对于生产环境来说这些都不是必要的。如果你通过 docker images 命令仔细对比两次构建镜像占用的空间大小，你会发现，第一种方式构建生成的镜像大概在 280M 左右，而第二种构建方式生成的镜像在 500M 左右。在实际的生产环境中，我更推荐你使用第一种方式来构建 Java 镜像。 2. Golang 应用容器化 下面我们继续来看 Go 应用的容器化。以 Echo 框架为例，我提前编写好了一个简单的示例应用。在将示例应用克隆到本地后，你可以进入 docker/13/golang 目录并查看。 $ cd gitops/docker/13/golang $ ls -al -rw-r--r-- 1 weiwang staff 292 10 5 14:16 Dockerfile -rw-r--r-- 1 weiwang staff 599 10 5 14:12 go.mod -rw-r--r-- 1 weiwang staff 2825 10 5 14:12 go.sum -rw-r--r-- 1 weiwang staff 235 10 5 14:13 main.go main.go 文件是应用的主体文件，包含一个 /hello 接口，通过 Get 方法请求后，将返回 Hello World 字符串。 package main import ( \"net/http\" \"github.com/labstack/echo/v4\" ) func main() { e := echo.New() e.GET(\"/hello\", func(c echo.Context) error { return c.String(http.StatusOK, \"Hello World Golang\") }) e.Logger.Fatal(e.Start(\":8080\")) } 接下来，我们来看 Dockerfile 的内容。 # syntax=docker/dockerfile:1 FROM golang:1.17 as builder WORKDIR /opt/app COPY . . RUN go build -o example FROM ubuntu:latest WORKDIR /opt/app COPY --from=builder /opt/app/example /opt/app/example EXPOSE 8080 CMD [\"/opt/app/example\"] 同样地，这个 Dockerfile 包含了两个构建阶段，第一个构建阶段是以 golang:1.17 为基础镜像，然后我们执行 go build 命令编译并输出可执行文件，将其命名为 example。第二个构建阶段是以 ubuntu:latest 为基础镜像，第 9 行通过 COPY 关键字将第一个阶段构建的 example 可执行文件复制到镜像的 /opt/app/ 目录下，最后，使用 CMD 来运行 example 启动应用。现在，我们可以通过 docker build 来构建镜像。 现在，我们可以通过 docker build 来构建镜像 $ docker build -t golang . 接下来，使用 docker run 来启动镜像。 $ docker run --publish 8080:8080 golang ____ __ / __/___/ / ___ / _// __/ _ \\/ _ \\ /___/\\__/_//_/\\___/ v4.9.0 High performance, minimalist Go web framework https://echo.labstack.com ____________________________________O/_______ O\\ ⇨ http server started on [::]:8080 果你还没有终止之前运行的 Spring Boot 示例，在运行 Golang 示例时，你可能会得到 “Bind for 0.0.0.0:8080 failed: port is already allocated” 的错误。你可以通过 docker ps 命令来查看 Spring Demo 的容器 ID，并通过 docker stop [Container ID] 来终止它。这时候再运行 Golang 示例就能够正常启动了。现在，你可以使用 curl 命令来访问 localhost:8080/hello 接口，查看是否返回了预期的 Hello World Golang 字符串。 3. Node.js 应用容器化 以 Express.js 框架为例，我已经提前编写好了一个简单示例，在将示例应用克隆到本地后，你可以进入 docker/13/node 目录并查看。 $ cd gitops/docker/13/node $ ls -al -rw-r--r-- 1 weiwang staff 12 10 5 16:45 .dockerignore -rw-r--r-- 1 weiwang staff 589 10 5 16:39 Dockerfile -rw-r--r-- 1 weiwang staff 230 10 5 16:44 app.js drwxr-xr-x 60 weiwang staff 1920 10 5 16:26 node_modules -rw-r--r-- 1 weiwang staff 39326 10 5 16:26 package-lock.json -rw-r--r-- 1 weiwang staff 251 10 5 16:26 package.json app.js 是示例应用的主体文件，包含一个 /hello 接口。当我们通过 Get 请求访问时，会返回“Hello World Node.js”字符串。 const express = require('express') const app = express() const port = 3000 app.get('/hello', (req, res) => { res.send('Hello World Node.js') }) app.listen(port, () => { console.log(`Example app listening on port ${port}`) }) .dockerignore 是构建镜像时的忽略文件，在这个例子中，忽略了 node_modules 目录。 $ cat .dockerignore node_modules 接着我们来看一下 Dockerfile 文件的内容。 # syntax=docker/dockerfile:1 FROM node:latest AS build RUN sed -i \"s@http://\\(deb\\|security\\).debian.org@https://mirrors.aliyun.com@g\" /etc/apt/sources.list RUN apt-get update && apt-get install -y dumb-init WORKDIR /usr/src/app COPY package*.json ./ RUN npm ci --only=production FROM node:16.17.0-bullseye-slim ENV NODE_ENV production COPY --from=build /usr/bin/dumb-init /usr/bin/dumb-init USER node WORKDIR /usr/src/app COPY --chown=node:node --from=build /usr/src/app/node_modules /usr/src/app/node_modules COPY --chown=node:node . /usr/src/app CMD [\"dumb-init\", \"node\", \"app.js\"] 这是一个由两个阶段组成的镜像构建方法。第一个阶段使用 node:latest 作为 build 阶段的基础镜像，同时安装了 dumb-init 组件。此外，这种构建方法还将 package.json 和 package-lock.json 复制到镜像内，并通过 npm ci --only=production 命令安装依赖。 从第 10 行开始是第二个构建阶段，这里使用了 node:16.17.0-bullseye-slim 作为基础镜像，此外，我们还为 Express 配置了 NODE_ENV=production 的环境变量，代表在生产环境中使用。这会改变 Express.js 框架的默认配置，如日志等级、缓存处理策略等。然后，我们还要将 build 阶段安装的 dumb-init 组件、依赖以及源码复制到第二个阶段的镜像中，修改源码和依赖目录的用户组。最后，通过 CMD 命令使用 node 启动 app.js。 在将 NodeJS 容器化的过程中，有一个需要特别注意的细节，由于 NodeJS 并不是设计以 PID=1 的进程运行的，所以常规的启动方式并不能让 NodeJS 程序在容器内接收到 Kill 信号，这会导致 Node 进程不能被优雅终止（例如更新时突然中断），所以我们可以通过 dumb-init 组件来启动 Node 进程。 现在，我们可以通过 docker build 来构建镜像。 docker build -t nodejs . 接下来，使用 docker run 来启动镜像。 $ docker run --publish 3000:3000 nodejs Example app listening on port 3000 进行到这里，你可以使用 curl 命令来访问 localhost:3000/hello 接口，查看是否返回了预期的 Hello World Node.js 字符串。 4. Vue 应用容器化 常见的 Vue 应用容器化方案有两种: 第一种是将 http-server 组件作为代理服务器来构建镜像， 第二种是让 Nginx 作为代理服务器来构建镜像。接下来，我会分别介绍这两种镜像构建方式。 4.1 Http-server 构建方式 先看 http-server 的构建方式。以 Vue 框架为例，我已经提前将项目进行了初始化，接下来你需要将示例应用克隆到本地，然后进入 docker/13/vue/example 目录并查看。 $ cd gitops/docker/13/vue/example $ ls -al -rw-r--r-- 1 weiwang staff 12 10 5 17:26 .dockerignore -rw-r--r-- 1 weiwang staff 172 10 5 17:27 Dockerfile -rw-r--r-- 1 weiwang staff 0 10 5 17:34 Dockerfile-Nginx -rw-r--r-- 1 weiwang staff 631 10 5 17:23 README.md -rw-r--r-- 1 weiwang staff 337 10 5 17:23 index.html ...... 在这个例子中，.dockerignore 文件的内容和 Node.js 应用一样，都是忽略 node_modules 目录，以便加速镜像的构建速度。接下来，我们重点关注 Dockerfile 文件内容。 接下来，我们重点关注 Dockerfile 文件内容。 # syntax=docker/dockerfile:1 FROM node:lts-alpine RUN npm install -g http-server WORKDIR /app COPY package*.json ./ RUN npm install COPY . . RUN npm run build EXPOSE 8080 CMD [ \"http-server\", \"dist\" ] 简单分析一下上面 Dockerfile 的内容。首先使用 node:lts-alpine 作为基础镜像，然后安装 http-server 作为代理服务器，第 6 行代表的含义是，将 package.json 和 package-lock.json 复制到镜像内，并使用 npm install 安装依赖。这里让依赖安装和源码安装解耦的目的是尽量使用 Docker 镜像构建缓存，只要在 package.json 文件内容不变的情况下，即便是源码改变，都可以使用已经下载好的 npm 依赖缓存。 依赖安装完毕后，第 8 行，我们要将项目源码复制到镜像内，并且通过 npm run build 来构建 dist 目录，最后，第 12 行，使用 http-server 来启动 dist 目录的静态文件。现在，我们可以通过 docker build 来构建镜像了。 docker build -t vue . 接下来，使用 docker run 启动镜像。 $ docker run --publish 8080:8080 vue Starting up http-server, serving dist http-server version: 14.1.1 http-server settings: CORS: disabled ...... 到这里，你可以打开浏览器访问 http://localhost:8080 ，如果出现 Vue 示例应用的项目，说明镜像构建完成，如下图所示。 4.2 Nginx 构建方式 在上面的例子中，我们使用 http-server 来对外提供服务，这在开发和测试场景，或者是在小型的使用场景中是完全可以的。不过，在正式的生产环境中，我推荐你把 Nginx 作为反向代理服务器来对外提供服务，它也是性能最好、使用最广泛和稳定性最高的一种方案。 在 Vue 示例项目的同级目录下，我已经创建好了名为 Dockerfile-Nginx 文件。 # syntax=docker/dockerfile:1 FROM node:lts-alpine as build-stage WORKDIR /app COPY package*.json ./ RUN npm install COPY . . RUN npm run build FROM nginx:stable-alpine as production-stage COPY --from=build-stage /app/dist /usr/share/nginx/html EXPOSE 80 CMD [\"nginx\", \"-g\", \"daemon off;\"] 这个 Dockerfile 定义了两个构建阶段，第一个阶段是第 3 行到第 8 行的内容，其他的是第二阶段的内容。 第一阶段的构建过程和我们在上面提到的 http-server 的构建方式非常类似，它是以 node:lts-alpine 为基础镜像，同时复制 package.json 和 package-lock.json 并安装依赖，然后再复制项目源码并且执行 npm run build 来构建项目，生成 dist 目录。 第二个阶段的构建过程则是引入了一个新的 nginx:stable-alpine 镜像作为运行镜像，还将第一阶段构建的 dist 目录复制到了第二阶段的 /usr/share/nginx/html 目录中。这个目录是 Nginx 默认的网页目录，默认情况下，Nginx 将使用该目录的内容作为静态资源。最后第 13 行以前台的方式启动 Nginx。 现在，我们可以通过 docker build 来构建镜像。 $ docker build -t vue-nginx -f Dockerfile-Nginx . 接下来，使用 docker run 启动镜像。 $ docker run --publish 8080:80 vue-nginx /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh ...... 最后，打开浏览器访问 http://localhost:8080 验证一下，如果出现和前面提到的 http-server 构建方式一样的 Vue 示例应用界面，就说明镜像构建成功了。 5. 构建多平台镜像 了，上面的案例，我们都是通过在本地执行 docker build 命令来构建镜像，然后在本地通过 docker run 命令来执行的。实际上，在构建镜像时，Docker 会默认构建本机对应平台的镜像，例如常见的 AMD64 平台，这在大多数情况是适用的。但是，当我们使用不同平台的设备尝试启动这个镜像时，可能会遇到下面的问题。 WARNING: The requested image's platform (linux/arm64/v8) does not match the detected host platform (linux/amd64) and no specific platform was requested 产生这个问题的原因是，构建和运行设备的 CPU 平台存在差异。在实际项目中，最典型的例子是构建镜像的计算机是 AMD64 架构，但运行镜像的机器是 ARM64。要查看镜像适用于什么平台，你可以找到 DockerHub 镜像详情页。例如， Alpine 镜像适用的平台就可以在这个链接查看，详情页截图如下。 从这个页面我们可以看出，Apline 镜像适用的平台非常多，例如 Linux/386、Linux/amd64 等等。一般情况下，在构建镜像时，我们只会构建本机平台的镜像，但是当拉取镜像时，Docker 会自动拉取符合当前平台的镜像版本。那么，怎么才能真正实现跨平台的“一次构建，到处运行”目标呢？Docker 为我们提供了构建多平台镜像的方法：buildx。 5.1 初始化 要使用 Buildx，首先需要创建构建器，你可以使用 docker buildx create 命令来创建它，并将其命名为 mybuilder。 $ docker buildx create --name builder builder 然后，将 mybuilder 设置为默认的构建器。 $ docker buildx use builder 接下来，初始化构建器，这一步主要是启动 buildkit 容器。 $ docker buildx inspect --bootstrap [+] Building 19.1s (1/1) FINISHED => [internal] booting buildkit 19.1s => => pulling image moby/buildkit:buildx-stable-1 18.3s => => creating container buildx_buildkit_mybuilder0 0.8s Name: builder Driver: docker-container Nodes: Name: mybuilder0 Endpoint: unix:///var/run/docker.sock Status: running Buildkit: v0.10.4 Platforms: linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/arm64, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/mips64le, linux/mips64, linux/arm/v7, linux/arm/v6 初始化完成后，我们可以从返回结果中看到支持的平台，例如 Linux/amd64、Linux/arm64 等。 5.2 构建多平台镜像 这时候，我们就可以尝试使用 buildx 来构建多平台镜像了。我已经提前编写好了一个简单示例，在将示例应用克隆到本地后，你可以进入 docker/13/multi-arch 目录并查看。 $ cd gitops/docker/13/multi-arch $ ls -al -rw-r--r-- 1 weiwang staff 439 10 5 23:49 Dockerfile -rw-r--r-- 1 weiwang staff 1075 10 5 18:34 go.mod -rw-r--r-- 1 weiwang staff 6962 10 5 18:34 go.sum -rw-r--r-- 1 weiwang staff 397 10 5 18:39 main.go main.go 是示例应用的主体文件，我们启动一个 HTTP 服务器，访问根路径可以返回 Runtime 包的一些内置变量。 package main import ( \"net/http\" \"runtime\" \"github.com/gin-gonic/gin\" ) var ( r = gin.Default() ) func main() { r.GET(\"/\", indexHandler) r.Run(\":8080\") } func indexHandler(c *gin.Context) { var osinfo = map[string]string{ \"arch\": runtime.GOARCH, \"os\": runtime.GOOS, \"version\": runtime.Version(), } c.JSON(http.StatusOK, osinfo) } 相比较单一平台的构建方法，在构建多平台镜像的时候，我们可以在 Dockerfile 内使用一些内置变量，例如 BUILDPLATFORM、TARGETOS 和 TARGETARCH，他们分别对应构建平台（例如 Linux/amd64）、系统（例如 Linux）和架构（例如 AMD64）。 # syntax=docker/dockerfile:1 FROM --platform=$BUILDPLATFORM golang:1.18 as build ARG TARGETOS TARGETARCH WORKDIR /opt/app COPY go.* ./ RUN go mod download COPY . . RUN --mount=type=cache,target=/root/.cache/go-build \\ GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o /opt/app/example . FROM ubuntu:latest WORKDIR /opt/app COPY --from=build /opt/app/example ./example CMD [\"/opt/app/example\"] 这个 Dockerfile 包含两个构建阶段，第一个构建阶段是从第 2 行至第 9 行，第二个构建阶段是从第 11 行到第 14 行。 我们先看第一个构建阶段。 第 2 行 FROM 基础镜像增加了一个 --platform=$BUILDPLATFORM 参数，它代表“强制使用不同平台的基础镜像”，例如 Linux/amd64。在没有该参数配置的情况下，Docker 默认会使用构建平台（本机）对应架构的基础镜像。 第 3 行 ARG 声明了使用两个内置变量 TARGETOS 和 TARGETARCH，TARGETOS 代表系统，例如 Linux，TARGETARCH 则代表平台，例如 Amd64。这两个参数将会在 Golang 交叉编译时生成对应平台的二进制文件。 第 4 行 WORKDIR 声明了工作目录。 第 5 行的意思是通过 COPY 将 go.mod 和 go.sum 拷贝到镜像中，并在第 6 行使用 RUN 来运行 go mod download 下载依赖。这样，在这两个文件不变的前提下，Docker 将使用构建缓存来加快构建速度。 在下载完依赖之后，我们通过第 7 行把所有源码文件复制到镜像内。 第 8 行有两个含义，首先， --mount=type=cache,target=/root/.cache/go-build 的目的是告诉 Docker 使用 Golang 构建缓存，加快镜像构建的速度。接下来，GOOS=$TARGETOSGOARCH=$TARGETARCH go build -o /opt/app/example . 代表的含义是 Golang 交叉编译。注意，TARGETOS和TARGETARCH 是我们提到的内置变量，在具体构建镜像的时候，Docker 会帮我们填充进去。 第二个构建阶段比较简单，主要是使用 ubuntu:latest 基础镜像，将第一个构建阶段生成的二进制文件复制到镜像内，然后指定镜像的启动命令。 接下来，我们就可以开始构建多平台镜像了。在开始构建之前，先执行 docker login 登录到 DockerHub。 $ docker login Username: Password: Login Succeeded 接下来，使用 docker buildx build 一次性构建多平台镜像 $ docker buildx build --platform linux/amd64,linux/arm64 -t lyzhang1999/multi-arch:latest --push . 在这个命令中，我们使用 --platform 参数指定了两个平台：Linux/amd64 和 Linux/arm64，同时 -t 参数指定了镜像的 Tag，而 --push 参数则代表构建完成后直接将镜像推送到 DockerHub。 还记得我们在 Dockerfile 第 2 行增加的 --platform=$BUILDPLATFORM 参数吗？当执行这条命令时，Docker 会分别使用 Amd64 和 Arm64 两个平台的 golang:1.18 镜像，并且在对应的镜像内执行编译过程。执行完命令后，镜像会上传到 DockerHub 平台。进入这个镜像详情页我们就会发现它同时兼容了 Amd64 和 Arm64 两个平台。这样，多平台镜像就构建完成了。 6. 总结 在这节课，我为你介绍了主流语言镜像构建的案例，包括后端语言 Golang、Java、Node.js 以及前端 Vue 框架。在这些案例中，我尽量按照真实的生产环境来编写 Dockerfile，我使用到了一些 Dockerfile 的高级用法，例如多阶段构建、使用缓存和使用 .dockerignore 等，这些用法可以帮助我们加速构建镜像和缩小镜像大小。当你需要将实际的业务进行容器化改造时，可以直接参考我编写的案例。 此外，我还介绍了如何使用 buildx 构建多平台镜像。在这一部分，我通过一个真实的例子介绍了如何构建 Golang 的多平台镜像。一般情况下，多平台镜像并不常用，但如果你构建的镜像需要兼容不同的 CPU 平台，那就可以通过这种方法来实现。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 13:44:31 "},"Docker/Image/docker_Image_volume_reduced.html":{"url":"Docker/Image/docker_Image_volume_reduced.html","title":"如何将镜像体积缩减 90","keywords":"","body":"如何将镜像体积缩减 90%？1. 背景2. 新手构建 Golang 镜像3. 替换基础镜像4. 重新思考 Dockerfile5. 多阶段构建6. 进一步压缩7. 极限压缩8. 如何复用构建缓存？9. 总结如何将镜像体积缩减 90%？ 1. 背景 构建镜像慢、构建镜像过大等问题，这会导致推送镜像变得缓慢，同时也会导致在 Kubernetes 更新应用镜像版本时拉取镜像的过程也变得缓慢，从而影响整体应用发布效率 准备好的示例应用仓库克隆到本地：https://github.com/lyzhang1999/gitops.git。 2. 新手构建 Golang 镜像 cd gitops/docker/13/golang 大部分人在最开始编写的 Dockerfile 的时候都以“能用”作为首要目标，内容和 Golang 应用中的 Dockerfile-1 文件类似。 # syntax=docker/dockerfile:1 FROM golang:1.17 WORKDIR /opt/app COPY . . RUN go build -o example CMD [\"/opt/app/example\"] 这个 Dockerfile 描述的构建过程非常简单，我们首选 Golang:1.17 版本的镜像作为编译环境，将源码拷贝到镜像中，然后运行 go build 编译源码生成二进制可执行文件，最后配置启动命令。接下来，我们使用 Dockerfile-1 文件来构建镜像。 docker build -t golang:1 -f Dockerfile-1 . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE golang 1 751ee3477c3d 5 minutes ago 903MB 返回的结果来看，这个 Dockerfile 构建的镜像大小非常惊人，Golang 示例程序使用 go build 命令编译后，二进制可执行文件大约 6M 左右，但容器化之后，镜像达到 900M，显然我们需要进一步优化镜像大小。 3. 替换基础镜像 怎么做呢？我们构建的 Golang 镜像的大小很大程度是由引入的基础镜像的大小决定的，在这种情况下，替换基础镜像是一个快速并且非常有效的办法。例如，将 Golang:1.17 基础镜像替换为 golang:1.17-alpine 版本。 # syntax=docker/dockerfile:1 FROM golang:1.17-alpine WORKDIR /opt/app COPY . . RUN go build -o example CMD [\"/opt/app/example\"] 一般来说，Alpine 版本的镜像相比较普通镜像来说删除了一些非必需的系统应用，所以镜像体积更小。接下来，我们使用 Dockerfile-2 文件来构建镜像。 $ docker build -t golang:2 -f Dockerfile-2 . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE golang 2 bbaa9e935080 4 minutes ago 408MB golang 1 751ee3477c3d 5 minutes ago 903MB 通过对比发现，新的 Dockerfile-2 构建的镜像比 Dockerfile-1 构建的镜像在大小上缩减了 50%，只有 408M 了。 4. 重新思考 Dockerfile 让我们进一步分析一下 Dockerfile-2 文件的内容。 # syntax=docker/dockerfile:1 FROM golang:1.17-alpine WORKDIR /opt/app COPY . . RUN go build -o example CMD [\"/opt/app/example\"] 这段 Dockerfile 可以看出，我们在容器内运行了 go build -o example，这条命令将会编译生成二进制的可执行文件，由于编译的过程中需要 Golang 编译工具的支持，所以我们必须要使用 Golang 镜像作为基础镜像，这是导致镜像体积过大的直接原因。既然如此，那么我能不能不在镜像里编译呢？这样不依赖镜像的编译工具，再使用一个体积更小的镜像来运行程序，构建出来的镜像自然就会变小了。思路完全没错，那么我们要怎么做呢？最简单的办法就是在本地先编译出可执行文件，再将它复制到一个更小体积的 ubuntu 镜像内。具体做法是，首先在本地使用交叉编译生成 Linux 平台的二进制可执行文件。 $ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o example . $ ls -lh -rwxr-xr-x 1 wangwei staff 6.4M 10 10 16:58 example ...... 接下来，使用 Dockerfile-3 文件构建镜像。 # syntax=docker/dockerfile:1 FROM ubuntu:latest WORKDIR /opt/app COPY example ./ CMD [\"/opt/app/example\"] 因为不再需要在容器里进行编译，所以我们直接引入了不包含 Golang 编译工具的 ubuntu 镜像作为基础运行环境，接下来使用 docker build 命令构建镜像。 $ docker build -t golang:3 -f Dockerfile-3 . 构建完成后，使用 docker images 来查看镜像大小。 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE golang 3 b53404869778 3 minutes ago 75.9MB golang 2 bbaa9e935080 4 minutes ago 408MB golang 1 751ee3477c3d 5 minutes ago 903MB 从返回内容可以看出，这种构建方式生成的镜像只有 76M，在体积上比最初的 917M 缩小了几乎 90% 。镜像的最终大小就相当于 ubuntu:latest 的大小加上 Golang 二进制可执行文件的大小。不过，这种方式将应用的编译过程拆分到了宿主机上，这会让 Dockerfile 失去描述应用编译和打包的作用，不是一个好的实践。 其实，我们仔细分析上面的构建方法，会发现它的本质是把构建和运行拆分为两个阶段，构建由本地环境的编译工具提供支持，运行由 ubuntu 镜像提供支持。那么，能不能将这个思想迁移到 Dockerfile 的构建过程中呢？说到这里，我相信你已经能联想到我们上节课提到的“多阶段构建”了，思路是不是非常一致？ 5. 多阶段构建 在我们上节课的镜像构建案例中，多阶段构建的本质其实就是将镜像构建过程拆分成编译过程和运行过程。第一个阶段对应编译的过程，负责生成可执行文件；第二个阶段对应运行过程，也就是拷贝第一阶段的二进制可执行文件，并为程序提供运行环境，最终镜像也就是第二阶段生成的镜像如下图所示。 通过这张原理图，我相信你已经发现了一个很有意思的结论。以 Golang 示例应用为例，多阶段构建其实就是将 Dockerfile-1 和 Dockerfile-3 的内容进行合并重组，最终完整的多阶段构建的 Dockerfile-4 内容如下。 # syntax=docker/dockerfile:1 # Step 1: build golang binary FROM golang:1.17 as builder WORKDIR /opt/app COPY . . RUN go build -o example # Step 2: copy binary from step1 FROM ubuntu:latest WORKDIR /opt/app COPY --from=builder /opt/app/example ./example CMD [\"/opt/app/example\"] 这段内容里有两个 FROM 语句，所以这是一个包含两个阶段的构建过程。 第一个阶段是从第 4 行至第 7 行，它的作用是编译生成二进制可执行文件，就像我们之前在本地执行的编译操作一样。 第二阶段在第 10 行到 13 行，它的作用是将第一阶段生成的二进制可执行文件复制到当前阶段，把 ubuntu:latest 作为运行环境，并设置 CMD 启动命令。 接下来，我们使用 docker build 构建镜像，并将其命名为 golang:4。 docker build -t golang:4 -f Dockerfile-4 . 构建完成后，使用 docker images 查看镜像大小。 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE golang 4 8d40b16bb409 2 minutes ago 75.8MB golang 3 b53404869778 3 minutes ago 75.9MB golang 2 bbaa9e935080 4 minutes ago 408MB golang 1 751ee3477c3d 5 minutes ago 903MB 从返回结果我们可以看到，golang:4 镜像大小和 golang:3 镜像大小几乎一致，大约为 76M。到这里，对镜像大小的优化已经基本上完成了，镜像大小也在可接受的范围内。在实际的项目中，我也推荐你使用 ubuntu:latest 作为第二阶段的程序运行镜像。不过，为了让你深入理解多阶段构建，我们还可以尝试进一步压缩构建的镜像大小。 6. 进一步压缩 当我们使用多阶段构建时，最终生成的镜像大小其实取决于第二阶段引用的镜像大小，它在上面的例子中对应的是 ubuntu:latest 镜像大小。要进一步缩小体积，我们可以继续使用其他更小的镜像作为第二阶段的运行镜像，这就要说到 Alpine 了。Alpine 镜像是专门为容器化定制的 Linux 发行版，它的最大特点是体积非常小。现在，我们尝试使用它，将第二阶段构建的镜像替换为 Alpine 镜像，修改后的文件命名为 Dockerfile-5，内容如下。 # syntax=docker/dockerfile:1 # Step 1: build golang binary FROM golang:1.17 as builder WORKDIR /opt/app COPY . . RUN CGO_ENABLED=0 go build -o example # Step 2: copy binary from step1 FROM alpine WORKDIR /opt/app COPY --from=builder /opt/app/example ./example CMD [\"/opt/app/example\"] 由于 Alpine 镜像并没有 glibc，所以我们在编译可执行文件时指定了 CGO_ENABLED=0，这意味着我们禁用了 CGO，这样程序才能在 Alpine 镜像中运行。接着我们使用 Dockerfile-5 构建镜像，并将镜像命名为 golang:5。 docker build -t golang:5 -f Dockerfile-5 . 构建完成后，使用 docker images 查看镜像大小。 $ ❯ docker images REPOSITORY TAG IMAGE ID CREATED SIZE golang 5 7b2de55bf367 About a minute ago 11.9MB golang 4 8d40b16bb409 2 minutes ago 75.8MB golang 3 b53404869778 3 minutes ago 75.9MB golang 2 bbaa9e935080 4 minutes ago 408MB golang 1 751ee3477c3d 5 minutes ago 903MB 从返回的结果我们得知，使用 Alpine 镜像作为第二阶段的运行镜像后，镜像大小从 76M 降低至了 12M。不过，由于 Alpine 镜像和常规 Linux 发行版存在一些差异，作为初学者，我并不推荐你在生产环境下把 Alpine 镜像作为业务的运行镜像。 7. 极限压缩 从前面的操作可以看出，如果把 Alpine 镜像作为第二阶段的镜像，得到的镜像已经足够小了，相比较 7M 的可执行文件大小，镜像只增加了 5M 大小。但是我们有没有可能再极端一点，让多阶段构建的镜像大小和二进制可执行文件的大小保持一致呢？ 答案是肯定的，我们只需要把第二个阶段的镜像替换为一个“空镜像”，这个空镜像称为 scratch 镜像，我们将 Dockerfile-4 第二阶段的构建替换为 scratch 镜像，修改后的文件命名为 Dockerfile-6，内容如下。 # syntax=docker/dockerfile:1 # Step 1: build golang binary FROM golang:1.17 as builder WORKDIR /opt/app COPY . . RUN CGO_ENABLED=0 go build -o example # Step 2: copy binary from step1 FROM scratch WORKDIR /opt/app COPY --from=builder /opt/app/example ./example CMD [\"/opt/app/example\"] 注意，由于 scratch 镜像不包含任何内容，所以我们在编译 Golang 可执行文件的时候禁用了 CGO，这样才能让编译出来的程序在 scratch 镜像中运行。接着，我们使用 docker build 构建这个镜像，将其命名为 golang:5，然后再查看镜像大小，你会发现镜像和 Golang 可执行文件的大小是一致的，只有 6.6M。 $ docker build -t golang:6 -f Dockerfile-6 . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE golang 6 aa61f2cff23d 35 seconds ago 6.63MB golang 5 7b2de55bf367 About a minute ago 11.9MB golang 4 8d40b16bb409 2 minutes ago 75.8MB golang 3 b53404869778 3 minutes ago 75.9MB golang 2 bbaa9e935080 4 minutes ago 408MB golang 1 751ee3477c3d 5 minutes ago 903MB scratch 镜像是一个空白镜像，甚至连 shell 都没有，所以我们也无法进入容器查看文件或进行调试。在生产环境中，如果对安全有极高的要求，你可以考虑把 scratch 作为程序的运行镜像。 8. 如何复用构建缓存？ 到这里，相信你已经理解多阶段构建的实际意义了。不过，因为上面的 Dockerfile 还可以做进一步的优化，我还想再插播一个知识点。比如，在第一阶段的构建过程中，我们先是用 COPY . . 的方式拷贝了源码，又进行了编译，这会产生一个缺点，那就是如果只是源码变了，但依赖并没有变，Docker 将无法复用依赖的镜像层缓存。在实际构建过程中，你会发现 Docker 每次都会重新下载 Golang 依赖。 这就引出了另外一个构建镜像的小技巧：尽量使用 Docker 构建缓存。 要使用 Golang 依赖的缓存，最简单的办法是：先复制依赖文件，再下载依赖，最后再复制源码进行编译。基于这种思路，我们可以将第一阶段的构建修改如下。 # Step 1: build golang binary FROM golang:1.17 as builder WORKDIR /opt/app COPY go.* ./ RUN go mod download COPY . . RUN go build -o example 这样，在每次代码变更而依赖不变的情况下，Docker 都会复用第 4 行和第 5 行产生的构建缓存，这可以加速镜像构建过程。 9. 总结 以构建 Golang 镜像为例子，向你展示了减小镜像体积的具体方法，不管是最常见的更换基础镜像，还是多阶段构建，都可以有效地减小镜像体积。但是不同构建方法对应的镜像大小仍然有很大差异。 多阶段镜像构建方法，它巧妙地将构建和运行环境拆分开来，大大缩小了最终生成的镜像体积。在实际工作中，我强烈推荐你使用它。另外，我还在多阶段构建介绍了一种尽量利用 Docker 缓存的构建技巧，虽然这种方法对于缩小镜像没有帮助，但它能够加快镜像构建的速度。不过要强调的是，镜像并不是越小越好，我们需要同时兼顾镜像的可调试、安全、可维护性等角度来选择基础镜像，并将镜像大小控制在合理的范围内。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 13:47:42 "},"Docker/Image/docker_image_1_library.html":{"url":"Docker/Image/docker_image_1_library.html","title":"docker 镜像源","keywords":"","body":"docker 镜像源1. 镜像源2. 私人阿里镜像加速器3. 国内镜像源3.1 dockerhub (docker.io)3.2 gcr.io3.3 quay.io3.4 k8s.gcr.io3.5 阿里云的google 镜像源3.6 定制命令拉取镜像docker 镜像源 1. 镜像源 网易：http://hub-mirror.c.163.com 中科大镜像地址：http://mirrors.ustc.edu.cn/ 中科大github地址：https://github.com/ustclug/mirrorrequest Azure中国镜像地址：http://mirror.azure.cn/ Azure中国github地址：https://github.com/Azure/container-service-for-azure-china DockerHub镜像仓库: https://hub.docker.com/ 阿里云镜像仓库： https://cr.console.aliyun.com google镜像仓库： https://console.cloud.google.com/gcr/images/google-containers/GLOBAL （如果你本地可以翻墙的话是可以连上去的 ） coreos镜像仓库： https://quay.io/repository/ RedHat镜像仓库： https://access.redhat.com/containers 2. 私人阿里镜像加速器 这里采用了阿里云的镜像加速器（需要阿里云账号进行登录），地址：阿里云 -> 容器镜像服务 -> 镜像工具 -> 镜像加速器。 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json 3. 国内镜像源 部分国外镜像仓库无法访问，但国内有对应镜像源，可以从以下镜像源拉取到本地然后重改tag即可： Azure Container Registry(ACR) 3.1 dockerhub (docker.io) #dockerhub(docker.io) #格式 dockerhub.azk8s.cn//: #原镜像地址示例，我们可能平时拉dockerhub镜像是直接docker pull nginx:1.15.但是docker client会帮你翻译成#docker pull docker.io/library/nginx:1.15 docker.io/library/nginx:1.15 #国内拉取示例 dockerhub.azk8s.cn/library/nginx:1.15 3.2 gcr.io #gcr.io #格式 gcr.azk8s.cn//: #原镜像地址示例 gcr.io/google-containers/pause:3.1 #国内拉取示例 gcr.azk8s.cn/google_containers/pause:3.1 3.3 quay.io #quay.io #格式 quay.azk8s.cn//: #原镜像地址示例 quay.io/coreos/etcd:v3.2.28 #国内拉取示例 quay.azk8s.cn/coreos/etcd:v3.2.28 3.4 k8s.gcr.io #k8s.gcr.io #格式 gcr.azk8s.cn/google_containers//: #原镜像地址示例 k8s.gcr.io/pause-amd64:3.1 #国内拉取示例 gcr.azk8s.cn/google_containers/pause:3.1 #原镜像格式 k8s.gcr.io/pause:3.1 #改为以下格式 googlecontainersmirrors/pause:3.1 3.5 阿里云的google 镜像源 #原镜像格式 k8s.gcr.io/pause:3.1 #改为以下格式 registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 3.6 定制命令拉取镜像 或使用azk8spull，只有50行命令的小脚本，就可以从dockerhub、gcr.io、quay.io直接拉取镜像： #download azk8spull curl -Lo /usr/local/bin/azk8spull https://github.com/xuxinkun/littleTools/releases/download/v1.0.0/azk8spull chmod +x /usr/local/bin/azk8spull ​ #直接拉取镜像 azk8spull k8s.gcr.io/pause:3.1 azk8spull quay.io/coreos/etcd:v3.2.28 ​ #查看拉取的镜像 # docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/etcd v3.2.28 b2756210eeab 3 months ago 247MB k8s.gcr.io/pause 3.1 google镜像 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Image/docker_image_2_manage.html":{"url":"Docker/Image/docker_image_2_manage.html","title":"docker 镜像管理","keywords":"","body":"Docker 镜像管理1. 拉取 (docker pull)2. 查看配置信息 (docker inspect)3. 修改tag (docker tag)4. 打包与解包 (docker save|load)5. 推送 (docker push)将本地的镜像推送至公共镜像源的自己仓库，一般区别官方镜像，为私人定制。ghostwritten为我的仓库名6. 删除 (docker rmi)7. 构建（docker build）Docker 镜像管理 dockerhub官网：https://hub.docker.com/ prometheus镜像为示例 1. 拉取 (docker pull) $ docker search prom/prometheus #搜索镜像 $ docker pull prom/prometheus #默认版本latest $ docker pull docker.io/prom/prometheus #默认版本latest $ docker pull docker.io/prom/prometheus:2.3.1 # 指定版本 $ docker images #查看镜像列表 $ docker images -a #标签得镜像也能被展示 2. 查看配置信息 (docker inspect) $ docker inspect prom/prometheus:latest 3. 修改tag (docker tag) $ docker tag prom/prometheus:latest prom/prometheus:v1.0 #修改版本 $ docker tag prom/prometheus:latest docker.registry.localhost/prometheus:latest #修改仓库名，修改自己的私有仓库docker.registry.localhost（自定义） 4. 打包与解包 (docker save|load) 打包 $ docker save -o prometheus.tar prom/prometheus:latest #第一种方式打包 $ docker save > prometheus.tar prom/prometheus:latest #第二种方式打包 $ docker save -o monitor.tar prom/prometheus:latest prom/alertmanager:latest # 多个镜像打包 $ docker save prom/prometheus:latest | gzip -> prometheus.tar.gz 解包 $ docker load -i prometheus.tar #第一种方式解包 $ docker load 5. 推送 (docker push) 将本地的镜像推送至公共镜像源的自己仓库，一般区别官方镜像，为私人定制。ghostwritten为我的仓库名 $ docker push docker.io/ghostwritten/prometheus:latest 将本地的镜像推送至本地搭建的私有仓库，一般为内网集群环境公用。搭建私有仓库请点击 $ docker push docker.registry.localhost/prometheus:latest $ docker push docker.registry.localhost/monitor/prometheus:latest #加monitor tag方便区分镜像类型 6. 删除 (docker rmi) $ docker rmi prom/prometheus:latest $ docker rmi -f prom/prometheus:latest #-f 为强制删除 7. 构建（docker build） docker build命令会根据Dockerfile文件及上下文构建新Docker镜像。构建上下文是指Dockerfile所在的本地路径或一个URL（Git仓库地址）。构建上下文环境会被递归处理，所以，构建所指定的路径还包括了子目录，而URL还包括了其中指定的子模块。 OPTIONS说明： --build-arg=[] :设置镜像创建时的变量； --cpu-shares :设置 cpu 使用权重； --cpu-period :限制 CPU CFS周期； --cpu-quota :限制 CPU CFS配额； --cpuset-cpus :指定使用的CPU id； --cpuset-mems :指定使用的内存 id； --disable-content-trust :忽略校验，默认开启； -f :指定要使用的Dockerfile路径； --force-rm :设置镜像过程中删除中间容器； --isolation :使用容器隔离技术； --label=[] :设置镜像使用的元数据； -m :设置内存最大值； --memory-swap :设置Swap的最大值为内存+swap，\"-1\"表示不限swap； --no-cache :创建镜像的过程不使用缓存； --pull :尝试去更新镜像的新版本； --quiet, -q :安静模式，成功后只输出镜像 ID； --rm :设置镜像成功后删除中间容器； --shm-size :设置/dev/shm的大小，默认值是64M； --ulimit :Ulimit配置。 --tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 --network: 默认 default。在构建期间设置RUN指令的网络模式 常用命令 $ docker build . #默认使用当前目录下Dockerfile $ docker build . -f centosdockerfile #其他名称dockerfile，需要指定 $ docker build -f /path/to/a/Dockerfile . #递归目录下的dockerfile $ docker build -t ghostwritten/app . #指定镜像名 $ docker build -t ghostwritten/app:1.0.2 -t ghostwritten/app:latest . #指定多个tag #Dockerfile文件中的每条指令会被独立执行，并会创建一个新镜像，Docker 会重用已生成的中间镜像，以加速docker build的构建速度，也可以通过--cache-from指定 $ docker build -t ghostwritten/app --cache-from 31f630c65071 . $ docker build -t ghostwritten/app --no-cache . #不使用缓存 详细请参考docker官网 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Install/":{"url":"Docker/Install/","title":"Install","keywords":"","body":"Docker Install Overview1. 简介2. 桌面3. 服务器Docker Install Overview 1. 简介 Docker Engine 可通过 Docker Desktop 在各种Linux 平台、 macOS和Windows 10 上使用，并且可以作为静态二进制安装使用。在下方找到您首选的操作系统。 2. 桌面 平台 x86_64 / amd64 arm64 (Apple Silicon)） 适用于 Linux 的 Docker 桌面 ✓ 适用于 Mac (macOS) 的 Docker 桌面 ✓ ✓ 适用于 Windows 的 Docker 桌面 ✓ 3. 服务器 平台 x86_64 / amd64 arm64 / aarch64 arm (32-bit) s390x CentOS ✓ ✓ Debian ✓ ✓ ✓ Fedora ✓ ✓ Raspbian ✓ RHEL ✓ SLES ✓ Ubuntu ✓ ✓ ✓ ✓ Binaries ✓ ✓ ✓ Docker Engine 有两种更新通道，stable 和 test： Stable 频道为您提供最新版本以供普遍使用。 test 频道提供在正式发布 (GA) 之前准备好进行测试的预发布版本。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Install/docker_1_install.html":{"url":"Docker/Install/docker_1_install.html","title":"docker 安装","keywords":"","body":"docker 安装1. centos 安装 docker2. docker 1.13 安装3. docker 17.09.0-ce 安装4. oracle 7.4 安装 dockerdocker 安装 1. centos 安装 docker yum-utils提供了yum-config-manager 效用，并device-mapper-persistent-data和lvm2由需要 devicemapper存储驱动程序 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 可选：启用边缘和测试存储库。这些存储库包含在docker.repo上面的文件中，但默认情况下处于禁用状态。您可以将它们与稳定的存储库一起启用。 sudo yum-config-manager --enable docker-ce-edge sudo yum-config-manager --enable docker-ce-test 禁用 $ sudo yum-config-manager --disable docker-ce-edge 按版本号排序结果 $ yum list docker-ce --showduplicates | sort -r $ sudo yum -y install docker-ce $ sudo systemctl start docke $ sudo docker run hello-world 卸载Docker包： $ sudo yum remove docker-ce 不会自动删除主机上的图像，容器，卷或自定义配置文件。删除所有图像，容器和卷： $ sudo rm -rf /var/lib/docker 2. docker 1.13 安装 导入安装源: rpm --import \"https://sks-keyservers.net/pks/lookup?op=get&search=0xee6d536cf7dc86e2d7d56f59a178ac6c6238f52e\" 如果没有响应使用：pgp.mit.edu 或keyserver.ubuntu.com yum-config-manager --add-repo https://packages.docker.com/1.13/yum/repo/main/centos/7 安装： yum makecache fast yum install -y docker-engine 注意：安装其他cs版本 yum list docker-engine.x86_64 --showduplicates |sort -r yum installdocker-engine- 3. docker 17.09.0-ce 安装 wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -ivh epel-release-latest-7.noarch.rpm wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-17.09.0.ce-1.el7.centos.x86_64.rpm yum -y install docker-engine-selinux yum -y remove selinux-policy-targeted-3.13.1-192.0.5.el7_5.6.noarch yum -y install docker-engine-selinux yum -y install container-selinux-2.68-1.el7.noarch.rpm yumdownloader --resolve container-selinux 4. oracle 7.4 安装 docker wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -ivh epel-release-latest-7.noarch.rpm yum -y install docker-engine-selinux yum -y remove selinux-policy-targeted-3.13.1-192.0.5.el7_5.6.noarch yum -y install docker-engine-selinux yum -y install container-selinux-2.68-1.el7.noarch.rpm yumdownloader --resolve container-selinux yum -y install docker-ce 参考： 安装 docker 下载 docker Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Issue/":{"url":"Docker/Issue/","title":"Issue","keywords":"","body":"Docker 问题讨论Docker 问题讨论 关于 Docker 遇到的困惑、问题、故障的解决方案。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Logging/":{"url":"Docker/Logging/","title":"Logging","keywords":"","body":"Logging OverviewLogging Overview Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Manage/":{"url":"Docker/Manage/","title":"Manage","keywords":"","body":"Docker 高级操作1. 容器的进程2. 命名空间3. chroot4. cgroups4.1 进程的CPU统计信息4.2 进程的内存配置4.3 如何配置cgroups?5. Seccomp / AppArmor6. Capabilities7. 容器镜像8. 创建空镜像9. 不使用Dockerfile创建镜像Docker 高级操作 1. 容器的进程 $ docker run -d --name=db redis:alpine $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e2edc005ea6e redis:alpine \"docker-entrypoint.s…\" 31 seconds ago Up 30 seconds 6379/tcp db #Docker容器启动一个名为redis-server的进程。在主机上，我们可以看到所有正在运行的进程，包括由Docker启动的进程。 $ ps aux | grep redis-server 999 1099 0.3 1.1 29156 11316 ? Ssl 08:50 0:00 redis-server *:6379 docker top db UID PID PPID C STIME TTY TIME CMD 999 1099 1083 0 08:50 ? 00:00:00 redis-server *:6379 #将列出所有子进程。 $ pstree -c -p -A $(pgrep dockerd) dockerd(679)-+-docker-containe(717)-+-docker-containe(1083)-+-redis-server(1099)-+-{bio_aof_fsync}(1134) | | | |-{bio_close_file}(1133) | | | |-{bio_lazy_free}(1135) | | | `-{jemalloc_bg_thd}(1136) | | |-{docker-containe}(1084) | | |-{docker-containe}(1085) | | |-{docker-containe}(1086) | | |-{docker-containe}(1087) | | |-{docker-containe}(1088) | | `-{docker-containe}(1089) | |-{docker-containe}(718) | |-{docker-containe}(719) | |-{docker-containe}(720) | |-{docker-containe}(721) | |-{docker-containe}(728) | |-{docker-containe}(757) | |-{docker-containe}(758) | `-{docker-containe}(766) |-{dockerd}(704) |-{dockerd}(705) |-{dockerd}(706) |-{dockerd}(713) |-{dockerd}(714) |-{dockerd}(715) |-{dockerd}(716) |-{dockerd}(734) `-{dockerd}(1047) $ DBPID=$(pgrep redis-server) $ echo Redis is $DBPID Redis is 1099 $ ls /proc 1 13 17 214 28 4 56 66 754 bus filesystems kpagecount partitions thread-self 10 130 170 215 29 473 57 67 8 cgroups fs kpageflags sched_debug timer_list 1083 131 18 22 3 475 58 674 844 cmdline interrupts loadavg schedstat timer_stats 1099 132 19 220 30 483 59 679 85 consoles iomem locks scsi tty 11 133 2 228 306 485 592 68 858 cpuinfo ioports mdstat self uptime 1172 134 20 23 309 5 6 698 86 crypto irq meminfo slabinfo version 12 135 200 235 31 52 60 7 87 devices kallsyms misc softirqs version_signature 124 14 203 24 32 53 61 707 9 diskstats kcore modules stat vmallocinfo 125 141 205 25 33 530 62 717 951 dma keys mounts swaps vmstat 126 15 21 26 34 54 63 72 957 driver key-users mtrr sys zoneinfo 127 16 210 264 35 540 64 725 acpi execdomains kmsg net sysrq-trigger 129 169 213 27 36 55 65 750 buddyinfo fb kpagecgroup pagetypeinfo sysvipc #每个进程都在不同的文件中定义了自己的配置和安全设置 $ ls /proc/$DBPID attr cmdline environ io mem ns pagemap schedstat stat timers autogroup comm exe limits mountinfo numa_maps personality sessionid statm uid_map auxv coredump_filter fd loginuid mounts oom_adj projid_map setgroups status wchan cgroup cpuset fdinfo map_files mountstats oom_score root smaps syscall clear_refs cwd gid_map maps net oom_score_adj sched stack task #例如，您可以查看和更新为该流程定义的环境变量 $ cat /proc/$DBPID/environ *:6379 $ docker exec -it db env PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=e2edc005ea6e TERM=xterm REDIS_VERSION=6.2.5 REDIS_DOWNLOAD_URL=http://download.redis.io/releases/redis-6.2.5.tar.gz REDIS_DOWNLOAD_SHA=4b9a75709a1b74b3785e20a6c158cab94cf52298aa381eea947a678a60d551ae HOME=/root 2. 命名空间 容器的基本组成部分之一是名称空间。名称空间的概念是限制进程可以看到和访问系统的某些部分，比如其他网络接口或进程。 当容器启动时，容器运行时(如Docker)将为进程创建新的命名空间。通过在它自己的Pid命名空间中运行进程，它将看起来像系统上的唯一进程。 可用的命名空间有: Mount (mnt) Process ID (pid) Network (net) Interprocess Communication (ipc) UTS (hostnames) User ID (user) Control group (cgroup) 在不使用Docker等运行时的情况下，进程仍然可以在自己的命名空间内运行。一个可以提供帮助的工具是unshare。 $ unshare --help Usage: unshare [options] [...] Run a program with some namespaces unshared from the parent. Options: -m, --mount[=] unshare mounts namespace -u, --uts[=] unshare UTS namespace (hostname etc) -i, --ipc[=] unshare System V IPC namespace -n, --net[=] unshare network namespace -p, --pid[=] unshare pid namespace -U, --user[=] unshare user namespace -f, --fork fork before launching --mount-proc[=] mount proc filesystem first (implies --mount) -r, --map-root-user map current user to root (implies --user) --propagation slave|shared|private|unchanged modify mount propagation in mount namespace -s, --setgroups allow|deny control the setgroups syscall in user namespaces -h, --help display this help and exit -V, --version output version information and exit 使用unshare，可以启动进程并让它创建一个新的名称空间，比如Pid。通过从主机取消Pid名称空间的共享，看起来bash提示符是唯一运行的进程： $ sudo unshare --fork --pid --mount-proc bash $ ps PID TTY TIME CMD 1 pts/0 00:00:00 bash 9 pts/0 00:00:00 ps $ exit exit 名称空间是磁盘上的索引节点位置。这允许进程shared/reused相同的名称空间，从而允许它们进行查看和交互。 #列出容器所有的名称空间 $ ls -lha /proc/$DBPID/ns/ total 0 dr-x--x--x 2 999 packer 0 Sep 27 08:50 . dr-xr-xr-x 9 999 packer 0 Sep 27 08:50 .. lrwxrwxrwx 1 999 packer 0 Sep 27 09:04 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 999 packer 0 Sep 27 08:52 ipc -> ipc:[4026532157] lrwxrwxrwx 1 999 packer 0 Sep 27 08:52 mnt -> mnt:[4026532155] lrwxrwxrwx 1 999 packer 0 Sep 27 08:50 net -> net:[4026532160] lrwxrwxrwx 1 999 packer 0 Sep 27 08:52 pid -> pid:[4026532158] lrwxrwxrwx 1 999 packer 0 Sep 27 08:52 user -> user:[4026531837] lrwxrwxrwx 1 999 packer 0 Sep 27 08:52 uts -> uts:[4026532156] 另一个工具nsenter用于将进程附加到现有的命名空间。对调试有用。 $ nsenter --target $DBPID --mount --uts --ipc --net --pid ps aux PID USER TIME COMMAND 1 redis 0:02 redis-server *:6379 16 root 0:00 ps aux 在Docker中，可以使用语法container:共享这些名称空间。例如，下面的命令将nginx连接到DB命名空间。 $ docker run -d --name=web --net=container:db nginx:alpine WEBPID=$(pgrep nginx | tail -n1) $ echo nginx is $WEBPID $ cat /proc/$WEBPID/cgroup 11:hugetlb:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 10:perf_event:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 9:cpu,cpuacct:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 8:cpuset:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 7:freezer:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 6:memory:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 5:pids:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 4:devices:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 3:net_cls,net_prio:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 2:blkio:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 1:name=systemd:/docker/42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 虽然网络已被共享，但它仍将作为名称空间列出。 $ ls -lha /proc/$WEBPID/ns/ total 0 dr-x--x--x 2 systemd-network systemd-journal 0 Sep 27 09:10 . dr-xr-xr-x 9 systemd-network systemd-journal 0 Sep 27 09:07 .. lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 ipc -> ipc:[4026532225] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 mnt -> mnt:[4026532223] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 net -> net:[4026532160] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 pid -> pid:[4026532226] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 user -> user:[4026531837] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 uts -> uts:[4026532224] 但是，这两个进程的网络名称空间指向相同的位置。 $ ls -lha /proc/$WEBPID/ns/ | grep net dr-x--x--x 2 systemd-network systemd-journal 0 Sep 27 09:10 . dr-xr-xr-x 9 systemd-network systemd-journal 0 Sep 27 09:07 .. lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 cgroup -> cgroup:[4026531835] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 ipc -> ipc:[4026532225] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 mnt -> mnt:[4026532223] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 net -> net:[4026532160] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 pid -> pid:[4026532226] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 user -> user:[4026531837] lrwxrwxrwx 1 systemd-network systemd-journal 0 Sep 27 09:10 uts -> uts:[4026532224] $ ls -lha /proc/$DBPID/ns/ | grep net lrwxrwxrwx 1 999 packer 0 Sep 27 08:50 net -> net:[4026532160] 3. chroot 容器进程的一个重要部分是能够拥有独立于主机的不同文件。这就是我们如何基于不同的操作系统在我们的系统上运行不同的Docker映像。 Chroot允许进程在父操作系统的不同根目录下启动。这允许不同的文件出现在根目录中。 4. cgroups CGroups限制了进程可以消耗的资源数量。这些cgroup是在/proc目录中的特定文件中定义的值。 需要查看映射关系，使用命令: $ cat /proc/$DBPID/cgroup 11:hugetlb:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 10:perf_event:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 9:cpu,cpuacct:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 8:cpuset:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 7:freezer:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 6:memory:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 5:pids:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 4:devices:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 3:net_cls,net_prio:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 2:blkio:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 1:name=systemd:/docker/e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 这些被映射到磁盘上的其他cgroup目录: $ ls /sys/fs/cgroup/ blkio cpuacct cpuset freezer memory net_cls,net_prio perf_event systemd cpu cpu,cpuacct devices hugetlb net_cls net_prio pids 4.1 进程的CPU统计信息 CPU统计数据和使用情况也存储在一个文件中! $ cat /sys/fs/cgroup/cpu,cpuacct/docker/$DBID/cpuacct.stat user 139 system 144 这里还定义了CPU共享限制。 $ cat /sys/fs/cgroup/cpu,cpuacct/docker/$DBID/cpu.shares 1024 4.2 进程的内存配置 所有用于容器内存配置的Docker cgroups都存储在: $ ls /sys/fs/cgroup/memory/docker/ 42e8d3ecb8137817669b58fd33c2b6e133bae8237513c7ed8af0a49a936248d1 memory.kmem.usage_in_bytes cgroup.clone_children memory.limit_in_bytes cgroup.event_control memory.max_usage_in_bytes cgroup.procs memory.move_charge_at_immigrate e2edc005ea6ef33fededdb8f7b58162665c81552a29a2ea777b8b9d05bd393d0 memory.numa_stat memory.failcnt memory.oom_control memory.force_empty memory.pressure_level memory.kmem.failcnt memory.soft_limit_in_bytes memory.kmem.limit_in_bytes memory.stat memory.kmem.max_usage_in_bytes memory.swappiness memory.kmem.slabinfo memory.usage_in_bytes memory.kmem.tcp.failcnt memory.use_hierarchy memory.kmem.tcp.limit_in_bytes notify_on_release memory.kmem.tcp.max_usage_in_bytes tasks memory.kmem.tcp.usage_in_bytes 4.3 如何配置cgroups? Docker的属性之一是控制内存限制的能力。这是通过cgroup设置完成的。 默认情况下，容器对内存没有限制。我们可以通过docker stats命令查看。 $ docker stats db --no-stream CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS e2edc005ea6e db 0.17% 6.754MiB / 992.1MiB 0.68% 1.3kB / 0B 0B / 0B 5 内存引号存储在一个名为memory.limit_in_bytes 通过写入文件，我们可以改变进程的限制 $ echo 8000000 > /sys/fs/cgroup/memory/docker/$DBID/memory.limit_in_bytes 如果将文件读回去，您将注意到它已被转换为7999488。 $ cat /sys/fs/cgroup/memory/docker/$DBID/memory.limit_in_bytes 7999488 当再次检查Docker Stats时，进程的内存限制现在是7.629M $ docker stats db --no-stream CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS e2edc005ea6e db 0.19% 6.176MiB / 992.1MiB 0.62% 1.3kB / 0B 61.4kB / 0B 5 5. Seccomp / AppArmor Linux中的所有操作都是通过系统调用来完成的。内核有330个系统调用，执行读取文件、关闭句柄和检查访问权限等操作。所有应用程序都使用这些系统调用的组合来执行所需的操作。 AppArmor是一个应用程序定义的配置文件，它描述了进程可以访问系统的哪些部分。 可以通过以下方式查看分配给进程的当前AppArmor配置文件 $ cat /proc/$DBPID/attr/current docker-default (enforce) Docker的默认AppArmor配置文件是docker-default (enforce) 在Docker 1.13之前，它将AppArmor配置文件存储在/etc/ AppArmor.d/Docker-default(在Docker启动时被覆盖，因此用户无法修改它。在v1.13之后，Docker现在在tmpfs中生成Docker -default，使用apparmor_parser将其加载到内核中，然后删除该文件 该模板可在https://github.com/moby/moby/blob/a575b0b1384b2ba89b79cbd7e770fbeb616758b3/profiles/apparmor/template.go Seccomp提供了限制系统调用的能力，阻止诸如安装内核模块或更改文件权限等方面。 Docker默认允许的调用可以在https://github.com/moby/moby/blob/a575b0b1384b2ba89b79cbd7e770fbeb616758b3/profiles/seccomp/default.json 当分配给进程时，它意味着进程将被限制在能力系统调用的子集。如果它试图调用一个被阻塞的系统调用将收到错误“操作不允许”。 SecComp的状态也在一个文件中定义。 $ cat /proc/$DBPID/status Name: redis-server State: S (sleeping) Tgid: 1099 Ngid: 0 Pid: 1099 PPid: 1083 TracerPid: 0 Uid: 999 999 999 999 Gid: 1000 1000 1000 1000 FDSize: 64 Groups: 1000 1000 NStgid: 1099 1 NSpid: 1099 1 NSpgid: 1099 1 NSsid: 1099 1 VmPeak: 29156 kB VmSize: 29156 kB VmLck: 0 kB VmPin: 0 kB VmHWM: 11316 kB VmRSS: 7032 kB VmData: 23204 kB VmStk: 132 kB VmExe: 1672 kB VmLib: 1656 kB VmPTE: 56 kB VmPMD: 12 kB VmSwap: 4324 kB HugetlbPages: 0 kB Threads: 5 SigQ: 0/3824 SigPnd: 0000000000000000 ShdPnd: 0000000000000000 SigBlk: 0000000000000000 SigIgn: 0000000000001001 SigCgt: 00000000000044ea CapInh: 00000000a80425fb CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 00000000a80425fb CapAmb: 0000000000000000 Seccomp: 2 Cpus_allowed: 3 Cpus_allowed_list: 0-1 Mems_allowed: 00000000,00000001 Mems_allowed_list: 0 voluntary_ctxt_switches: 24583 nonvoluntary_ctxt_switches: 507 $ cat /proc/$DBPID/status | grep Seccomp Seccomp: 2 标志位含义为:0:关闭。1:严格。2:过滤 6. Capabilities Capabilities是关于进程或用户有权做什么的分组。这些功能可能包括多个系统调用或操作，例如更改系统时间或主机名。 状态文件还包含了Capabilities标志。一个进程可以丢弃尽可能多的capability，以确保其安全。 $ cat /proc/$DBPID/status | grep ^Cap CapInh: 00000000a80425fb CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 00000000a80425fb CapAmb: 0000000000000000 标志被存储为一个可以用capsh解码的位掩码 $ capsh --decode=00000000a80425fb 0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcap 7. 容器镜像 容器映像是一个包含tar文件的tar文件。每个tar文件都是一个层。一旦所有tar文件被提取到相同的位置，那么您就拥有了容器的文件系统。 这可以通过Docker进行探索。把这些image放到本地系统上。 $ docker pull redis:3.2.11-alpine 3.2.11-alpine: Pulling from library/redis ff3a5c916c92: Pull complete aae70a2e6027: Pull complete 87c655da471c: Pull complete bc3141806bdc: Pull complete 53616fb426d9: Pull complete 9791c5883c6a: Pull complete Digest: sha256:ebf1948b84dcaaa0f8a2849cce6f2548edb8862e2829e3e7d9e4cd5a324fb3b7 Status: Downloaded newer image for redis:3.2.11-alpine 将images导出为原始tar格式。 docker save redis:3.2.11-alpine > redis.tar $ tar -xvf redis.tar 46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557/ 46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557/VERSION 46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557/json 46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557/layer.tar 498654318d0999ce36c7b90901ed8bd8cb63d86837cb101ea1ec9bb092f44e59/ 498654318d0999ce36c7b90901ed8bd8cb63d86837cb101ea1ec9bb092f44e59/VERSION 498654318d0999ce36c7b90901ed8bd8cb63d86837cb101ea1ec9bb092f44e59/json 498654318d0999ce36c7b90901ed8bd8cb63d86837cb101ea1ec9bb092f44e59/layer.tar ad01e7adb4e23f63a0a1a1d258c165d852768fb2e4cc2d9d5e71698e9672093c/ ad01e7adb4e23f63a0a1a1d258c165d852768fb2e4cc2d9d5e71698e9672093c/VERSION ad01e7adb4e23f63a0a1a1d258c165d852768fb2e4cc2d9d5e71698e9672093c/json ad01e7adb4e23f63a0a1a1d258c165d852768fb2e4cc2d9d5e71698e9672093c/layer.tar ca0b6709748d024a67c502558ea88dc8a1f8a858d380f5ddafa1504126a3b018.json da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c/ da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c/VERSION da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c/json da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c/layer.tar db1a23fc1daa8135a1c6c695f7b416a0ac0eb1d8ca873928385a3edaba6ac9a3/ db1a23fc1daa8135a1c6c695f7b416a0ac0eb1d8ca873928385a3edaba6ac9a3/VERSION db1a23fc1daa8135a1c6c695f7b416a0ac0eb1d8ca873928385a3edaba6ac9a3/json db1a23fc1daa8135a1c6c695f7b416a0ac0eb1d8ca873928385a3edaba6ac9a3/layer.tar f07352aa34c241692cae1ce60ade187857d0bffa3a31390867038d46b1e7739c/ f07352aa34c241692cae1ce60ade187857d0bffa3a31390867038d46b1e7739c/VERSION f07352aa34c241692cae1ce60ade187857d0bffa3a31390867038d46b1e7739c/json f07352aa34c241692cae1ce60ade187857d0bffa3a31390867038d46b1e7739c/layer.tar manifest.json repositories 所有的tar层文件现在都是可见的。 $ ls 46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557 498654318d0999ce36c7b90901ed8bd8cb63d86837cb101ea1ec9bb092f44e59 ad01e7adb4e23f63a0a1a1d258c165d852768fb2e4cc2d9d5e71698e9672093c ca0b6709748d024a67c502558ea88dc8a1f8a858d380f5ddafa1504126a3b018.json da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c db1a23fc1daa8135a1c6c695f7b416a0ac0eb1d8ca873928385a3edaba6ac9a3 f07352aa34c241692cae1ce60ade187857d0bffa3a31390867038d46b1e7739c manifest.json redis.tar repositories images还包含关于images的元数据，如版本信息和标记名称. $ cat repositories {\"redis\":{\"3.2.11-alpine\":\"46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557\"}} $ cat manifest.json [{\"Config\":\"ca0b6709748d024a67c502558ea88dc8a1f8a858d380f5ddafa1504126a3b018.json\",\"RepoTags\":[\"redis:3.2.11-alpine\"],\"Layers\":[\"498654318d0999ce36c7b90901ed8bd8cb63d86837cb101ea1ec9bb092f44e59/layer.tar\",\"ad01e7adb4e23f63a0a1a1d258c165d852768fb2e4cc2d9d5e71698e9672093c/layer.tar\",\"da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c/layer.tar\",\"db1a23fc1daa8135a1c6c695f7b416a0ac0eb1d8ca873928385a3edaba6ac9a3/layer.tar\",\"f07352aa34c241692cae1ce60ade187857d0bffa3a31390867038d46b1e7739c/layer.tar\",\"46a2fed8167f5d523f9a9c07f17a7cd151412fed437272b517ee4e46587e5557/layer.tar\"]}] tar中文件 $ tar -xvf da2a73e79c2ccb87834d7ce3e43d274a750177fe6527ea3f8492d08d3bb0123c/layer.tar etc/ etc/apk/ etc/apk/world lib/ lib/apk/ lib/apk/db/ lib/apk/db/installed lib/apk/db/lock lib/apk/db/scripts.tar lib/apk/db/triggers sbin/ sbin/su-exec var/ var/cache/ var/cache/misc/ 8. 创建空镜像 由于images只是一个tar文件，可以使用下面的命令创建空images $ tar cv --files-from /dev/null | docker import - empty sha256:ba14edd8949ad44677f1955e37b9a35f9978d2a687b3f8ab86711811d46e2c53 通过导入tar，将创建额外的元数据。 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE empty latest ba14edd8949a 24 seconds ago 0B redis 3.2.11-alpine ca0b6709748d 3 years ago 20.7MB 但是，由于容器不包含任何内容，所以它不能启动进程。 9. 不使用Dockerfile创建镜像 可以扩展前面导入Tar文件的想法，从零开始创建整个映像。 首先，我们将使用BusyBox作为基础。这将为我们提供基本的linux命令。它被定义为rootfs。rootfs是…… Docker提供了一个脚本来下载BusyBox rootfs https://github.com/moby/moby/blob/a575b0b1384b2ba89b79cbd7e770fbeb616758b3/contrib/mkimage/busybox-static $ curl -LO https://raw.githubusercontent.com/moby/moby/a575b0b1384b2ba89b79cbd7e770fbeb616758b3/contrib/mkimage/busybox-static && chmod +x busybox-static % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 782 100 782 0 0 2177 0 --:--:-- --:--:-- --:--:-- 2184 $ $ ./busybox-static busybox 运行该脚本将下载rootfs和主二进制文件。 $ ls -lha busybox total 20K drwxr-xr-x 5 root root 4.0K Sep 27 09:43 . drwx------ 15 root root 4.0K Sep 27 09:43 .. drwxr-xr-x 2 root root 4.0K Sep 27 09:43 bin drwxr-xr-x 2 root root 4.0K Sep 27 09:43 sbin drwxr-xr-x 4 root root 4.0K Sep 27 09:43 usr 默认的Busybox rootfs不包含任何版本信息，所以让我们创建一个文件。 $ echo KatacodaPrivateBuild > busybox/release 与前面一样，该目录可以转换为tar，并自动导入到Docker中作为映像。 $ tar -C busybox -c . | docker import - busybox sha256:73b25a6703da535db5cbc43073c2920b60f5f8a76db17de093e851f1a2d5f69c 现在可以将其作为容器启动。 $ docker run busybox cat /release KatacodaPrivateBuild Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Manage/docker_clean.html":{"url":"Docker/Manage/docker_clean.html","title":"docker 资源清理","keywords":"","body":"docker 资源清理1. 查看 docker 占用资源2. 清理docker 资源清理 1. 查看 docker 占用资源 docker container ls #默认只列出正在运行的容器，-a 选项会列出包括停止的所有容器。 docker image l s# 列出镜像信息，-a 选项会列出 intermediate 镜像(就是其它镜像依赖的层)。 docker volume ls #列出数据卷。 docker network ls #列出 network。 docker info #显示系统级别的信息，比如容器和镜像的数量等。 2. 清理 删除镜像 sudo docker rmi 删除容器 sudo docker rm 删除所有镜像 sudo docker rmi -a 删除所有容器 sudo docker rm -a 另外，容器的数据卷(volume)也是占用磁盘空间，可以通过以下命令删除失效的volume: sudo docker volume rm $(docker volume ls -qf dangling=true) 当然，最暴力的方式是删除Docker存储镜像，容器与数据卷的目录(/var/lib/docker) 谨慎使用！！！: sudo service docker stop sudo rm -rf /var/lib/docker sudo service docker start 只删除那些未被使用的资源 docker system prune 安全起见，这个命令默认不会删除那些未被任何容器引用的数据卷，如果需要同时删除这些数据卷， 你需要显式的指定 --volumns 参数。比如你可能想要执行下面的命令： docker system prune --all --force --volumns 镜像。这表示旧的镜像已经不再被引用了，此时它们就变成了 dangling images docker container prune # 删除所有退出状态的容器 docker volume prune # 删除未被使用的数据卷 docker network prune #清理没有再被任何容器引用的networks docker network prune --filter \"until=24h\" #清理没有被引用的、创建超过24小时的networks docker image prune # 删除 dangling 或所有未被使用的镜像 docker image prune -a #清除所有没有容器引用的镜像 docker image prune -a --filter \"until=24h\" # 只清除超过创建时间超过24小时的镜像 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Manage/docker_container_isolation_and_restrictions.html":{"url":"Docker/Manage/docker_container_isolation_and_restrictions.html","title":"docker 容器隔离与限制","keywords":"","body":"Docker 容器隔离与限制Docker 容器隔离与限制 Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 有意思的是，Google 的工程师在 2006 年发起这项特性的时候，曾将它命名为“进程容 器”（process container）。实际上，在 Google 内部，“容器”这个术语长期以来都被用于 形容被 Cgroups 限制过的进程组。后来 Google 的工程师们说，他们的 KVM 虚拟机也运行在 Borg 所管理的“容器”里，其实也是运行在 Cgroups“容器”当中。 这和我们今天说的 Docker 容器差别很大。 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能 够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。 在今天 的分享中，我只和你重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识 一下 Cgroups。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织 在操作系统的 /sys/fs/cgroup 路径下。 $ mount -t cgroup cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu) cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct) blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) ... 它的输出结果，是一系列文件系统目录。如果你在自己的机器上没有看到这些目录，那你就需要 自己去挂载 Cgroups，具体做法可以自行 Google。 可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫 子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资 源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就 可以看到如下几个配置文件，这个指令是： $ ls /sys/fs/cgroup/cpu cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只 能被分配到总量为 cfs_quota 的 CPU 时间。 而这样的配置文件又如何使用呢？ 你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下： root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container root@ubuntu:/sys/fs/cgroup/cpu$ ls container/ cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自 动生成该子系统对应的资源限制文件。 现在，我们在后台执行这样一条脚本： $ while : ; do : ; done & [1] 226 显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到 这个脚本在后台运行的进程号（PID）是 226。 这样，我们可以用 top 指令来确认一下 CPU 有没有被打满： $ top %Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 在输出里可以看到，CPU 的使用率已经 100% 了（%Cpu0 :100.0 us）。 而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）： $ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1 $ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 接下来，我们可以通过修改这些文件的内容来设置限制。 比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）： $ echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组 限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。 接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该 进程生效了： $ echo 226 > /sys/fs/cgroup/cpu/container/tasks 我们可以用 top 指令查看一下： $ top %Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 可以看到，计算机的 CPU 使用率立刻降到了 20%（%Cpu0 : 20.3 us）。 除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如： blkio，为 块 设 备 设 定 I/O 限 制，一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组 资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下 面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程 的 PID 填写到对应控制组的 tasks 文件中就可以了。 而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定 了，比如这样一条命令： $ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash 在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这 个控制组里的资源限制文件的内容来确认： $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。 参考： 白话容器基础（二）：隔离与限制 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:47 "},"Docker/Monitor/":{"url":"Docker/Monitor/","title":"Monitor","keywords":"","body":"Docker 监控 OverviewDocker 监控 Overview Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Network/":{"url":"Docker/Network/","title":"Network","keywords":"","body":"Docker 网络 OverviewDocker 网络 Overview Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Network/docker_analyze_the_packet_loss.html":{"url":"Docker/Network/docker_analyze_the_packet_loss.html","title":"docker 容器应用总是丢包如何分析","keywords":"","body":"Docker 容器应用总是丢包如何分析？1. 回顾2. 案例准备3. 案例分析3.1 链路层3.2 网络层和传输层3.3 iptables3.4 tcpdumpDocker 容器应用总是丢包如何分析？ 1. 回顾 容器化后的应用程序，问题分析结合： cgroups 会影响容器应用的运行； iptables 中的 NAT，会影响容器的网络性能； 叠加文件系统，会影响应用的 I/O 性能等。 就以最常用的反向代理服务器 Nginx 为例，学习丢包的分析方法 所谓丢包，是指在网络数据的收发过程中，由于种种原因，数据包还没传输到应用程序中，就被丢弃了。这些被丢弃包的数量，除以总的传输包数，也就是我们常说的丢包率。丢包率是网络性能中最核心的指标之一。 2. 案例准备 Ubuntu 18.04 两台 机器配置：2 CPU，8GB 内存 预先安装 docker、curl、hping3 等工具，如 apt install docker.io curl hping3。 3. 案例分析 在终端一中执行下面的命令，启动 Nginx 应用，并在 80 端口监听。如果一切正常，你应该可以看到如下的输出： $ docker run --name nginx --hostname nginx --privileged -p 80:80 -itd feisky/nginx:drop dae0202cc27e5082b282a6aeeb1398fcec423c642e63322da2a97b9ebd7538e0 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES dae0202cc27e feisky/nginx:drop \"/start.sh\" 4 minutes ago Up 4 minutes 0.0.0.0:80->80/tcp nginx 接着，我们切换到终端二中，执行下面的 hping3 命令，进一步验证 Nginx 是不是真的可以正常访问了。注意，这里我没有使用 ping，是因为 ping 基于 ICMP 协议，而 Nginx 使用的是 TCP 协议。 # -c表示发送10个请求，-S表示使用TCP SYN，-p指定端口为80 $ hping3 -c 10 -S -p 80 192.168.0.30 HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data bytes len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=3 win=5120 rtt=7.5 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=4 win=5120 rtt=7.4 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=5 win=5120 rtt=3.3 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=7 win=5120 rtt=3.0 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=3027.2 ms --- 192.168.0.30 hping statistic --- 10 packets transmitted, 5 packets received, 50% packet loss round-trip min/avg/max = 3.0/609.7/3027.2 ms 从 hping3 的输出中，我们可以发现，发送了 10 个请求包，却只收到了 5 个回复，50% 的包都丢了。再观察每个请求的 RTT 可以发现，RTT 也有非常大的波动变化，小的时候只有 3ms，而大的时候则有 3s。根据这些输出，我们基本能判断，已经发生了丢包现象。可以猜测，3s 的 RTT ，很可能是因为丢包后重传导致的。那到底是哪里发生了丢包呢？ 在这里，为了帮你理解网络丢包的原理，我画了一张图，你可以保存并打印出来使用： 从图中你可以看出，可能发生丢包的位置，实际上贯穿了整个网络协议栈。换句话说，全程都有丢包的可能。比如我们从下往上看： 在两台 VM 连接之间，可能会发生传输失败的错误，比如网络拥塞、线路错误等； 在网卡收包后，环形缓冲区可能会因为溢出而丢包； 在链路层，可能会因为网络帧校验失败、QoS 等而丢包； 在 IP 层，可能会因为路由失败、组包大小超过 MTU 等而丢包； 在传输层，可能会因为端口未监听、资源占用超过内核限制等而丢包； 在套接字层，可能会因为套接字缓冲区溢出而丢包； 在应用层，可能会因为应用程序异常而丢包； 此外，如果配置了 iptables 规则，这些网络包也可能因为 iptables 过滤规则而丢包。 现在我们切换回终端一，执行下面的命令，进入容器的终端中： $ docker exec -it nginx bash root@nginx:/# 么， 接下来，我们就可以从协议栈中，逐层排查丢包问题。 3.1 链路层 首先，来看最底下的链路层。当缓冲区溢出等原因导致网卡丢包时，Linux 会在网卡收发数据的统计信息中，记录下收发错误的次数。你可以通过 ethtool 或者 netstat ，来查看网卡的丢包记录。比如，可以在容器中执行下面的命令，查看丢包情况： root@nginx:/# netstat -i Kernel Interface table Iface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flg eth0 100 31 0 0 0 8 0 0 0 BMRU lo 65536 0 0 0 0 0 0 0 0 LRU 输出中的 RX-OK、RX-ERR、RX-DRP、RX-OVR ，分别表示接收时的总包数、总错误数、进入 Ring Buffer 后因其他原因（如内存不足）导致的丢包数以及 Ring Buffer 溢出导致的丢包数。 TX-OK、TX-ERR、TX-DRP、TX-OVR 也代表类似的含义，只不过是指发送时对应的各个指标。 注意，由于 Docker 容器的虚拟网卡，实际上是一对 veth pair，一端接入容器中用作 eth0，另一端在主机中接入 docker0 网桥中。veth 驱动并没有实现网络统计的功能，所以使用 ethtool -S 命令，无法得到网卡收发数据的汇总信息。 从这个输出中，我们没有发现任何错误，说明容器的虚拟网卡没有丢包。不过要注意，如果用 tc 等工具配置了 QoS，那么 tc 规则导致的丢包，就不会包含在网卡的统计信息中。所以接下来，我们还要检查一下 eth0 上是否配置了 tc 规则，并查看有没有丢包。我们继续容器终端中，执行下面的 tc 命令，不过这次注意添加 -s 选项，以输出统计信息： root@nginx:/# tc -s qdisc show dev eth0 qdisc netem 800d: root refcnt 2 limit 1000 loss 30% Sent 432 bytes 8 pkt (dropped 4, overlimits 0 requeues 0) backlog 0b 0p requeues 0 从 tc 的输出中可以看到， eth0 上面配置了一个网络模拟排队规则（qdisc netem），并且配置了丢包率为 30%（loss 30%）。再看后面的统计信息，发送了 8 个包，但是丢了 4 个。看来，应该就是这里，导致 Nginx 回复的响应包，被 netem 模块给丢了。既然发现了问题，解决方法也就很简单了，直接删掉 netem 模块就可以了。我们可以继续在容器终端中，执行下面的命令，删除 tc 中的 netem 模块： root@nginx:/# tc qdisc del dev eth0 root netem loss 30% 删除后，问题到底解决了没？我们切换到终端二中，重新执行刚才的 hping3 命令，看看现在还有没有问题： $ hping3 -c 10 -S -p 80 192.168.0.30 HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data bytes len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=7.9 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=2 win=5120 rtt=1003.8 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=5 win=5120 rtt=7.6 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=7.4 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=9 win=5120 rtt=3.0 ms --- 192.168.0.30 hping statistic --- 10 packets transmitted, 5 packets received, 50% packet loss round-trip min/avg/max = 3.0/205.9/1003.8 ms 不幸的是，从 hping3 的输出中，我们可以看到，跟前面现象一样，还是 50% 的丢包；RTT 的波动也仍旧很大，从 3ms 到 1s。显然，问题还是没解决，丢包还在继续发生。不过，既然链路层已经排查完了，我们就继续向上层分析，看看网络层和传输层有没有问题。 3.2 网络层和传输层 在网络层和传输层中，引发丢包的因素非常多。不过，其实想确认是否丢包，是非常简单的事，因为 Linux 已经为我们提供了各个协议的收发汇总情况。我们继续在容器终端中，执行下面的 netstat -s 命令，就可以看到协议的收发汇总，以及错误信息了： root@nginx:/# netstat -s Ip: Forwarding: 1 //开启转发 31 total packets received //总收包数 0 forwarded //转发包数 0 incoming packets discarded //接收丢包数 25 incoming packets delivered //接收的数据包数 15 requests sent out //发出的数据包数 Icmp: 0 ICMP messages received //收到的ICMP包数 0 input ICMP message failed //收到ICMP失败数 ICMP input histogram: 0 ICMP messages sent //ICMP发送数 0 ICMP messages failed //ICMP失败数 ICMP output histogram: Tcp: 0 active connection openings //主动连接数 0 passive connection openings //被动连接数 11 failed connection attempts //失败连接尝试数 0 connection resets received //接收的连接重置数 0 connections established //建立连接数 25 segments received //已接收报文数 21 segments sent out //已发送报文数 4 segments retransmitted //重传报文数 0 bad segments received //错误报文数 0 resets sent //发出的连接重置数 Udp: 0 packets received ... TcpExt: 11 resets received for embryonic SYN_RECV sockets //半连接重置数 0 packet headers predicted TCPTimeouts: 7 //超时数 TCPSynRetrans: 4 //SYN重传数 ... netstat 汇总了 IP、ICMP、TCP、UDP 等各种协议的收发统计信息。不过，我们的目的是排查丢包问题，所以这里主要观察的是错误数、丢包数以及重传数。根据上面的输出，你可以看到，只有 TCP 协议发生了丢包和重传，分别是： 11 次连接失败重试（11 failed connection attempts） 4 次重传（4 segments retransmitted） 11 次半连接重置（11 resets received for embryonic SYN_RECV sockets） 4 次 SYN 重传（TCPSynRetrans） 7 次超时（TCPTimeouts） 这个结果告诉我们，TCP 协议有多次超时和失败重试，并且主要错误是半连接重置。换句话说，主要的失败，都是三次握手失败。不过，虽然在这儿看到了这么多失败，但具体失败的根源还是无法确定。所以，我们还需要继续顺着协议栈来分析。 3.3 iptables 首先我们要知道，除了网络层和传输层的各种协议，iptables 和内核的连接跟踪机制也可能会导致丢包。所以，这也是发生丢包问题时，我们必须要排查的一个因素。 我们先来看看连接跟踪，要确认是不是连接跟踪导致的问题，其实只需要对比当前的连接跟踪数和最大连接跟踪数即可。不过，由于连接跟踪在 Linux 内核中是全局的（不属于网络命名空间），我们需要退出容器终端，回到主机中来查看。你可以在容器终端中，执行 exit ；然后执行下面的命令，查看连接跟踪数： # 容器终端中执行exit root@nginx:/# exit exit # 主机终端中查询内核配置 $ sysctl net.netfilter.nf_conntrack_max net.netfilter.nf_conntrack_max = 262144 $ sysctl net.netfilter.nf_conntrack_count net.netfilter.nf_conntrack_count = 182 从这儿你可以看到，连接跟踪数只有 182，而最大连接跟踪数则是 262144。显然，这里的丢包，不可能是连接跟踪导致的。接着，再来看 iptables。回顾一下 iptables 的原理，它基于 Netfilter 框架，通过一系列的规则，对网络数据包进行过滤（如防火墙）和修改（如 NAT）。 些 iptables 规则，统一管理在一系列的表中，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。而每张表又可以包括一系列的链，用于对 iptables 规则进行分组管理。 对于丢包问题来说，最大的可能就是被 filter 表中的规则给丢弃了。要弄清楚这一点，就需要我们确认，那些目标为 DROP 和 REJECT 等会弃包的规则，有没有被执行到。 你可以把所有的 iptables 规则列出来，根据收发包的特点，跟 iptables 规则进行匹配。不过显然，如果 iptables 规则比较多，这样做的效率就会很低。当然，更简单的方法，就是直接查询 DROP 和 REJECT 等规则的统计信息，看看是否为 0。如果统计值不是 0 ，再把相关的规则拎出来进行分析。 我们可以通过 iptables -nvL 命令，查看各条规则的统计信息。比如，你可以执行下面的 docker exec 命令，进入容器终端；然后再执行下面的 iptables 命令，就可以看到 filter 表的统计数据了： # 在主机中执行 $ docker exec -it nginx bash # 在容器中执行 root@nginx:/# iptables -t filter -nvL Chain INPUT (policy ACCEPT 25 packets, 1000 bytes) pkts bytes target prot opt in out source destination 6 240 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.29999999981 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 15 packets, 660 bytes) pkts bytes target prot opt in out source destination 6 264 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.29999999981 从 iptables 的输出中，你可以看到，两条 DROP 规则的统计数值不是 0，它们分别在 INPUT 和 OUTPUT 链中。这两条规则实际上是一样的，指的是使用 statistic 模块，进行随机 30% 的丢包。再观察一下它们的匹配规则。0.0.0.0/0 表示匹配所有的源 IP 和目的 IP，也就是会对所有包都进行随机 30% 的丢包。看起来，这应该就是导致部分丢包的“罪魁祸首”了。既然找出了原因，接下来的优化就比较简单了。比如，把这两条规则直接删除就可以了。我们可以在容器终端中，执行下面的两条 iptables 命令，删除这两条 DROP 规则： root@nginx:/# iptables -t filter -D INPUT -m statistic --mode random --probability 0.30 -j DROP root@nginx:/# iptables -t filter -D OUTPUT -m statistic --mode random --probability 0.30 -j DROP 删除后，问题是否就被解决了呢？我们可以切换到终端二中，重新执行刚才的 hping3 命令，看看现在是否正常： $ hping3 -c 10 -S -p 80 192.168.0.30 HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data bytes len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=11.9 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=1 win=5120 rtt=7.8 ms ... len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=9 win=5120 rtt=15.0 ms --- 192.168.0.30 hping statistic --- 10 packets transmitted, 10 packets received, 0% packet loss round-trip min/avg/max = 3.3/7.9/15.0 ms 这次输出你可以看到，现在已经没有丢包了，并且延迟的波动变化也很小。看来，丢包问题应该已经解决了。 不过，到目前为止，我们一直使用的 hping3 工具，只能验证案例 Nginx 的 80 端口处于正常监听状态，却还没有访问 Nginx 的 HTTP 服务。所以，不要匆忙下结论结束这次优化，我们还需要进一步确认，Nginx 能不能正常响应 HTTP 请求。 我们继续在终端二中，执行如下的 curl 命令，检查 Nginx 对 HTTP 请求的响应： $ curl --max-time 3 http://192.168.0.30 curl: (28) Operation timed out after 3000 milliseconds with 0 bytes received 从 curl 的输出中，你可以发现，这次连接超时了。可是，刚才我们明明用 hping3 验证了端口正常，现在却发现 HTTP 连接超时，是不是因为 Nginx 突然异常退出了呢？不妨再次运行 hping3 来确认一下： $ hping3 -c 3 -S -p 80 192.168.0.30 HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data bytes len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=7.8 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=1 win=5120 rtt=7.7 ms len=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=2 win=5120 rtt=3.6 ms --- 192.168.0.30 hping statistic --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 3.6/6.4/7.8 ms 奇怪，hping3 的结果显示，Nginx 的 80 端口确确实实还是正常状态。这该如何是好呢？别忘了，我们还有个大杀器——抓包操作。看来有必要抓包看看了。 3.4 tcpdump 接下来，我们切换回终端一，在容器终端中，执行下面的 tcpdump 命令，抓取 80 端口的包： root@nginx:/# tcpdump -i eth0 -nn port 80 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 然后，切换到终端二中，再次执行前面的 curl 命令： $ curl --max-time 3 http://192.168.0.30/ curl: (28) Operation timed out after 3000 milliseconds with 0 bytes received 等到 curl 命令结束后，再次切换回终端一，查看 tcpdump 的输出： 14:40:00.589235 IP 10.255.255.5.39058 > 172.17.0.2.80: Flags [S], seq 332257715, win 29200, options [mss 1418,sackOK,TS val 486800541 ecr 0,nop,wscale 7], length 0 14:40:00.589277 IP 172.17.0.2.80 > 10.255.255.5.39058: Flags [S.], seq 1630206251, ack 332257716, win 4880, options [mss 256,sackOK,TS val 2509376001 ecr 486800541,nop,wscale 7], length 0 14:40:00.589894 IP 10.255.255.5.39058 > 172.17.0.2.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 486800541 ecr 2509376001], length 0 14:40:03.589352 IP 10.255.255.5.39058 > 172.17.0.2.80: Flags [F.], seq 76, ack 1, win 229, options [nop,nop,TS val 486803541 ecr 2509376001], length 0 14:40:03.589417 IP 172.17.0.2.80 > 10.255.255.5.39058: Flags [.], ack 1, win 40, options [nop,nop,TS val 2509379001 ecr 486800541,nop,nop,sack 1 {76:77}], length 0 经过这么一系列的操作，从 tcpdump 的输出中，我们就可以看到： 前三个包是正常的 TCP 三次握手，这没问题； 但第四个包却是在 3 秒以后了，并且还是客户端（VM2）发送过来的 FIN 包，也就说明，客户端的连接关闭了。 我想，根据 curl 设置的 3 秒超时选项，你应该能猜到，这是因为 curl 命令超时后退出了。我把这一过程，用 TCP 交互的流程图（实际上来自 Wireshark 的 Flow Graph）来表示，你可以更清楚地看到上面这个问题： 这里比较奇怪的是，我们并没有抓取到 curl 发来的 HTTP GET 请求。那么，究竟是网卡丢包了，还是客户端压根儿就没发过来呢？我们可以重新执行 netstat -i 命令，确认一下网卡有没有丢包问题： root@nginx:/# netstat -i Kernel Interface table Iface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flg eth0 100 157 0 344 0 94 0 0 0 BMRU lo 65536 0 0 0 0 0 0 0 0 LRU 还是那句话，遇到搞不懂的现象，不妨先去查查工具和方法的原理。我们可以对比一下这两个工具： hping3 实际上只发送了 SYN 包； 而 curl 在发送 SYN 包后，还会发送 HTTP GET 请求。 HTTP GET ，本质上也是一个 TCP 包，但跟 SYN 包相比，它还携带了 HTTP GET 的数据。那么，通过这个对比，你应该想到了，这可能是 MTU 配置错误导致的。为什么呢？ 其实，仔细观察上面 netstat 的输出界面，第二列正是每个网卡的 MTU 值。eth0 的 MTU 只有 100，而以太网的 MTU 默认值是 1500，这个 100 就显得太小了。 当然，MTU 问题是很好解决的，把它改成 1500 就可以了。我们继续在容器终端中，执行下面的命令，把容器 eth0 的 MTU 改成 1500： root@nginx:/# ifconfig eth0 mtu 1500 修改完成后，再切换到终端二中，再次执行 curl 命令，确认问题是否真的解决了： $ curl --max-time 3 http://192.168.0.30/ ... Thank you for using nginx. 非常不容易呀，这次终于看到了熟悉的 Nginx 响应，说明丢包的问题终于彻底解决了。 来自： 服务器总是时不时丢包，我该怎么办？（下） Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Network/docker_config_DNS.html":{"url":"Docker/Network/docker_config_DNS.html","title":"docker 配置 dns","keywords":"","body":"Docker 配置 DNS1. docker配置DNS方法2. 默认DNS配置3. 启动时配置dns参数4. daemon.json配置DNS格式Docker 配置 DNS 1. docker配置DNS方法 docker容器配置dns解析地址，我知道的有以下几种办法（优先级从高到低）： 启动的时候加--dns=IP_ADDRESS； 守护进程启动参数中添加DOCKER_OPTS=\"--dns 8.8.8.8\" ； 在/etc/docker/deamon.json中添加dns信息（与守护进程参数会冲突不能同时添加。）； 使用宿主机的/etc/resolv.conf文件； 2. 默认DNS配置 怎样为Docker提供的每一个容器进行主机名和DNS配置，而不必建立自定义镜像并将主机名写 到里面？它的诀窍是覆盖三个至关重要的在/etc下的容器内的虚拟文件，那几个文件可以写入 新的信息。你可以在容器内部运行mount看到这个： $ mount ... /dev/disk/by-uuid/1fec...ebdf on /etc/hostname type ext4 ... /dev/disk/by-uuid/1fec...ebdf on /etc/hosts type ext4 ... /dev/disk/by-uuid/1fec...ebdf on /etc/resolv.conf type ext4 ... ... 3. 启动时配置dns参数 Options Description -h HOSTNAME or --hostname=HOSTNAME 在该容器启动时，将HOSTNAME设置到容器内的/etc/hosts, /etc/hostname, /bin/bash提示中。 --link=CONTAINER_NAME or ID:ALIAS 在该容器启动时，将ALIAS和CONTAINER_NAME/ID对应的容器IP添加到/etc/hosts. 如果 CONTAINER_NAME/ID有多个IP地址 ？ --dns=IP_ADDRESS... 在该容器启动时，将nameserver IP_ADDRESS添加到容器内的/etc/resolv.conf中。可以配置多个。 --dns-search=DOMAIN... 在该容器启动时，将DOMAIN添加到容器内/etc/resolv.conf的dns search列表中。可以配置多个。 --dns-opt=OPTION... 在该容器启动时，将OPTION添加到容器内/etc/resolv.conf中的options选项中，可以配置多个 如果docker run时不含--dns=IP_ADDRESS..., --dns-search=DOMAIN..., or --dns-opt=OPTION...参数，docker daemon会将copy本主机的/etc/resolv.conf，然后对该copy进行处理（将那些/etc/resolv.conf中ping不通的nameserver项给抛弃）,处理完成后留下的部分就作为该容器内部的/etc/resolv.conf。因此，如果你想利用宿主机中的/etc/resolv.conf配置的nameserver进行域名解析，那么你需要宿主机中该dns service配置一个宿主机内容器能ping通的IP。 如果宿主机的/etc/resolv.conf内容发生改变，docker daemon有一个对应的file change notifier会watch到这一变化，然后根据容器状态采取对应的措施： 如果容器状态为stopped，则立刻根据宿主机的/etc/resolv.conf内容更新容器内的/etc/resolv.conf. 如果容器状态为running，则容器内的/etc/resolv.conf将不会改变，直到该容器状态变为stopped. 如果容器启动后修改过容器内的/etc/resolv.conf，则不会对该容器进行处理，否则可能会丢失已经完成的修改，无论该容器为什么状态。 如果容器启动时，用了--dns, --dns-search, or --dns-opt选项，其启动时已经修改了宿主机的/etc/resolv.conf过滤后的内容，因此docker daemon永远不会更新这种容器的/etc/resolv.conf。 注意: docker daemon监控宿主机/etc/resolv.conf的这个file change notifier的实现是依赖linux内核的inotify特性，而inotfy特性不兼容overlay fs，因此使用overlay fs driver的docker deamon将无法使用该/etc/resolv.conf自动更新的功能。、 $ sudo docker run --hostname 'myhost' -it centos [root@myhost /]# cat /etc/hosts 172.17.0.7 myhost $ sudo docker run -it --dns=192.168.5.1 centos [root@6a38049c9052 /]# cat /etc/resolv.conf nameserver 192.168.5.1 $ sudo docker run -it --dns-search=www.domain.com centos [root@ae0e9e99596f /]# cat /etc/resolv.conf nameserver 192.168.4.1 search www.mydomain.com 4. daemon.json配置DNS格式 root@node-7:~# cat /etc/docker/daemon.json { \"data-root\": \"/data/docker\", \"dns\": [\"172.18.0.52\", \"172.18.0.70\", \"183.XX.XX.XX\"], \"dns-search\": [\"fiibeacon.local\"], \"hosts\": [\"unix:///var/run/docker.sock\", \"tcp://172.18.0.141:2375\"], \"storage-driver\": \"overlay2\" } 参考： docker高级网络配置 docker container DNS配置介绍和源码分析 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Registry/":{"url":"Docker/Registry/","title":"Registry","keywords":"","body":"Docker Registry Overview1. 什么是 Docker registry ？2. Docker Hub3. Docker Hub 常见操作4. 私有 docker 仓库5. 创建本地仓库Docker Registry Overview 1. 什么是 Docker registry ？ Docker registry 是命名 Docker镜像的 存储和分发系统。同一个image可能有多个不同的版本，由它们的标签标识。 Docker registry 被组织成 Docker 存储库 ，其中存储库包含特定镜像的所有版本。仓库允许 Docker 用户在本地拉取镜像，以及将新镜像推送到仓库（在适用时给予足够的访问权限）。 默认情况下，Docker 引擎与 Docker Hub 交互，Docker 的公共仓库实例。但是，可以在本地运行开源 Docker registry /分发版，以及称为 Docker Trusted Registry 的商业支持版本。网上还有其他公共登记处。 要从本地仓库中提取镜像，您可以运行类似于以下内容的命令： docker pull my-registry:9000/foo/bar:2.1 docker pull foo/bar #默认latest 2. Docker Hub DockerHub 是 Docker Inc. 的托管仓库解决方案。除了公共和私有存储库之外，它还提供自动构建、组织帐户以及与 Github 和 Bitbucket 等源控制解决方案的集成。 任何运行 Docker 的人都可以访问公共存储库，并且镜像名称包括组织/用户名。例如， 将从Jenkins 组织中提取docker pull jenkins/jenkins 带有标签的 Jenkins CI 服务器镜像 。latest 有成千上万的公共image可用。私有存储库限制对存储库创建者或其组织成员的访问。 DockerHub 支持官方存储库，其中包括经过安全性和最佳实践验证的image。这些不需要组织/用户名，例如 docker pull nginx 将提取 latest Nginx 负载均衡器的image。 如果 DockerHub 存储库链接到包含构建上下文（Dockerfile 和同一文件夹中的所有任何文件）的源代码控制存储库，则 DockerHub 可以执行自动镜像构建。源存储库中的提交将触发 DockerHub 中的构建。 如需进一步阅读，请参阅 Docker 文档： 配置自动化 Docker 构建 › DockerHub 还可以自动扫描私有存储库中的镜像以查找漏洞，生成一份报告，详细说明在每个镜像层中发现的漏洞，按严重性（严重、主要或次要）。 如需进一步阅读，请参阅 Docker 文档： Docker 安全扫描 › 3. Docker Hub 常见操作 使用 DockerHub 的常见操作包括： 登录 DockerHub：运行 docker login 将询问您的 DockerHub ID 和密码。 在公共存储库中搜索image：使用 docker search 带有搜索词的命令在公共（包括官方）存储库中查找与该词匹配的所有image。 拉取现有image：使用 docker pull 并指定image名称。默认情况下，检索最新版本，但可以通过指定不同的image标签/版本来覆盖此行为。例如，要拉取 Ubuntu 镜像的（旧）版本 14.04： docker pull ubuntu:14.04 推送本地镜像：您可以通过运行 docker push 命令推送镜像。例如，要将（最新）本地版本的 my-image 推送到我的仓库： docker push my-username/my-image 创建一个新组织：这必须通过浏览器完成。转到 Docker Hub ，单击 组织 ，然后单击 创建组织 并填写所需的数据。 创建一个新的存储库：这必须从浏览器中完成。转到 Docker Hub ，单击 Create 下拉菜单并选择 Create Repository。填写所需数据。您现在可以开始将image推送到此存储库。 创建自动构建：这必须从浏览器中完成。首先，通过导航到您的个人资料设置， 将您的 Github 或 Bitbucket 帐户链接到 Docker Hub ，然后单击Linked Accounts & Services 。选择 公共和私人访问（强制）并授权。然后，单击“ 创建” 下拉菜单，选择 “创建自动构建 ”并选择要从中构建image的源存储库。 4. 私有 docker 仓库 在本地（组织内部）运行私有仓库的用例包括： 在隔离网络内分发image （不通过 Internet 发送image）； 创建更快的 CI/CD 管道 （从内部网络拉取和推送image），包括更快地部署到本地环境 ； 在大型机器集群上部署新镜像； 严格控制image的存储位置 运行私有镜像系统，尤其是当交付到生产依赖于它时，需要操作技能，例如确保可用性、日志记录和日志处理、监控和安全性。对 http 和整体网络通信的深入了解也很重要。 一些供应商提供自己的开源 Docker registry 扩展。这些可以帮助缓解上述一些运营问题： Docker Trusted Registry 是 Docker Inc 的商业支持版本，通过复制、镜像审计、签名和安全扫描、与 LDAP 和 Active Directory 的集成提供高可用性。 Harbor 是一个 VMWare 开源产品，它还通过复制、镜像审计、与 LDAP 和 Active Directory 的集成来提供高可用性。 GitLab Container Registry 与 GitLab CI 的工作流程紧密集成，只需最少的设置。 JFrog Artifactory 用于强大的工件管理（不仅是 Docker 镜像，还包括任何工件）。 5. 创建本地仓库 管理本地仓库安装所需的常见操作包括： 启动仓库：仓库本身是一个 Docker 镜像，需要使用 docker run . 例如，根据默认配置运行它，并将主机端口 5001 上的请求转发到容器端口 5000（注册中心将侦听的默认端口）： docker run -d -p 5001:5000 --name registry registry:2 默认情况下，仓库数据保存为 Docker 卷。要指定主机上的特定存储位置（例如，SSD 或 SAN 文件系统），请使用绑定挂载选项： -v : Automatically restart registry：要在主机重新启动或仅仅因为仓库容器停止时保持仓库运行，只需将选项添加 --restart=always 到命令中。 停止仓库：停止仓库只是使用 docker stop registry 命令停止正在运行的仓库容器的问题。要实际删除容器，还需要运行： docker rm -v registry 请注意，开源 Docker registry 带有一组默认配置，用于日志记录、存储、身份验证、中间件、报告、http、通知、健康检查等。这些可以通过将特定环境变量传递给仓库启动命令来单独覆盖。例如，以下命令告诉仓库在容器的端口 5001 而不是默认端口 5000 上侦听请求。 docker run -d -e REGISTRY_HTTP_ADDR=0.0.0.0:5001 -p 5001:5001 \\ --name registry registry:2 另一种选择是使用YAML 文件 完全覆盖配置设置 。它还需要作为卷安装在容器上，例如： docker run -d -p 5001:5000 -v config.yml:/etc/docker/registry/config.yml \\ --name registry registry:2 如需进一步阅读，请参阅 Docker 文档： 部署仓库服务器 ›和 配置仓库 › todo: [ ] How To Set Up a Private Docker Registry on Ubuntu 18.04 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Registry/harbor_get_started_guide.html":{"url":"Docker/Registry/harbor_get_started_guide.html","title":"harbor部署入门指南","keywords":"","body":"harbor部署入门指南1. 什么是harbor2. Harbor的架构准备条件Harbor 安装条件docker安装docker-compose安装3. Harbor http ip部署3.1 harbor安装3.2 测试结果4. Harbor http 域名部署4.1 清理杂质4.2 修改配置4.3 harbor安装4.3 测试结果5. Harbor https ip访问部署5.1 清理杂质5.2 修改配置5.3 部署或重新配置 Harbor5.4 测试结果6 Harbor https 域名访问部署6.1 清理杂质6.2 修改配置6.3 配置证书6.4 部署或重新配置 Harbor6.5 测试harbor部署入门指南 tagsstart registry tagsstop docker registry仓库搭建并配置证书 docker部署带有界面的registry仓库 harbor【1】初级部署入门指南 docker圣经 云原生圣经 1. 什么是harbor Docker有个形象的比喻叫集装箱，kubernetes是舵手，而Harbor是港湾，其实是用来保存容器镜像的仓库，企业使用docker、kubernetes时，一般都需要个私有镜像仓库的，Harbor就是其中的佼佼者。官方的解释：harbor通过策略和基于角色的访问控制来保护工件，确保图像被扫描且没有漏洞，并将图像签名为受信任的。Harbor 是 CNCF 毕业的项目，可提供合规性、性能和互操作性，帮助您跨云原生计算平台（如 Kubernetes 和 Docker）一致且安全地管理工件。 特点： 安全 安全和漏洞分析 内容签名和验证 管理 多租户 可扩展的 API 和 Web UI 跨多个注册中心复制，包括 Harbor 身份集成和基于角色的访问控制 2. Harbor的架构 Proxy: Harbor的registry、UI、token services等组件，都处在一个反向代理后边。该代理将来自浏览器、docker clients的请求转发到后端服务上。 Registry: 负责存储Docker镜像，以及处理Docker push/pull请求。因为Harbor强制要求对镜像的访问做权限控制， 在每一次push/pull请求时，Registry会强制要求客户端从token service那里获得一个有效的token。 Core services: Harbor的核心功能，主要包括如下3个服务: UI: 作为Registry Webhook, 以图像用户界面的方式辅助用户管理镜像。 1 WebHook是在registry中配置的一种机制， 当registry中镜像发生改变时，就可以通知到Harbor的webhook endpoint。Harbor使用webhook来更新日志、初始化同步job等。 2 Token service会根据该用户在一个工程中的角色，为每一次的push/pull请求分配对应的token。假如相应的请求并没有包含token的话，registry会将该请求重定向到token service。 3 Database 用于存放工程元数据、用户数据、角色数据、同步策略以及镜像元数据。 Job services: 主要用于镜像复制，本地镜像可以被同步到远程Harbor实例上。 Log: 负责收集其他模块的日志到一个地方。 准备条件 Harbor 安装条件 harbor 安装条件 docker安装 docker安装 docker-compose安装 docker-compose安装 官方harbor 安装 3. Harbor http ip部署 3.1 harbor安装 $ tar xzvf harbor-online-installer-v2.3.4.tgz harbor/prepare harbor/LICENSE harbor/install.sh harbor/common.sh harbor/harbor.yml.tmpl $ ls harbor harbor-online-installer-v2.3.4.tgz $ cd harbor/ harbor/$ ls common.sh harbor.yml.tmpl install.sh LICENSE prepare 如果我们 尝试重新安装的话，一定要保持harbor目录最初解压的样子，当然如果我们重新安装不修改数据目录，应该也要将其删除：rm -r /data/* $ cp harbor.yml.tmpl harbor.yml $ vim harbor.yml $ cat harbor.yml|grep -v '#' |grep -v '^$' hostname: 192.168.211.70 http: port: 80 harbor_admin_password: 123456 database: password: root123 max_idle_conns: 100 max_open_conns: 900 data_volume: /data trivy: ignore_unfixed: false skip_update: false insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.3.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy 我修改如下： hostname为本机的ip地址：192.168.211.70 注释掉了https部分的相关配置 也许你在配置文件harbor.yaml有改密码的冲动，但在注释中的解释说我们应该在登陆界面ui去修改它，因此，如果配置文件修改了admin的登陆密码为：123456，也许会报错，不管怎么说，我遇到了，也许是我的浏览器缓存问题。或者部署机器的杂质问题。 $ ./prepare prepare base dir is set to /root/harbor/harbor1/harbor WARNING:root:WARNING: HTTP protocol is insecure. Harbor will deprecate http protocol in the future. Please make sure to upgrade to https Generated configuration file: /config/portal/nginx.conf Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/log/rsyslog_docker.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/registryctl/config.yml Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml Generated and saved secret to file: /data/secret/keys/secretkey Successfully called func: create_root_cert Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir #会在/data目录生成一些配置和数据。 $ ls /data/ ca_download database job_logs redis registry secret 由于我们部署的是非安全的harbor，我们不要忘了对docker的配置做一些修改。添加insecure-registries参数。 $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ], \"insecure-registries\": [ \"192.168.211.70\" ] } 然后部署安装，如果第一次可能因为拉取镜像有点慢，但往后部署也就几秒钟的时间。 $ ./install.sh [Step 0]: checking if docker is installed ... Note: docker version: 20.10.10 [Step 1]: checking docker-compose is installed ... Note: docker-compose version: 2.1.1 [Step 2]: preparing environment ... [Step 3]: preparing harbor configs ... prepare base dir is set to /root/harbor/harbor1/harbor WARNING:root:WARNING: HTTP protocol is insecure. Harbor will deprecate http protocol in the future. Please make sure to upgrade to https Clearing the configuration file: /config/nginx/nginx.conf Clearing the configuration file: /config/core/app.conf Clearing the configuration file: /config/core/env Clearing the configuration file: /config/portal/nginx.conf Clearing the configuration file: /config/registryctl/config.yml Clearing the configuration file: /config/registryctl/env Clearing the configuration file: /config/registry/passwd Clearing the configuration file: /config/registry/config.yml Clearing the configuration file: /config/log/rsyslog_docker.conf Clearing the configuration file: /config/log/logrotate.conf Clearing the configuration file: /config/jobservice/config.yml Clearing the configuration file: /config/jobservice/env Clearing the configuration file: /config/db/env Generated configuration file: /config/portal/nginx.conf Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/log/rsyslog_docker.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/registryctl/config.yml Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml loaded secret from file: /data/secret/keys/secretkey Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir [Step 4]: starting Harbor ... [+] Running 10/10 ⠿ Network harbor_harbor Created 0.1s ⠿ Container harbor-log Started 2.2s ⠿ Container registry Started 5.5s ⠿ Container harbor-portal Started 6.1s ⠿ Container registryctl Started 8.2s ⠿ Container redis Started 8.7s ⠿ Container harbor-db Started 6.5s ⠿ Container harbor-core Started 10.1s ⠿ Container nginx Started 12.4s ⠿ Container harbor-jobservice Started 13.4s ✔ ----Harbor has been installed and started successfully.---- $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES aace64158bb9 goharbor/nginx-photon:v2.3.4 \"nginx -g 'daemon of…\" 20 minutes ago Up 20 minutes (healthy) 0.0.0.0:80->8080/tcp, 0.0.0.0:443->8443/tcp nginx 2fb44007a910 goharbor/harbor-jobservice:v2.3.4 \"/harbor/entrypoint.…\" 21 minutes ago Up 20 minutes (healthy) harbor-jobservice 07e9c7fc4789 goharbor/harbor-core:v2.3.4 \"/harbor/entrypoint.…\" 21 minutes ago Up 20 minutes (healthy) harbor-core 6a530d9902f0 goharbor/redis-photon:v2.3.4 \"redis-server /etc/r…\" 21 minutes ago Up 20 minutes (healthy) redis 857e8929f318 goharbor/registry-photon:v2.3.4 \"/home/harbor/entryp…\" 21 minutes ago Up 20 minutes (healthy) registry a6f1e3951798 goharbor/harbor-registryctl:v2.3.4 \"/home/harbor/start.…\" 21 minutes ago Up 20 minutes (healthy) registryctl 044a0dbe8f0f goharbor/harbor-db:v2.3.4 \"/docker-entrypoint.…\" 21 minutes ago Up 20 minutes (healthy) harbor-db 3a111e636acd goharbor/harbor-portal:v2.3.4 \"nginx -g 'daemon of…\" 21 minutes ago Up 20 minutes (healthy) harbor-portal da038195ace4 goharbor/harbor-log:v2.3.4 \"/bin/sh -c /usr/loc…\" 21 minutes ago Up 20 minutes (healthy) 127.0.0.1:1514->10514/tcp harbor-log $ docker-compose ps NAME COMMAND SERVICE STATUS PORTS harbor-core \"/harbor/entrypoint.…\" core running (healthy) harbor-db \"/docker-entrypoint.…\" postgresql running (healthy) harbor-jobservice \"/harbor/entrypoint.…\" jobservice running (healthy) harbor-log \"/bin/sh -c /usr/loc…\" log running (healthy) 127.0.0.1:1514->10514/tcp harbor-portal \"nginx -g 'daemon of…\" portal running (healthy) nginx \"nginx -g 'daemon of…\" proxy running (healthy) 0.0.0.0:80->8080/tcp, 0.0.0.0:443->8443/tcp redis \"redis-server /etc/r…\" redis running (healthy) registry \"/home/harbor/entryp…\" registry running (healthy) registryctl \"/home/harbor/start.…\" registryctl running (healthy) Harbor安装结束，我们验证一下。 3.2 测试结果 3.2.1 仓库登陆 $ docker login 192.168.211.70 Authenticating with existing credentials... Stored credentials invalid or expired Username (admin): admin Password: 隐藏输入（Harbor12345） WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 4.2.2 界面登陆 界面登陆用户密码：admin/Harbor12345 4.2.3 修改密码 现在我们把admin的初始密码Harbor12345改为123456，试一试。 发现原来admin密码的设置要支持大小写字符并且有数字。 那么我们把admin的密码改为Ghost12345 修改成功，并且退出重新登陆成功了（图略）。 当然，我们的仓库登陆密码也会随之变化。 $ docker login 192.168.211.70 Authenticating with existing credentials... Stored credentials invalid or expired Username (admin): Password: 隐藏输入（Ghost12345） WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 4.2.4 推送镜像 我们要把镜像推送到仓库，那么镜像名要遵守恰当的格式 docker push 仓库名/项目名/镜像名:标签 我们的仓库名是192.168.211.70,项目名当前默认是library，当然我们可以根据自己的需求在界面创建一个新的项目名。例如base 然后给一个镜像打一个标签。推送到仓库。 $ docker tag busybox:latest 192.168.211.70/base/busybox:latest $ docker push 192.168.211.70/base/busybox:latest The push refers to repository [192.168.211.70/base/busybox] cfd97936a580: Pushed latest: digest: sha256:febcf61cd6e1ac9628f6ac14fa40836d16f3c6ddef3b303ff0321606e55ddd0b size: 527 界面我们也能看到推送进来的镜像。 4.2.5 拉取镜像 我们换到另一台机器尝试一下拉取这个镜像，要怎么做呢？修改/etc/docker/daemon.json 添加insecure-registries参数是最为关键的一步。 $ vim /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ], \"insecure-registries\": [ \"192.168.211.70\" ] } $ systemctl daemon-reload && systemctl restart docker #拉取成功 $ docker pull 192.168.211.70/base/busybox:latest latest: Pulling from base/busybox 24fb2886d6f6: Pull complete Digest: sha256:febcf61cd6e1ac9628f6ac14fa40836d16f3c6ddef3b303ff0321606e55ddd0b Status: Downloaded newer image for 192.168.211.70/base/busybox:latest 192.168.211.70/base/busybox:latest 配置 Harbor YML 文件 4. Harbor http 域名部署 4.1 清理杂质 $ docker-compose down [+] Running 10/10 ⠿ Container harbor-jobservice Removed 10.5s ⠿ Container registryctl Removed 10.5s ⠿ Container nginx Removed 0.5s ⠿ Container harbor-portal Removed 0.4s ⠿ Container harbor-core Removed 10.5s ⠿ Container redis Removed 1.0s ⠿ Container harbor-db Removed 0.9s ⠿ Container registry Removed 11.1s ⠿ Container harbor-log Removed 10.7s ⠿ Network harbor_harbor Removed $ rm -rf /data/* $ rm -rf common 浏览器界面清理缓存 4.2 修改配置 a. harbor.yaml配置文件只修改了hostname参数 hostname: ghost.harbor.com b. $ echo \"192.168.211.70 ghost.harbor.com\" >> /etc/hosts $ nslookup ghost.harbor.com Server: 127.0.0.53 Address: 127.0.0.53#53 Non-authoritative answer: Name: ghost.harbor.com Address: 192.168.211.70 c. docker配置文件/etc/docker/daemon.json 参数insecure-registries 由192.168.211.70修改为ghost.harbor.com $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ], \"insecure-registries\": [ \"ghost.harbor.com\" ] } 并重启docker systemctl daemon-reload && systemctl restart docker 4.3 harbor安装 $ ./prepare $ ./install.sh $ docker ps $ docker-compose ps 4.3 测试结果 4.3.1 仓库登陆 $ docker login ghost.harbor.com Username: admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 4.3.2 界面登陆 本机windows配置 C:\\Windows\\System32\\drivers\\etc\\hosts 192.168.211.70 ghost.harbor.com 访问：http://ghost.harbor.com 用户名/密码：admin/Harbor12345 登陆成功 4.3.3 镜像推送 $ docker tag busybox:latest ghost.harbor.com/library/busybox:latest $ docker push ghost.harbor.com/library/busybox:latest The push refers to repository [ghost.harbor.com/library/busybox] cfd97936a580: Pushed latest: digest: sha256:febcf61cd6e1ac9628f6ac14fa40836d16f3c6ddef3b303ff0321606e55ddd0b size: 527 Harbor http域名访问的部署结束。 5. Harbor https ip访问部署 默认情况下，Harbor 不附带证书。可以在没有安全性的情况下部署 Harbor，以便您可以通过 HTTP 连接到它。但是，仅在没有连接到外部 Internet 的气隙测试或开发环境中才可接受使用 HTTP。在公有环境中使用 HTTP 会使您面临中间人攻击。在生产环境中，始终使用 HTTPS。如果您启用 Content Trust with Notary 以正确签署所有图像，则必须使用 HTTPS。 要配置 HTTPS，您必须创建 SSL 证书。您可以使用受信任的第三方 CA 签署的证书，也可以使用自签名证书。本节介绍如何使用 OpenSSL创建 CA，以及如何使用您的 CA 签署服务器证书和客户端证书。您可以使用其他 CA 提供商，例如 Let's Encrypt。 以下过程假设您的 Harbor 注册表的主机名是192.168.211.70，并且其 DNS 记录指向您运行 Harbor 的主机。 5.1 清理杂质 $ docker-compose down [+] Running 10/10 ⠿ Container harbor-jobservice Removed 10.5s ⠿ Container registryctl Removed 10.5s ⠿ Container nginx Removed 0.5s ⠿ Container harbor-portal Removed 0.4s ⠿ Container harbor-core Removed 10.5s ⠿ Container redis Removed 1.0s ⠿ Container harbor-db Removed 0.9s ⠿ Container registry Removed 11.1s ⠿ Container harbor-log Removed 10.7s ⠿ Network harbor_harbor Removed $ rm -rf /data/* $ rm -rf common 5.2 修改配置 5.2.1 harbor.yaml harbor.yaml配置文件修改hostname参数并配置了https相关参数 $ cat harbor.yml|grep -v '#' | grep -v '^$' hostname: 192.168.211.70 http: port: 80 https: port: 443 certificate: /data/cert/192.168.211.70.crt private_key: /data/cert/192.168.211.70.key harbor_admin_password: Harbor12345 database: password: root123 max_idle_conns: 100 max_open_conns: 900 data_volume: /data trivy: ignore_unfixed: false skip_update: false insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.3.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy 5.2.2 /etc/docker/daemon.json docker配置文件/etc/docker/daemon.json 参数insecure-registries 要把它去掉。 $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } 并重启docker systemctl daemon-reload && systemctl restart docker 5.2.3 生成证书颁发机构证书 在生产环境中，您应该从 CA 获取证书。在测试或开发环境中，您可以生成自己的 CA。要生成 CA 证书，请运行以下命令。 生成 CA 证书私钥 $ openssl genrsa -out ca.key 4096 Generating RSA private key, 4096 bit long modulus (2 primes) ..............++++ .................................++++ e is 65537 (0x010001) 生成 CA 证书 调整-subj选项中的值以反映您的组织。如果使用 FQDN 连接 Harbor 主机，则必须将其指定为公用名 ( CN) 属性。 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=192.168.211.70\" \\ -key ca.key \\ -out ca.crt 5.2.4 生成服务器证书 证书通常包含一个.crt文件和一个.key文件，例如，192.168.211.70.crt和192.168.211.70.key. 生成私钥 openssl genrsa -out 192.168.211.70.key 4096 生成证书签名请求 (CSR) 调整-subj选项中的值以反映您的组织。如果使用 FQDN 连接 Harbor 主机，则必须将其指定为公用名 ( CN) 属性并在密钥和 CSR 文件名中使用它。 openssl req -sha512 -new \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=192.168.211.70\" \\ -key 192.168.211.70.key \\ -out 192.168.211.70.csr 生成 x509 v3 扩展文件 无论您是使用 FQDN 还是 IP 地址连接到您的 Harbor 主机，您都必须创建此文件，以便为您的 Harbor 主机生成符合主题备用名称 (SAN) 和 x509 v3 的证书扩展要求。替换DNS条目以反映您的域。 使用该extfile.cnf文件为您的 Harbor 主机生成证书 将192.168.211.70CRS 和 CRT 文件名中的 替换为 Harbor 主机名 echo subjectAltName = IP:192.168.211.70 > extfile.cnf openssl x509 -req -sha512 -days 3650 \\ -extfile extfile.cnf \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in 192.168.211.70.csr \\ -out 192.168.211.70.crt 5.2.5 向 Harbor 和 Docker 提供证书 生成后ca.crt，192.168.211.70.crt和192.168.211.70.key文件，必须将它们提供给Harbor and Docker，和重新配置harbor使用它们。 a. 将服务器证书和密钥复制到 Harbor 主机上的 certficates 文件夹中 mkdir /data/cert cp 192.168.211.70.crt /data/cert/ cp 192.168.211.70.key /data/cert/ b. 转换192.168.211.70.crt为192.168.211.70.cert，供 Docker 使用 Docker 守护进程将.crt文件解释为 CA 证书，将.cert文件解释为客户端证书。 openssl x509 -inform PEM -in 192.168.211.70.crt -out 192.168.211.70.cert c. 将服务器证书、密钥和 CA 文件复制到 Harbor 主机上的 Docker 证书文件夹中。您必须首先创建适当的文件夹。 mkdir -p /etc/docker/certs.d/192.168.211.70/ cp 192.168.211.70.cert /etc/docker/certs.d/192.168.211.70/ cp 192.168.211.70.key /etc/docker/certs.d/192.168.211.70/ cp ca.crt /etc/docker/certs.d/192.168.211.70/ 如果您将默认nginx端口 443映射到其他端口，请创建文件夹/etc/docker/certs.d/192.168.211.70:port或/etc/docker/certs.d/harbor_IP:port。 d. 重启docker systemctl daemon-reload && systemctl restart docker 5.2.6 操作系统级别信任证书 e. 当 Docker 守护程序在某些操作系统上运行时，您可能需要在操作系统级别信任证书。 例如，运行以下命令 ubuntu $ cp 192.168.211.70.crt /usr/local/share/ca-certificates/192.168.211.70.crt $ update-ca-certificates Updating certificates in /etc/ssl/certs... 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d... done. Red Hat (CentOS etc): $ cp yourdomain.com.crt /etc/pki/ca-trust/source/anchors/yourdomain.com.crt $ update-ca-trust 5.3 部署或重新配置 Harbor 如果您尚未部署 Harbor，请参阅 配置 Harbor YML 文件以获取有关如何通过在 中指定hostname和https属性来配置 Harbor 以使用证书的信息harbor.yml。 如果您已经使用 HTTP 部署了 Harbor 并希望将其重新配置为使用 HTTPS，请执行以下步骤。 a. 运行prepare脚本以启用 HTTPS ./prepare b. 如果 Harbor 正在运行，请停止并删除现有实例 您的图像数据保留在文件系统中，因此不会丢失任何数据。 docker-compose down -v c. 重启 docker-compose up -d 5.4 测试结果 5.4.1 仓库登陆 $ docker login 192.168.211.70 Authenticating with existing credentials... Stored credentials invalid or expired Username (admin): admin Password: WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 5.4.2 界面登陆 https://192.168.211.70 用户密码：admin/Harbor12345 5.4.3 推送镜像 $ docker push 192.168.211.70/library/alpine:v1.0 The push refers to repository [192.168.211.70/library/alpine] e2eb06d8af82: Pushed v1.0: digest: sha256:69704ef328d05a9f806b6b8502915e6a0a4faa4d72018dc42343f511490daf8a size: 528 5.4.4 拉取镜像 我们换到另一台机器尝试一下拉取这个镜像，需要什么配置呢，不需要什么，只需连通即可。 $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } $ docker pull 192.168.211.70/library/alpine:v1.0 v1.0: Pulling from library/alpine a0d0a0d46f8b: Pull complete Digest: sha256:69704ef328d05a9f806b6b8502915e6a0a4faa4d72018dc42343f511490daf8a Status: Downloaded newer image for 192.168.211.70/library/alpine:v1.0 192.168.211.70/library/alpine:v1.0 harbor https ip访问部署成功结束 6 Harbor https 域名访问部署 6.1 清理杂质 $ docker-compose down [+] Running 10/10 ⠿ Container harbor-jobservice Removed 10.5s ⠿ Container registryctl Removed 10.5s ⠿ Container nginx Removed 0.5s ⠿ Container harbor-portal Removed 0.4s ⠿ Container harbor-core Removed 10.5s ⠿ Container redis Removed 1.0s ⠿ Container harbor-db Removed 0.9s ⠿ Container registry Removed 11.1s ⠿ Container harbor-log Removed 10.7s ⠿ Network harbor_harbor Removed $ rm -rf /data/* $ rm -rf common $ rm -rf /etc/docker/certs.d/* 6.2 修改配置 6.2.1 harbor.yaml harbor.yaml配置文件修改hostname参数并重新配置了https相关参数 $ cat harbor.yml|grep -v '#' | grep -v '^$' hostname: ghost.harbor.com http: port: 80 https: port: 443 certificate: /data/cert/ghost.harbor.com.crt private_key: /data/cert/ghost.harbor.com.key harbor_admin_password: Harbor12345 database: password: root123 max_idle_conns: 100 max_open_conns: 900 data_volume: /data trivy: ignore_unfixed: false skip_update: false insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.3.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy 6.2.2 /etc/docker/daemon.json 如下： $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } 并重启docker systemctl daemon-reload && systemctl restart docker 6.3 配置证书 6.3.1 生成证书颁发机构证书 #生成 CA 证书私钥 $ openssl genrsa -out ca.key 4096 #生成 CA 证书 openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=ghost.harbor.com\" \\ -key ca.key \\ -out ca.crt 6.3.2 生成服务器证书 #生成私钥 openssl genrsa -out ghost.harbor.com.key 4096 #生成证书签名请求 (CSR) openssl req -sha512 -new \\ -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=ghost.harbor.com\" \\ -key ghost.harbor.com.key \\ -out ghost.harbor.com.csr #生成 x509 v3 扩展文件** cat > v3.ext 6.3.4 向 Harbor 和 Docker 提供证书 #a. 将服务器证书和密钥复制到 Harbor 主机上的 certficates 文件夹中** mkdir /data/cert cp ghost.harbor.com.crt /data/cert/ cp ghost.harbor.com.key /data/cert/ #b. 转换ghost.harbor.com.crt为ghost.harbor.com.cert，供 Docker 使用** openssl x509 -inform PEM -in ghost.harbor.com.crt -out ghost.harbor.com.cert #**c. 将服务器证书、密钥和 CA 文件复制到 Harbor 主机上的 Docker 证书文件夹中。您必须首先创建适当的文件夹。** mkdir -p /etc/docker/certs.d/ghost.harbor.com/ cp ghost.harbor.com.cert /etc/docker/certs.d/ghost.harbor.com/ cp ghost.harbor.com.key /etc/docker/certs.d/ghost.harbor.com/ cp ca.crt /etc/docker/certs.d/ghost.harbor.com/ 如果您将默认nginx端口443映射到其他端口，请创建文件夹/etc/docker/certs.d/ghost.harbor.com:port或/etc/docker/certs.d/harbor_IP:port。 重启docker systemctl daemon-reload && systemctl restart docker 6.3.5 操作系统级别信任证书 e. 当 Docker 守护程序在某些操作系统上运行时，您可能需要在操作系统级别信任证书。 ubuntu $ cp ghost.harbor.com.crt /usr/local/share/ca-certificates/ghost.harbor.com.crt $ update-ca-certificates Updating certificates in /etc/ssl/certs... 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d... done. Red Hat (CentOS etc): $ cp ghost.harbor.com.crt /etc/pki/ca-trust/source/anchors/ghost.harbor.com.crt $ update-ca-trust 6.4 部署或重新配置 Harbor 如果您尚未部署 Harbor，请参阅 配置 Harbor YML 文件以获取有关如何通过在 中指定hostname和https属性来配置 Harbor 以使用证书的信息harbor.yml。 如果您已经使用 HTTP 部署了 Harbor 并希望将其重新配置为使用 HTTPS，请执行以下步骤。 a. 运行prepare脚本以启用 HTTPS ./prepare b. 如果 Harbor 正在运行，请停止并删除现有实例 您的图像数据保留在文件系统中，因此不会丢失任何数据。 docker-compose down -v c. 重启 docker-compose up -d 6.5 测试 6.5.1 仓库登陆 $ docker login ghost.harbor.com Authenticating with existing credentials... WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 6.5.2 界面登陆 6.5.3 推送镜像 $ docker push ghost.harbor.com/library/busybox:latest The push refers to repository [ghost.harbor.com/library/busybox] cfd97936a580: Pushed latest: digest: sha256:febcf61cd6e1ac9628f6ac14fa40836d16f3c6ddef3b303ff0321606e55ddd0b size: 527 6.5.4 拉取镜像 我们换到另一台机器192.168.211.71尝试一下拉取这个镜像，需要什么配置呢， /etc/docker/daemon.json配置如下： $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } 配置/etc/hosts 192.168.211.70 ghost.harbor.com 如果没有配置hosts可能会报这样的错 $ docker pull ghost.harbor.com/library/busybox:latest Error response from daemon: Get https://ghost.harbor.com/v2/: x509: certificate has expired or is not yet valid 当然，也有可能是两台机器的时间没有同步，需要配置ntp 另外要配置docker证书 mkdir -p /etc/docker/certs.d/ghost.harbor.com/ scp root@192.168.211.70:/etc/docker/certs.d/ghost.harbor.com/ghost.harbor.com.cert /etc/docker/certs.d/ghost.harbor.com/ scp root@192.168.211.70:/etc/docker/certs.d/ghost.harbor.com/ghost.harbor.com.key /etc/docker/certs.d/ghost.harbor.com/ scp root@192.168.211.70:/etc/docker/certs.d/ghost.harbor.com/ca.crt /etc/docker/certs.d/ghost.harbor.com/ 如果没有证书将会这样报错 $ docker pull ghost.harbor.com/library/busybox:latest Error response from daemon: Get https://ghost.harbor.com/v2/: x509: certificate signed by unknown authority 最后，经历重重险阻，成功了。 docker pull ghost.harbor.com/library/busybox:latest latest: Pulling from library/busybox Digest: sha256:febcf61cd6e1ac9628f6ac14fa40836d16f3c6ddef3b303ff0321606e55ddd0b Status: Downloaded newer image for ghost.harbor.com/library/busybox:latest ghost.harbor.com/library/busybox:latest harbor https ip访问部署成功结束 浏览器界面清理缓存 参考： 多种模式部署 配置 Harbor 组件之间的内部 TLS 通信 Harbor 安装故障排除 重新配置 Harbor 并管理 Harbor 生命周期 自定义 Harbor 令牌服务 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Registry/podman_deploy_private_registry.html":{"url":"Docker/Registry/podman_deploy_private_registry.html","title":"podman 部署私有镜像仓库","keywords":"","body":"Podman 部署私有镜像仓库1. 安装 Podman 和 httpd-tools2. 配置仓库存储位置3. 生成访问仓库的凭据3.1 htpasswd 用户名和密码3.2 TLS 密钥对4. 启动容器5. 测试5.1 登陆5.2 API访问5.3 镜像入库5.4 查询镜像信息Podman 部署私有镜像仓库 tagsstart registry tagsstop Podman是一个无守护进程的开源 Linux 原生工具，旨在使用开放容器倡议 ( OCI )容器和容器映像轻松查找、运行、构建、共享和部署应用程序。主要是由RedHat推动改进。 关于了解 Podman 更多内容： Podman 下一代 Linux 容器工具 Podman 入门指南 1. 安装 Podman 和 httpd-tools yum install -y podman httpd-tools 2. 配置仓库存储位置 存储目录为 /opt/registry/ mkdir -p /opt/registry/{auth,certs,data} Auth子目录存储htpasswd用于身份验证的文件。 Certs子目录存储仓库使用的证书验证。 Data目录存储存储在仓库中的实际镜像。 如果你想单独挂载一块盘来存储数据可以利用parted命令 sudo parted -s -a optimal -- /dev/sdb mklabel gpt sudo parted -s -a optimal -- /dev/sdb mkpart primary 0% 100% sudo parted -s -- /dev/sdb align-check optimal 1 sudo pvcreate /dev/sdb1 sudo vgcreate vg0 /dev/sdb1 sudo lvcreate -n registry -l +100%FREE vg0 sudo mkfs.xfs /dev/vg0/registry echo \"/dev/vg0/registry /opt/registry/data xfs defaults 0 0\" | sudo tee -a /etc/fstab 挂载验证 $ sudo mount -a $ df -hT /opt/registry/data Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/vg0-registry xfs 200G 1.5G 199G 1% /opt/registry/data 3. 生成访问仓库的凭据 3.1 htpasswd 用户名和密码 身份验证由一个简单的htpasswd文件和一个 SSL 密钥对提供 htpasswd将在该/opt/registry/auth/目录中创建一个名为 Bcrypt Htpasswd 的文件 htpasswd -bBc /opt/registry/auth/htpasswd registryuser registryuserpassword b通过命令提供密码。 B使用 Bcrypt 加密存储密码。 c创建文件。 用户名为 registryuser。 密码是 registryuserpassword。 查看文件 $ tac /opt/registry/auth/htpasswd registryuser:$2y$05$XciI1wfzkUETe7XazJfc/uftBnMQfYOV1jOnbV/QOXw/SXhmLsApK 3.2 TLS 密钥对 通过使用由可信机构（内部或外部）签名的密钥和证书或简单的自签名证书，仓库通过 TLS 得到保护。要使用自签名证书： cat ssl.conf [ req ] prompt = no distinguished_name = req_subj x509_extensions = x509_ext [ req_subj ] CN = Localhost [ x509_ext ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer basicConstraints = CA:true subjectAltName = @alternate_names [ alternate_names ] DNS.1 = localhost IP.1 = 192.168.10.80 EOF openssl req -config ssl.conf -new -x509 -nodes -sha256 -days 365 -newkey rsa:4096 -keyout /opt/registry/certs/domain.key -out /opt/registry/certs/domain.crt openssl x509 -inform PEM -in /opt/registry/certs/domain.crt -out /opt/registry/certs/domain.cert req OpenSSL 生成和处理证书请求。 -newkey OpenSSL 创建一个新的私钥和匹配的证书请求。 rsa:4096 OpenSSL 生成一个 4096 位的 RSA 密钥。 -nodes OpenSSL 私钥没有密码要求。私钥不会被加密。 -sha256 OpenSSL 使用 sha256 来签署请求。 -keyout OpenSSL 存储新密钥的名称和位置。 -x509 OpenSSL 生成一个自签名证书。 -days OpenSSL 密钥对有效的天数。 -out OpenSSL 在哪里存储证书。 输入证书的相应选项。CN=值是您的主机的主机名。主机的主机名应该可由 DNS 或/etc/hosts文件解析。 $ ll /opt/registry/certs/ total 12 -rw-r--r-- 1 root root 1842 Nov 21 20:01 domain.cert -rw-r--r-- 1 root root 1842 Nov 21 20:01 domain.crt -rw------- 1 root root 3272 Nov 21 20:01 domain.key 将服务器证书、密钥和 CA 文件复制到 podman证书文件夹中。您必须首先创建适当的文件夹 mkdir -p /etc/containers/certs.d/192.168.10.80\\:5000/ cp -r /opt/registry/certs/* /etc/containers/certs.d/192.168.10.80\\:5000/ 注意：如果仓库未使用 TLS 保护，则/etc/containers/registries.conf可能必须为仓库配置文件中的不安全设置。 该证书还必须得到您的主机和客户端的信任： cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust trust list | grep -i \"\" 4. 启动容器 $ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/registry latest 81c944c2288b 9 days ago 24.7 MB podman run --name myregistry \\ -p 5000:5000 \\ -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt\" \\ -e \"REGISTRY_HTTP_TLS_KEY=/certs/domain.key\" \\ -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\ -e REGISTRY_STORAGE_DELETE_ENABLED=true \\ -d \\ docker.io/library/registry:latest 选项的详细信息是： --name myregistry将容器命名为myregistry。 -p 5000:5000将容器中的端口 5000 公开为主机上的端口 5000。 -v /opt/registry/data:/var/lib/registry:z像 在具有正确 SELinux 上下文的容器中一样安装/opt/registry/data 在主机/var/lib/registry -v /opt/registry/auth:/auth:z/opt/registry/auth在主机上安装，就像/auth 在具有正确 SELinux 上下文的容器中一样。 -v opt/registry/certs:/certs:z像 在具有正确 SELinux 上下文的容器中一样安装/opt/registry/certs 在主机上 。/certs -e \"REGISTRY_AUTH=htpasswd\" 使用bcrypt加密htpasswd文件进行身份验证。由容器的 REGISTRY_AUTH_HTPASSWD_PATH 环境变量设置的文件位置。 -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" 指定用于htpasswd. -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd使用容器中的 bcrypt 加密/auth/htpasswd 文件。 -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt\"设置证书文件的路径。 -e \"REGISTRY_HTTP_TLS_KEY=/certs/domain.key\"设置私钥路径。 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true为 schema1 清单提供向后兼容性。 -e REGISTRY_STORAGE_DELETE_ENABLED=true 可以通过API 删除镜像 -d docker.io/library/registry:latest是一个允许存储和分发镜像的仓库应用程序。 注意：如果防火墙在主机上运行，​​则需要允许暴露的端口 (5000)。 firewall-cmd --add-port=5000/tcp --zone=internal --permanent firewall-cmd --add-port=5000/tcp --zone=public --permanent firewall-cmd --reload 或者直接关闭 systemctl stop firewalld && systemctl disable firewalld setenforce 0 5. 测试 5.1 登陆 docker login -u registryuser -p registryuserpassword 192.168.10.80:5000 Login Succeeded! 5.2 API访问 $ curl -k -u \"registryuser:registryuserpassword\" https://192.168.10.80:5000/v2/_catalog {\"repositories\":[]} 更多API 访问策略请参考这里 5.3 镜像入库 从公共拉取alpine:latest镜像 $ podman pull alpine:latest Resolved \"alpine\" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf) Trying to pull docker.io/library/alpine:latest... Getting image source signatures Copying blob ca7dd9ec2225 [--------------------------------------] 0.0b / 0.0b Copying config bfe296a525 done Writing manifest to image destination Storing signatures bfe296a525011f7eb76075d688c681ca4feaad5afe3b142b36e30f1a171dc99a 打标签 podman tag alpine:latest 192.168.10.80:5000/alpine:latest 推送入库 podman push 192.168.10.80:5000/alpine:latest 5.4 查询镜像信息 查询是否入库 $ curl -k -u \"registryuser:registryuserpassword\" https://192.168.10.80:5000/v2/_catalog {\"repositories\":[\"alpine\"]} 查看镜像标签 $ curl -k -u \"registryuser:registryuserpassword\" https://192.168.10.80:5000/v2/alpine/tags/list {\"name\":\"alpine\",\"tags\":[\"latest\"]} 查看镜像 manifests $ curl -k -u \"registryuser:registryuserpassword\"https://192.168.10.80:5000/v2/alpine/manifests/latest { \"schemaVersion\": 1, \"name\": \"alpine\", \"tag\": \"latest\", \"architecture\": \"amd64\", \"fsLayers\": [ { \"blobSum\": \"sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\" }, { \"blobSum\": \"sha256:60f8044dac9f779802600470f375c7ca7a8f7ad50e05b0ceb9e3b336fa5e7ad3\" } ], \"history\": [ { \"v1Compatibility\": \"{\\\"architecture\\\":\\\"amd64\\\",\\\"config\\\":{\\\"Hostname\\\":\\\"\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":[\\\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\\"],\\\"Cmd\\\":[\\\"/bin/sh\\\"],\\\"Image\\\":\\\"sha256:18f412e359de0426344f4fe1151796e2d9dc121b01d737e953f043a10464d0b7\\\",\\\"Volumes\\\":null,\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"OnBuild\\\":null,\\\"Labels\\\":null},\\\"container\\\":\\\"3cd2ce612b9119be9673860022420eee020f0a6d44e9072ca25196f4f0a4613d\\\",\\\"container_config\\\":{\\\"Hostname\\\":\\\"3cd2ce612b91\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":[\\\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\\"],\\\"Cmd\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"#(nop) \\\",\\\"CMD [\\\\\\\"/bin/sh\\\\\\\"]\\\"],\\\"Image\\\":\\\"sha256:18f412e359de0426344f4fe1151796e2d9dc121b01d737e953f043a10464d0b7\\\",\\\"Volumes\\\":null,\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"OnBuild\\\":null,\\\"Labels\\\":{}},\\\"created\\\":\\\"2022-11-12T04:19:23.199716539Z\\\",\\\"docker_version\\\":\\\"20.10.12\\\",\\\"id\\\":\\\"260323e12fa2abcb1ff61576931037c6f8538afeb5ff82fa256670a20b384b6b\\\",\\\"os\\\":\\\"linux\\\",\\\"parent\\\":\\\"faa2cddd53c99ad978614b839a2a20a47f143a4d6ecb86bda576dfb3124c0cad\\\",\\\"throwaway\\\":true}\" }, { \"v1Compatibility\": \"{\\\"id\\\":\\\"faa2cddd53c99ad978614b839a2a20a47f143a4d6ecb86bda576dfb3124c0cad\\\",\\\"created\\\":\\\"2022-11-12T04:19:23.05154209Z\\\",\\\"container_config\\\":{\\\"Cmd\\\":[\\\"/bin/sh -c #(nop) ADD file:ceeb6e8632fafc657116cbf3afbd522185a16963230b57881073dad22eb0e1a3 in / \\\"]}}\" } ], \"signatures\": [ { \"header\": { \"jwk\": { \"crv\": \"P-256\", \"kid\": \"5BQE:5CXW:TWNN:OFV7:ZPNY:ARAG:ZJ7K:Z5GI:ZVQ3:SZYQ:2M3J:D7YG\", \"kty\": \"EC\", \"x\": \"-JvBdARI6NPMx8g6d1zyPzmSkkZ8rKIcxdz2BEonpzU\", \"y\": \"4OlY36zLCvLHXzMrb4w8W2TZSJdVc5ijM0Y9DieEkWY\" }, \"alg\": \"ES256\" }, \"signature\": \"ZL0HFyuq9G9cYsBzZZqMlwGK3aQMJHFKeQ2Dh8XByzGKtfoJCJ5kQY0W3yynzb3Mj9WYrzeabZwey-dZIHt_7Q\", \"protected\": \"eyJmb3JtYXRMZW5ndGgiOjIwODgsImZvcm1hdFRhaWwiOiJDbjAiLCJ0aW1lIjoiMjAyMi0xMS0yMVQxMjoyNjowM1oifQ\" } ] } 参考： How to implement a simple personal/private Linux container image registry for internal use docker registry仓库私搭并配置证书 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Registry/docker_registry_1_deploy.html":{"url":"Docker/Registry/docker_registry_1_deploy.html","title":"docker registry 仓库搭建并配置证书","keywords":"","body":"docker registry 仓库搭建并配置证书1. 仓库简介2. Docker Registry角色2.1 Index2.2 Registry2.3 Registry Client3. registry部署3.1 拉取镜像3.2 运行容器创建registry仓库3.3 将镜像推入仓库3.4 配置Docker Registry签名证书3.5 使签名证书生效3.6 配置Docker Registry用户认证3.7 部署带有证书与用户认证的registry仓库3.8 登录测试docker registry 仓库搭建并配置证书 1. 仓库简介 Docker Registry，它是所有仓库（包括共有和私有）以及工作流的中央Registry Repositories（仓库）可以被标记为喜欢或者像书签一样标记起来。 用户可以在仓库下评论。 私有仓库和共有仓库类似，不同之处在于前者不会在搜索结果中显示，也没有访问它的权限。只有用户设置为合作者才能访问私有仓库。 成功推送之后配置webhooks。 2. Docker Registry角色 Docker Registry有三个角色，分别是index、registry和registry client。 2.1 Index index 负责并维护有关用户帐户、镜像的校验以及公共命名空间的信息。它使用以下组件维护这些信息： Web UI 元数据存储 认证服务 符号化 这也分解了较长的URL，以方便使用和验证用户存储库。 2.2 Registry registry是镜像和图表的仓库。然而，它没有一个本地数据库，也不提供用户的身份认证，由S3、云文件和本地文件系统提供数据库支持。此外，通过Index Auth service的Token方式进行身份认证。Registries可以有不同的类型。现在让我们来分析其中的几种类型： Sponsor Registry：第三方的registry，供客户和Docker社区使用。 Mirror Registry：第三方的registry，只让客户使用。 Vendor Registry：由发布Docker镜像的供应商提供的registry。 Private Registry：通过设有防火墙和额外的安全层的私有实体提供的registry。 2.3 Registry Client Docker充当registry客户端来负责维护推送和拉取的任务，以及客户端的授权。 3. registry部署 3.1 拉取镜像 docker pull registry 报错net/http: TLS handshake timeout 修改docker配置,使用国内镜像 daocloud镜像加速器 $ vim /etc/docker/daemon.json {\"registry-mirrors\": [\"http://d1d9aef0.m.daocloud.io\"]} $ systemctl restart docker $ docker pull registry 3.2 运行容器创建registry仓库 $ docker run -d --restart=always --name registry -p 5000:5000 -v /storage/registry:/var/lib/registry registry：2.3.0 $ docker ps 3.3 将镜像推入仓库 $ docker pull centos $ docker tag centos:latest 192.168.211.15:5000/centos:latest $ docker push 192.168.211.15:5000/centos:latest The push refers to a repository [192.168.211.15:5000/centos] Get https://192.168.211.15:5000/v1/_ping: http: server gave HTTP response to HTTPS client 在推送镜像中出现错误，因为client与Registry交互默认将采用https访问，但我们在install Registry时并未配置指定任何tls相关的key和crt文件，https将无法访问。因此， 我们需要配置客户端的Insecure Registry选项（另一种解决方案需要配置Registry的证书）。 $ vim /etc/sysconfig/docker OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false --insecure-registry 192.168.211.15:5000' $ docker stop registry $ systemctl restart docker $ docker start registry $ docker push 192.168.211.15:5000/centos:latest $ $ curl https://192.168.211.15:5000/v2/_catalog {\"repositories\":[\"centos\"]} #获取镜像列表 3.4 配置Docker Registry签名证书 在Docker Registry主机中生成OpenSSL的自签名证书： cat ssl.conf [ req ] prompt = no distinguished_name = req_subj x509_extensions = x509_ext [ req_subj ] CN = Localhost [ x509_ext ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer basicConstraints = CA:true subjectAltName = @alternate_names [ alternate_names ] DNS.1 = localhost IP.1 = 192.168.211.15 EOF $ openssl req -config ssl.conf -new -x509 -nodes -sha256 -days 365 -newkey rsa:4096 -keyout /certs/server-key.pem -out /certs/server-crt.pem 3.5 使签名证书生效 Docker Registry所在本机操作： 证书生成好了，客户端现在就不需要 --insecure-registry 了 $ vim /etc/sysconfig/docker OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false' $ mkdir -p /etc/docker/certs.d/192.168.211.15:5000/ $ cp /certs/server-crt.pem /etc/docker/certs.d/192.168.211.15\\:5000/ $ systemctl restart docker 客户端操作： $ mkdir -p /etc/docker/certs.d/192.168.211.15:5000/ $ scp /certs/server-crt.pem root@192.168.211.16:/etc/docker/certs.d/192.168.211.15:5000/ $ systemctl restart docker 3.6 配置Docker Registry用户认证 为了相对安全，可以给仓库加上基本的身份认证。使用 htpasswd 创建用户： $ htpasswd -Bbn testuser testpassword > /auth/htpasswd $ cat /auth/htpasswd testuser:$2y$05$MO4iv425uurfqY2Y/X71TuNTUPu4Vrn.oNE4NxRTjsPTTU6QywiwG 或者借用镜像命令创建用户 $ sudo sh -c \"docker run --entrypoint htpasswd registry:2.3.0 -Bbn testuser testpassword > /auth/htpasswd\" or $ docker run --entrypoint htpasswd registry:2.3.0 -Bbn testuser testpassword > auth/htpasswd 3.7 部署带有证书与用户认证的registry仓库 docker run -d \\ -p 5000:5000 \\ --name registry \\ --restart=always \\ -v /var/lib/registry:/var/lib/registry \\ -v /auth:/auth \\ -v /certs:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server-crt.pem \\ -e REGISTRY_HTTP_TLS_KEY=/certs/server-key.pem \\ -e REGISTRY_AUTH=htpasswd \\ -e REGISTRY_AUTH_HTPASSWD_REALM=\"Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ registry:2.3.0 3.8 登录测试 $ docker login -u testuser -p testpassword 192.168.211.15:5000 Login Succeeded 命令行访问仓库： $ curl -k -u \"testuser:testpassword\" https://192.168.211.15:5000/v2/_catalog {\"repositories\":[]} $ docker push 192.168.211.15:5000/centos:latest $ curl -k -u \"testuser:testpassword\" https://192.168.211.15:5000/v2/_catalog {\"repositories\":[\"centos\"]} #获取镜像列表 $ curl -k -u \"testuser:testpassword\" https://192.168.211.15:5000/v2/centos/tags/list {\"name\":\"centos\",\"tags\":[\"latest\"]} #查询镜像是否存在以及标签列表 参考： Docker Registry registry 镜像 Deploy a registry server Configuring a registry Docker Registry私有仓库介绍与部署章 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Registry/docker_registry_2_deploy_UI.html":{"url":"Docker/Registry/docker_registry_2_deploy_UI.html","title":"docker 部署 registry ui","keywords":"","body":"docker 部署 registry UI1. 创建 registry 仓库2. 将镜像推入仓库3. 创建registry-webdocker 部署 registry UI 1. 创建 registry 仓库 $ docker run -d --restart=always --name registry -p 5000:5000 -v /storage/registry:/var/lib/registry registry：2.3.0 $ docker ps 2. 将镜像推入仓库 $ docker pull centos $ docker tag centos:latest 192.168.211.15:5000/centos:latest $ docker push 192.168.211.15:5000/centos:latest The push refers to a repository [192.168.211.15:5000/centos] Get https://192.168.211.15:5000/v1/_ping: http: server gave HTTP response to HTTPS client 在推送镜像中出现错误，因为client与Registry交互默认将采用https访问，但我们在install Registry时并未配置指定任何tls相关的key和crt文件，https将无法访问。因此， 我们需要配置客户端的Insecure Registry选项（另一种解决方案需要配置Registry的证书）。 $ vim /etc/sysconfig/docker OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false --insecure-registry 192.168.211.15:5000' $ docker stop registry $ systemctl restart docker $ docker start registry $ docker push 192.168.211.15:5000/centos:latest $ $ curl https://192.168.211.15:5000/v2/_catalog {\"repositories\":[\"centos\"]} #获取镜像列表 3. 创建registry-web Docker官方只提供了REST API，并没有给我们一个界面。 可以使用Portus来管理私有仓库， 同时可以使用简单的UI管理工具， Docker提供私有库“hyper/docker-registry-web”， 下载该镜像就可以使用了。 $ docker run -d -p 8080:8080 --name registry-web --link registry -e REGISTRY_URL=http://registry:5000/v2 -e REGISTRY_NAME=localhost:5000 hyper/docker-registry-web 界面： 参考： registry-web Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Registry/centos_7.9_deploy_harbor_practice.html":{"url":"Docker/Registry/centos_7.9_deploy_harbor_practice.html","title":"centos 7 9 harbor 部署镜像仓库","keywords":"","body":"Centos 7.9 harbor 部署镜像仓库1. 安装 docker1.1 配置 docker2. 安装 docker-compose3. 下载 harbor4. 定制配置文件 harbor.yml5. 配置证书5.1 生成证书颁发机构证书生成 CA 证书5.2 生成服务器证书5.3 向 Harbor 和 Docker 提供证书6. 部署 harbor7. 测试Centos 7.9 harbor 部署镜像仓库 tagsstart registry tagsstop 1. 安装 docker Docker 安装 1.1 配置 docker $ cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"registry-mirrors\": [ \"https://hub-mirror.c.163.com\", \"https://mirror.baidubce.com\" ] } 启动 docker systemctl start docker 2. 安装 docker-compose 下载最新版本：https://github.com/docker/compose/releases sudo curl -L \"https://github.com/docker/compose/releases/download/v2.12.2/docker-compose-linux-x86_64\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose $ docker-compose --version Docker Compose version v2.12.2 3. 下载 harbor 下载最新harbor：https://github.com/goharbor/harbor/releases sudo curl -L \"https://github.com/goharbor/harbor/releases/download/v2.6.2/harbor-offline-installer-v2.6.2.tgz\" -o harbor-offline-installer-v2.6.2.tgz $ tar xzvf harbor-offline-installer-v2.6.2.tgz harbor/harbor.v2.6.2.tar.gz harbor/prepare harbor/LICENSE harbor/install.sh harbor/common.sh harbor/harbor.yml.tmpl $ ls harbor common.sh harbor.v2.6.2.tar.gz harbor.yml.tmpl install.sh LICENSE prepare 4. 定制配置文件 harbor.yml cp harbor.yml.tmpl harbor.yml $ vim harbor.yml hostname: harbor.fumai.com http: port: 80 https: port: 443 certificate: /data/cert/harbor.fumai.com.crt private_key: /data/cert/harbor.fumai.com.key harbor_admin_password: Harbor12345 database: password: root123 max_idle_conns: 100 max_open_conns: 900 data_volume: /data trivy: ignore_unfixed: false skip_update: false offline_scan: false security_check: vuln insecure: false jobservice: max_job_workers: 10 notification: webhook_job_max_retry: 10 chart: absolute_url: disabled log: level: info local: rotate_count: 50 rotate_size: 200M location: /var/log/harbor _version: 2.6.0 proxy: http_proxy: https_proxy: no_proxy: components: - core - jobservice - trivy upload_purging: enabled: true age: 168h interval: 24h dryrun: false cache: enabled: false expire_hours: 24 5. 配置证书 5.1 生成证书颁发机构证书 生成 CA 证书私钥ca.key openssl genrsa -out ca.key 4096 生成 CA 证书 openssl req -x509 -new -nodes -sha512 -days 3650 -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.fumai.com\" -key ca.key -out ca.crt 5.2 生成服务器证书 生成私钥 openssl genrsa -out harbor.fumai.com.key 4096 生成证书签名请求 (CSR) openssl req -sha512 -new -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=harbor.fumai.com\" -key harbor.fumai.com.key -out harbor.fumai.com.csr 生成 x509 v3 扩展文件 cat > v3.ext 使用该v3.ext文件为您的 Harbor 主机生成证书 openssl x509 -req -sha512 -days 3650 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in harbor.fumai.com.csr -out harbor.fumai.com.crt 5.3 向 Harbor 和 Docker 提供证书 将服务器证书和密钥复制到 Harbor 主机上的 certficates 文件夹中 mkdir -p /data/cert cp harbor.fumai.com.crt /data/cert/ cp harbor.fumai.com.key /data/cert/ 转换harbor.fumai.com.crt为harbor.fumai.com.key.cert，供 docker使用 openssl x509 -inform PEM -in harbor.fumai.com.crt -out harbor.fumai.com.cert 将服务器证书、密钥和 CA 文件复制到 Harbor 主机上的 docker 证书文件夹中。您必须首先创建适当的文件夹 mkdir -p /etc/docker/certs.d/harbor.fumai.com/ cp harbor.fumai.com.cert /etc/docker/certs.d/harbor.fumai.com/ cp harbor.fumai.com.key /etc/docker/certs.d/harbor.fumai.com/ cp ca.crt /etc/docker/certs.d/harbor.fumai.com/ 配置生效 systemctl daemon-reload && systemctl restart docker 6. 部署 harbor 运行prepare脚本以启用 HTTPS ./prepare 输出： prepare base dir is set to /root/harbor Unable to find image 'goharbor/prepare:v2.6.2' locally v2.6.2: Pulling from goharbor/prepare d46c4d5563bc: Pulling fs layer 2014728b1023: Pulling fs layer aab288eb9305: Pulling fs layer f5624bd14a09: Waiting d706af45859a: Waiting 758da3aa4679: Waiting af6231a55025: Waiting 8c758607ff4a: Waiting fb477479c0dd: Waiting 99767f301e98: Waiting v2.6.2: Pulling from goharbor/prepare d46c4d5563bc: Pull complete 2014728b1023: Pull complete aab288eb9305: Pull complete f5624bd14a09: Pull complete d706af45859a: Pull complete 758da3aa4679: Pull complete af6231a55025: Pull complete 8c758607ff4a: Pull complete fb477479c0dd: Pull complete 99767f301e98: Pull complete Digest: sha256:43e0c17257f4ebe982edd0fbf8e8f2081c81550769dc92ed06ed16e1641fc8a9 Status: Downloaded newer image for goharbor/prepare:v2.6.2 Generated configuration file: /config/portal/nginx.conf Generated configuration file: /config/log/logrotate.conf Generated configuration file: /config/log/rsyslog_docker.conf Generated configuration file: /config/nginx/nginx.conf Generated configuration file: /config/core/env Generated configuration file: /config/core/app.conf Generated configuration file: /config/registry/config.yml Generated configuration file: /config/registryctl/env Generated configuration file: /config/registryctl/config.yml Generated configuration file: /config/db/env Generated configuration file: /config/jobservice/env Generated configuration file: /config/jobservice/config.yml Generated and saved secret to file: /data/secret/keys/secretkey Successfully called func: create_root_cert Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir 启动 docker-compose up -d 查看容器状态 $ docker-compose ps NAME COMMAND SERVICE STATUS PORTS harbor-core \"/harbor/entrypoint.…\" core running (healthy) harbor-db \"/docker-entrypoint.…\" postgresql running (healthy) harbor-jobservice \"/harbor/entrypoint.…\" jobservice running (healthy) harbor-log \"/bin/sh -c /usr/loc…\" log running (healthy) 127.0.0.1:1514->10514/tcp harbor-portal \"nginx -g 'daemon of…\" portal running (healthy) nginx \"nginx -g 'daemon of…\" proxy running (healthy) 0.0.0.0:80->8080/tcp, :::80->8080/tcp, 0.0.0.0:443->8443/tcp, :::443->8443/tcp redis \"redis-server /etc/r…\" redis running (healthy) registry \"/home/harbor/entryp…\" registry running (healthy) registryctl \"/home/harbor/start.…\" registryctl running (healthy) 7. 测试 命令行登陆 $ docker login harbor.fumai.com Username: admin Password: Harbor12345 WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 界面登陆 https://harbor.fumai.com admin/Harbor12345 终于部署结束了，如果你想参考更多关于 harbor 内容，请参考： harbor 初级部署入门指南 官方 Harbor 亨利笔记 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Security/":{"url":"Docker/Security/","title":"Security","keywords":"","body":"Docker 安全 OverviewDocker 安全 Overview Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Storage/":{"url":"Docker/Storage/","title":"Storage","keywords":"","body":"Docker Volume1. 简介2. 原理3. 命令4. 应用4.1 目录 bind mount4.2 隐式创建 Docker volume4.3 显式创建 Docker 卷4.4 从 Dockerfile 声明一个 Docker 卷4.5 另一种方式挂载 mount 参数4.6 使用配置卷 docker-compose4.7 从共享卷在容器之间复制文件Docker Volume 1. 简介 默认情况下，在容器内创建的所有文件都存储在可写容器层上。这意味着： 当该容器不再存在时，数据不会持续存在，并且如果另一个进程需要数据，则可能很难将数据从容器中取出。 容器的可写层与运行容器的主机紧密耦合。您无法轻松地将数据移动到其他地方。 写入容器的可写层需要 存储驱动程序来管理文件系统。存储驱动程序提供了一个联合文件系统，使用 Linux 内核。与使用直接写入主机文件系统的数据卷相比，这种额外的抽象会降低性能 。 Docker 有两个选项让容器在主机上存储文件，以便即使在容器停止后文件也能持久保存：volumes和 bind mounts。 Docker 还支持将文件存储在主机内存中的容器。此类文件不会持久保存。如果您在 Linux 上运行 Docker，则使用tmpfs mount将文件存储在主机的系统内存中。 挂载类型 volumes：由 Docker（/var/lib/docker/volumes/在 Linux 上）管理的主机文件系统的一部分中。非 Docker 进程不应修改文件系统的这一部分。卷是在 Docker 中持久化数据的最佳方式 bind mounts：可以存储在主机系统的任何位置。它们甚至可能是重要的系统文件或目录。Docker 主机或 Docker 容器上的非 Docker 进程可以随时修改它们。 tmpfs mounts：挂载仅存储在主机系统的内存中，永远不会写入主机系统的文件系统 2. 原理 在 linux 系统上，docker 将images, containers, volumes等相关的数据存储在/var/lib/docker下。 当我们运行docker build命令时，docker 会为 dockerfile 中的每条指令构建一层。这些图像层是只读层。当我们运行docker run命令时，docker 会构建容器层，它们是读写层。 您可以在容器上创建新文件，例如下图中的temp.txt。您还可以修改容器上属于图像层的文件，例如下图中的app.py。执行此操作时，会在容器层上创建该文件的本地副本，并且更改仅存在于容器上——这称为 Copy-on-Write 机制。这很重要，因为多个容器和子图像使用相同的图像层。容器上的文件的生命周期与容器的生命周期一样长。当容器被销毁时，其上的文件/修改也会被销毁。为了持久化数据，我们可以使用我们在上一节中看到的卷映射技术。 3. 命令 您可以使用docker volume create命令创建 docker 卷。此命令将在/var/lib/docker/volumes目录中创建一个卷。 docker volume create data_volume docker volume ls docker volume inspect data_volume docker run命令时，您可以使用-v标志指定要使用的卷。这称为卷挂载。 docker run -v data_volume:/var/lib/postgres postgres 如果该卷不存在，docker 会为您创建一个。现在，即使容器被销毁，数据也会保留在卷中。 如果您想将数据放在 docker 主机上的特定位置或磁盘上已有数据，您也可以将此位置挂载到容器上。这称为绑定安装。 docker run -v /data/postgres:/var/lib/postgres postgres 删除 docker volume rm data_volume 4. 应用 4.1 目录 bind mount echo \"Hello from Host\" > ./target/index.html docker run -it --rm --name nginx -p 8080:80 -v \"$(pwd)\"/target:/usr/share/nginx/html nginx 访问：http://localhost:8080/，您应该会看到“Hello from Host” 4.2 隐式创建 Docker volume #创建demo-earthly卷挂载 docker run -it --rm --name nginx -p 8080:80 -v demo-earthly:/usr/share/nginx/html nginx docker volume ls ls /var/lib/docker/volumes/target/_data/demo-earthly #查看卷内容 docker run -it --rm -v demo-earthly:/opt/demo-earthly ubuntu ls /opt/demo-earthly 4.3 显式创建 Docker 卷 docker volume create --name demo-earthly 4.4 从 Dockerfile 声明一个 Docker 卷 可以使用语句在 Dockerfile 中声明卷VOLUME Dockerfile： FROM nginx:latest RUN echo \"Hello from Volume\" > /usr/share/nginx/html/index.html VOLUME /usr/share/nginx/html 利用Dockerfile构建镜像 $ docker build -t demo-earthly . $ docker run -p 8080:80 demo-earthly $ docker volume ls DRIVER VOLUME NAME local 20879e3f0bfaf0eed63cb7f37c4b9545084a703f888a230b8aedc2082c836281 $ docker volume inspect 20879e3f0bfaf0eed63cb7f37c4b9545084a703f888a230b8aedc2082c836281 [ { \"CreatedAt\": \"2022-07-28T11:02:14+08:00\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/20879e3f0bfaf0eed63cb7f37c4b9545084a703f888a230b8aedc2082c836281/_data\", \"Name\": \"20879e3f0bfaf0eed63cb7f37c4b9545084a703f888a230b8aedc2082c836281\", \"Options\": null, \"Scope\": \"local\" } ] $ docker inspect -f '{{ .Mounts }}' amazing_carson [{volume 20879e3f0bfaf0eed63cb7f37c4b9545084a703f888a230b8aedc2082c836281 /var/lib/docker/volumes/20879e3f0bfaf0eed63cb7f37c4b9545084a703f888a230b8aedc2082c836281/_data /usr/share/nginx/html local true }] 每次启动一个新容器时，都会创建另一个卷，内容为/usr/share/nginx/html. 4.5 另一种方式挂载 mount 参数 -v并且--volume是使用以下语法将卷安装到容器的最常见方法： -v :: 该卷将以只读方式安装： docker run -it -v demo-volume:/data:ro ubuntu 另一种方法-v是--mount选项添加到docker run命令中。--mount是 的更详细的对应物-v。 语法： docker run --mount source=[volume_name],destination=[path_in_container] [docker_image] 示例 docker run -it --name=example --mount source=demo-volume,destination=/data ubuntu 4.6 使用配置卷 docker-compose 使用docker-compose命令在多个容器之间轻松共享数据更方便。 docker-compose.yml 目录挂载 version: \"3.2\" services: web: image: nginx:latest ports: - 8080:80 volumes: - ./target:/usr/share/nginx/html docker-compose.yml 创建卷 version: \"3.2\" services: web: image: nginx:latest ports: - 8080:80 volumes: - html_files:/usr/share/nginx/html web1: image: nginx:latest ports: - 8081:80 volumes: - html_files:/usr/share/nginx/html volumes: html_files: 声明了一个名为并html_files在服务中使用它的卷。多个容器（web、web1）可以挂载同一个卷。 运行docker-compose up将创建一个名为_html_filesif 它不存在的卷。然后运行docker volume ls以列出创建的两个卷，从项目名称开始。 您还可以在 docker-compose 文件之外管理容器，但您仍然需要在下面声明它们volumes并设置属性external: true。 version: \"3.2\" services: web: image: nginx:latest ports: - 8080:80 volumes: - html_files:/usr/share/nginx/html volumes: html_files: external: true 如果你没有html_files，你可以使用docker volume create html_files来创建它。当你添加 时external，Docker 会找出卷是否存在；但如果没有，就会报错。 4.7 从共享卷在容器之间复制文件 创建容器并创建挂载卷 docker create volume demo-earthly docker run -it --name=another-example --mount source=demo-volume,destination=/data ubuntu root@ded392c589ea:/# touch /data/demo.txt 导航到数据卷目录并使用命令创建一个文件touch demo.txt。退出容器，然后启动一个another-example-two具有相同数据量的新容器： docker run -it --name=another-example-two --mount source=demo-volume,destination=/data ubuntu root@feef37293ea5:/# ls /data 参考： Manage data in Docker Docker Storage Understanding Docker Volumes Guide to Docker Volumes Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Storage/docker_storage_1_docker_Device_Mapper_introduce.html":{"url":"Docker/Storage/docker_storage_1_docker_Device_Mapper_introduce.html","title":"docker device mapper 简介","keywords":"","body":"Docker Device Mapper 简介1. 简介2. 用户空间和内核空间3. Device Mapper 技术分析4. Docker 中的 Device Mapper 核心技术Docker Device Mapper 简介 1. 简介 Device Mapper 是 linux 的内核用来将块设备映射到虚拟块设备的 framework，它支持许多高级卷管理技术。docker 的 devicemapper 存储驱动程序利用此框架的自动精简配置(thin provisioning) 和快照功能来管理 docker 镜像和容器。本文将 Device Mapper 存储驱动称为 devicemapper，将它的内核框架称为 Device Mapper。 Device Mapper 不同于 AUFS、ext4、NFS 等，因为它并不是一个文件系统（File System），而是 Linux 内核映射块设备的一种技术框架。提供的一种从逻辑设备（虚拟设备）到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。 当前比较流行的 Linux 下的逻辑卷管理器如 LVM2（Linux Volume Manager 2 version)、EVMS(Enterprise Volume Management System)、dmraid(Device Mapper Raid Tool)等都是基于该机制实现的。 值得一提的是 Device Mapper 工作在块级别（block），并不 工作在文件级别（file）。Device Mapper 自 Linux 2.6.9 后编入 Linux 内核，所有基于 Linux 内核 2.6.9 以后的发行版都内置 Device Mapper，但你需要进行一些额外的配置才能在 docker 中使用它。比如在 RHEL 和 CentOS 系统中，docker 默认使用的存储驱动是 overlay。 devicemapper 存储驱动使用专用于 docker 的块设备，它运行在块级别上而不是文件级别。使用块设备比直接使用文件系统性能更好，通过向 Docker 的宿主机添加物理存储可以扩展块设备的存储空间。 2. 用户空间和内核空间 Device Mapper主要分为用户空间部分和内核空间部分 用户空间相关部分主要负责配置具体的策略和控制逻辑，比如逻辑设备和哪些物理设备建立映射，怎么建立这些映射关系等，包含 device mapper 库和 dmsetup 工具。对用户空间创建删除 device mapper 设备的操作进行封装。 内核中主要提供完成这些用户空间策略所需要的机制，负责具体过滤和重定向 IO 请求。通过不同的驱动插件，转发 IO 请求至目的设备上。附上 Device Mapper 架构图。 3. Device Mapper 技术分析 Device Mapper 作为 Linux 块设备映射技术框架，向外部提供逻辑设备。包含三个重要概念，映射设备（mapped device），映射表（map table），目标设备（target device）。 映射设备即对外提供的逻辑设备，映射设备向下寻找必须找到支撑的目标设备。 映射表存储映射设备和目标设备的映射关系。 目标设备可以是映射设备或者物理设备，如果目标设备是一块映射设备，则属于嵌套，理论上可以无限迭代下去。 简而言之，Device Mapper 对外提供一个虚拟设备供使用，而这块虚拟设备可以通过映射表找到相应的地址，该地址可以指向一块物理设备，也可以指向一个虚拟设备。 映射表，是由用户空间创建，传递到内核空间。映射表里有映射设备逻辑的起始地址、范围、和表示在目标设备所在物理设备的地址偏移量以及Target 类型等信息（注：这些地址和偏移量都是以磁盘的扇区为单位的，即 512 个字节大小，所以，当你看到 128 的时候，其实表示的是 128*512=64K）。 映射驱动在内核空间是插件，Device Mapper 在内核中通过一个一个模块化的 Target Driver 插件实现对 IO 请求的过滤或者重新定向等工作，当前已经实现的插件包括软 Raid、加密、多路径、镜像、快照等，这体现了在 Linux 内核设计中策略和机制分离的原则。 Device Mapper 中的 IO 流处理，从虚拟设备（逻辑设备）根据映射表并指定特定的映射驱动转发到目标设备上。 4. Docker 中的 Device Mapper 核心技术 Docker 的 devicemapper 驱动有三个核心概念，copy on-write（写复制），thin-provisioning（精简配置）。snapshot（快照），首先简单介绍一下这三种技术。 CoW（copy on write）写复制：一些文件系统提供的写时复制策略。 aufs 的 cow 原理如下： 当容器需要修改一个文件，而该文件位于低层 branch 时，顶层 branch 会直接复制低层 branch 的文件至顶层再进行修改，而低层的文件不变，这种方式即是 CoW 技术（写复制）。 当容器删除一个低层 branch 文件时，只是在顶层 branch 对该文件进行重命名并隐藏，实际并未删除文件，只是不可见。 下图所示，容器层所见 file1 文件为镜像层文件，当需要修改 file1 时，会从镜像层把文件复制到容器层，然后进行修改，从而保证镜像层数据的完整性和复用性。 下图所示，当需要删除 file1 时，由于 file1 是镜像层文件，容器层会创建一个 .wh 前置的隐藏文件，从而实现对 file1 的隐藏，实际并未删除 file1，从而保证镜像层数据的完整性和复用性。 devicemapper 支持在块级别（block）写复制。 Snapshot（快照技术）：关于指定数据集合的一个完全可用拷贝，该拷贝包括相应数据在某个时间点（拷贝开始的时间点）的映像。快照可以是其所表示的数据的一个副本，也可以是数据的一个复制品。而从具体的技术细节来讲，快照是指向保存在存储设备中的数据的引用标记或指针。 Thin-provisioning（精简配置），直译为精简配置。Thin-provisioning 是动态分配，需要多少分配多少，区别于传统分配固定空间从而造成的资源浪费。 它是什么意思呢？你可以联想一下我们计算机中的内存管理中用到的——“虚拟内存技术”——操作系统给每个进程 N 多 N 多用不完的内址地址（32 位下，每个进程可以有最多 2GB 的内存空间），但是呢，我们知道，物理内存是没有那么多的，如果按照进程内存和物理内存一一映射来玩的话，那么，我们得要多少的物理内存啊。所以，操作系统引入了虚拟内存的设计，意思是，我逻辑上给你无限多的内存，但是实际上是实报实销，因为我知道你一定用不了那么多，于是，达到了内存使用率提高的效果。（今天云计算中很多所谓的虚拟化其实完全都是在用和“虚拟内存”相似的 Thin Provisioning 的技术，所谓的超配，或是超卖）。 好了，话题拉回来，我们这里说的是存储。看下面两个图，第一个是 Fat Provisioning，第二个是 Thin Provisioning，其很好的说明了是个怎么一回事（和虚拟内存是一个概念）。 下图中展示了某位用户向服务器管理员请求分配 10TB 的资源的情形。实际情况中这个数值往往是峰值，根据使用情况，分配 2TB 就已足够。因此，系统管理员准备 2TB 的物理存储，并给服务器分配 10TB 的虚拟卷。服务器即可基于仅占虚拟卷容量 1/5 的现有物理磁盘池开始运行。这样的“始于小”方案能够实现更高效地利用存储容量。 那么，Docker 是怎么使用 Thin Provisioning 这个技术做到像 UnionFS 那样的分层镜像的呢？答案是，Docker 使用了 Thin Provisioning 的 Snapshot 的技术。下面一篇我们来介绍一下 Thin Provisioning 的 Snapshot。 来自： https://fuckcloudnative.io/posts/devicemapper-theory/ Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Storage/docker_storage_2_docker_drive_devicemapper_config.html":{"url":"Docker/Storage/docker_storage_2_docker_drive_devicemapper_config.html","title":"docker  storage 驱动 devicemapper 配置","keywords":"","body":"Docker storage 驱动 devicemapper 配置1. 准备条件2. 配置Docker使用devicemapper2.1 生产环境配置direct-lvm模式3. 管理 devicemapper3.1 监控 thin pool3.2 为正在运行的设备增加容量4. devicemapper 存储驱动的工作原理4.1 磁盘上的镜像和容器层4.2 镜像分层与共享5. devicemapper 读写数据的过程5.1 读数据5.2 写数据6. Device Mapper 对 Docker 性能的影响6.1 按需分配对性能的影响6.2 写时拷贝对性能的影响6.3 其他注意事项Docker storage 驱动 devicemapper 配置 1. 准备条件 devicemapper 存储驱动是 RHEL, CentOS 和 Oracle Linux 系统上唯一一个支持 Docker EE 和 Commercially Supported Docker Engine (CS-Engine) 的存储驱动，具体参考 Product compatibility matrix. devicemapper 在 CentOS, Fedora, Ubuntu 和 Debian 上也支持 Docker CE。 如果你更改了 Docker 的存储驱动，那么你之前在本地创建的所有容器都将无法访问。 2. 配置Docker使用devicemapper Docker 主机运行 devicemapper 存储驱动时，默认的配置模式为 loop-lvm。此模式使用空闲的文件来构建用于镜像和容器快照的精简存储池。该模式设计为无需额外配置开箱即用(out-of-the-box)。不过生产部署不应该以 loop-lvm 模式运行。 2.1 生产环境配置direct-lvm模式 CentOS7 从 Docker 17.06 开始支持通过 Docker 自动配置 direct-lvm，所以推荐使用该工具配置。当然也可以手动配置 lvm，添加相关配置选项，不过过程较为繁琐一点。 2.1.1 自动配置 direct-lvm 模式 该方法只适用于一个块设备，如果你有多个块设备，请通过手动配置 direct-lvm 模式。 示例配置文件位置 /usr/lib/docker-storage-setup/docker-storage-setup，可以查看其中相关配置的详细说明，或者通过 man docker-storage-setup 获取帮助，以下介绍几个关键的选项： | 参数 | 解释 | 是否必须 | 默认值 | 示例 | |-------------------------------|---------------------------------------------------------------------|------|-------|----------------------------------| | dm.directlvm_device | 准备配置 direct-lvm 的块设备的路径 | 是 | | dm.directlvm_device=\"/dev/xvdf\" | | dm.thinp_percent | 定义创建 data thin pool 的大小 | 否 | 95 | dm.thinp_percent=95 | | dm.thinp_metapercent | 定义创建 metadata thin pool 的大小 | 否 | 1 | dm.thinp_metapercent=1 | | dm.thinp_autoextend_threshold | 定义自动扩容的百分比，100 表示 disable，最小为 50，参考 lvmthin — LVM thin provisioning | 否 | 80 | dm.thinp_autoextend_threshold=80 | | dm.thinp_autoextend_percent | 定义每次扩容的大小，100 表示 disable | 否 | 20 | dm.thinp_autoextend_percent=20 | | dm.directlvm_device_force | 当块设备已经存在文件系统时，是否格式化块设备 | 否 | false | dm.directlvm_device_force=true | 编辑 /etc/docker/daemon.json，设置好参数后重新启动 Docker 使更改生效。下面是一个示例： { \"storage-driver\": \"devicemapper\", \"storage-opts\": [ \"dm.directlvm_device=/dev/xdf\", \"dm.thinp_percent=95\", \"dm.thinp_metapercent=1\", \"dm.thinp_autoextend_threshold=80\", \"dm.thinp_autoextend_percent=20\", \"dm.directlvm_device_force=false\" ] } 关于存储的更多参数请参考： Stable Edge 2.1.2 手动配置 direct-lvm 模式 下面的步骤创建一个逻辑卷，配置用作存储池的后端。我们假设你有在 /dev/xvdf 的充足空闲空间的块设备。也假设你的 Docker daemon 已停止。 1.登录你要配置的 Docker 主机并停止 Docker daemon。 2.安装LVM2软件包。LVM2软件包含管理Linux上逻辑卷的用户空间工具集。 RHEL / CentOS: device-mapper-persistent-data, lvm2 以及相关依赖 Ubuntu / Debian: thin-provisioning-tools, lvm2 以及相关依赖 3.创建物理卷。 $ pvcreate /dev/xvdf Physical volume \"/dev/xvdf\" successfully created. 4.创建一个 “docker” 卷组。 $ vgcreate docker /dev/xvdf Volume group \"docker\" successfully created 5.创建一个名为thinpool的存储池。 在此示例中，设置池大小为 “docker” 卷组大小的 95％。 其余的空闲空间可以用来自动扩展数据或元数据。 $ lvcreate --wipesignatures y -n thinpool docker -l 95%VG Logical volume \"thinpool\" created. $ lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG Logical volume \"thinpoolmeta\" created. 6.将存储池转换为 thinpool 格式。 $ lvconvert -y \\ --zero n \\ -c 512K \\ --thinpool docker/thinpool \\ --poolmetadata docker/thinpoolmeta WARNING: Converting logical volume docker/thinpool and docker/thinpoolmeta to thin pool's data and metadata volumes with metadata wiping. THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.) Converted docker/thinpool to thin pool. 7.通过 lvm profile 配置存储池的自动扩展。 $ vi /etc/lvm/profile/docker-thinpool.profile 8.设置参数 thin_pool_autoextend_threshold 和 thin_pool_autoextend_percent 的值。 设置 thin_pool_autoextend_threshold 值。这个值应该是之前设置存储池余下空间的百分比(100 = disabled)。 thin_pool_autoextend_threshold = 80 设置当存储池自动扩容时，增加存储池的空间百分比（100 =禁用） thin_pool_autoextend_percent = 20 检查你的 docker-thinpool.profile 的设置。一个示例 /etc/lvm/profile/docker-thinpool.profile 应该类似如下： activation { thin_pool_autoextend_threshold=80 thin_pool_autoextend_percent=20 } 9.应用新的 lvm 配置。 $ lvchange --metadataprofile docker-thinpool docker/thinpool Logical volume docker/thinpool changed. 10.查看卷的信息，验证 lv 是否受监控。 $ lvs -o+seg_monitor LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert Monitor thinpool docker twi-a-t--- 95.00g 0.00 0.01 monitored 11.备份 Docker 存储。 $ mkdir /var/lib/docker.bk $ mv /var/lib/docker/* /var/lib/docker.bk 12.配置一些特定的 devicemapper 选项。 $ cat /etc/docker/daemon.json { \"storage-driver\": \"devicemapper\", \"storage-opts\": [ \"dm.thinpooldev=/dev/mapper/docker-thinpool\", \"dm.use_deferred_removal=true\", \"dm.use_deferred_deletion=true\" ] } Note: Always set both dm.use_deferred_removal=true and dm.use_deferred_deletion=true to prevent unintentionally leaking mount points. 启用上述2个参数来阻止可能意外产生的挂载点泄漏问题 检查主机上的 devicemapper 结构 你可以使用 lsblk 命令来查看以上创建的设备文件和存储池。 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk └─xvda1 202:1 0 8G 0 part / xvdf 202:80 0 10G 0 disk ├─vg--docker-data 253:0 0 90G 0 lvm │ └─docker-202:1-1032-pool 253:2 0 10G 0 dm └─vg--docker-metadata 253:1 0 4G 0 lvm └─docker-202:1-1032-pool 253:2 0 10G 0 dm 下图显示由 lsblk 命令输出的之前镜像的详细信息。 可以看出，名为 Docker-202:1-1032-pool 的 pool 横跨在 data 和 metadata 设备之上。pool 的命名规则为： Docker-主设备号:二级设备号-inode号-pool 3. 管理 devicemapper 3.1 监控 thin pool 不要过于依赖 lvm 的自动扩展，通常情况下 Volume Group 会自动扩展，但有时候 volume 还是会被塞满，你可以通过命令 lvs 或 lvs -a 来监控 volume 剩余的空间。也可以考虑使用 nagios 等监控工具来进行监控。 可以查看 lvm 日志，了解 thin pool 在自动扩容触及阈值时的状态： $ journalctl -fu dm-event.service 如果你在使用精简池（thin pool）的过程中频繁遇到问题，你可以在 /etc/docker.daemon.json 中设置参数 dm.min_free_space 的值（表示百分比）。例如将其设置为 10，以确保当可用空间达到或接近 10％ 时操作失败，并发出警告。参考 storage driver options in the Engine daemon reference. 3.2 为正在运行的设备增加容量 如果 lv 的存储空间已满，并且 vg 处于满负荷状态，你可以为正在运行的 thin-pool 设备增加存储卷的容量，具体过程取决于您是使用 loop-lvm 精简池还是使用 direct-lvm 精简池。 3.2.1 调整 loop-lvm 精简池的大小 调整 loop-lvm 精简池的最简单方法是使用 device_tool 工具，你也可以使用操作系统自带的工具。 a. 使用 device_tool 工具 在 docker 官方 github 仓库的 contrib/ 目录中有一个社区贡献的脚本 device_tool.go，你可以通过此工具免去繁琐的步骤来调整 loop-lvm 精简池的大小。这个工具不能保证 100% 有效，最好不要在生产环境中使用 loop-lvm 模式。 clone 整个仓库 docker-ce，切换到目录 contrib/docker-device-tool ，按照 README.md 中的说明编译该工具。 使用该工具。例如调整 thin pool 的大小为 200GB。 $ ./device_tool resize 200GB b. 使用操作系统工具 如果你不想使用 device_tool 工具，可以通过操作系统工具手动调整 loop-lvm 精简池的大小。 在 loop-lvm 模式中，Docker 使用的 Device Mapper 设备默认使用 loopback 设备，后端为自动生成的稀疏文件，如下: $ ls -lsh /var/lib/docker/devicemapper/devicemapper/ 总用量 510M 508M -rw-------. 1 root root 100G 10月 30 00:00 data 1.9M -rw-------. 1 root root 2.0G 10月 30 00:00 metadata data [存放数据] 和 metadata [存放元数据] 的大小从输出可以看出初始化默认为 100G 和 2G 大小，都是稀疏文件，使用多少占用多少。 Docker 在初始化的过程中，创建 data 和 metadata 这两个稀疏文件，并分别附加到回环设备 /dev/loop0 和 /dev/loop1 上，然后基于回环设备创建 thin pool。 默认一个 container 最大存放数据不超过 10G。 查看 data 和 metadata 的文件路径： $ docker info |grep 'loop file' Data loop file: /var/lib/docker/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/metadata 按照以下步骤来增加精简池的大小。在这个例子中，thin-pool 原来的容量为 100GB，增加到200GB。 查看 data 和 metadata 的大小。 $ ls -lh /var/lib/docker/devicemapper/ total 1175492 -rw------- 1 root root 100G Mar 30 05:22 data -rw------- 1 root root 2.0G Mar 31 11:17 metadata 使用 truncate 命令将数据文件的大小增加到 200G。 $ truncate -s 200G /var/lib/docker/devicemapper/data 注意：减小数据文件的大小有可能会对数据造成破坏，请慎重考虑。 验证文件大小。 $ ls -lh /var/lib/docker/devicemapper/ total 1.2G -rw------- 1 root root 200G Apr 14 08:47 data -rw------- 1 root root 2.0G Apr 19 13:27 metadata 可以看到 loopback 文件的大小已经改变，但还没有保存到内存中。 在内存中列出环回设备的大小，重新加载该设备，然后再次列出大小。 $ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ] 100 $ losetup -c /dev/loop0 $ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ] 200 重新加载之后，loopback 设备的大小变为 200GB。 重新加载 devicemapper thin pool。 查看 thin pool 的名称 $ dmsetup status | grep ' thin-pool ' | awk -F ': ' {'print $1'} 查看当前卷的信息表 $ dmsetup table docker-8:1-123141-pool 0 209715200 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing 第二个数字是设备的大小，表示有多少个 512－bytes 的扇区。 128 是最小的可分配的 sector 数。 32768 是最少可用 sector 的 water mark，也就是一个 threshold。 1 代表有一个附加参数。 skip_block_zeroing是个附加参数，表示略过用0填充的块。 使用输出的第二个字段计算扩展后的 thin pool 总大小，该字段表示有多少个扇区。100G 的文件含有 209715200 个扇区，扩展到 200G 后，扇区数为 419430400。 使用新的扇区数重新加载 thin pool。 $ dmsetup suspend docker-8:1-123141-pool $ dmsetup reload docker-8:1-123141-pool --table '0 419430400 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing' $ dmsetup resume docker-8:1-123141-pool 3.2.2 调整 direct-lvm 精简池的大小 要调整 direct-lvm 精简池的大小，需要添加一块新的块设备到 Docker 的宿主机。并记下内核分配给它的设备名称。例如新的块设备名称为 /dev/xvdg。 按照以下步骤来增加 direct-lvm 精简池的大小，请根据实际情况替换以下部分参数。 查看卷组的信息。 使用 pvdisplay 命令查看精简池当前正在使用的物理块设备以及卷组的名称 $ pvdisplay |grep 'VG Name' PV Name /dev/xvdf VG Name docker 扩展卷组。 $ vgextend docker /dev/xvdg Physical volume \"/dev/xvdg\" successfully created. Volume group \"docker\" successfully extended 扩展逻辑卷 docker/thinpool。 $ lvextend -l+100%FREE -n docker/thinpool Size of logical volume docker/thinpool_tdata changed from 95.00 GiB (24319 extents) to 198.00 GiB (50688 extents). Logical volume docker/thinpool_tdata successfully resized. 该命令使用了存储卷的全部空间，没有配置自动扩展。如果要扩展 metadata 精简池，请使用 docker/thinpool_tmeta 替换 docker/thinpool。 验证新的 thin pool 的大小。 $ docker info ...... Storage Driver: devicemapper Pool Name: docker-thinpool Pool Blocksize: 524.3 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: Metadata file: Data Space Used: 212.3 MB Data Space Total: 212.6 GB Data Space Available: 212.4 GB Metadata Space Used: 286.7 kB Metadata Space Total: 1.07 GB Metadata Space Available: 1.069 GB 通过 Data Space Available 字段的值查看 thin pool 的大小。 重启操作系统后重新激活 devicemapper 如果重启系统后发现 docker 服务启动失败，你会看到像 “Non existing device” 这样的报错信息。这时需要重新激活逻辑卷。 $ lvchange -ay docker/thinpool 4. devicemapper 存储驱动的工作原理 注意：不要直接操作 /var/lib/docker/ 中的任何文件或目录，这些文件和目录由 docker 自动管理。 查看设备和存储池： $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk └─xvda1 202:1 0 8G 0 part / xvdf 202:80 0 100G 0 disk ├─docker-thinpool_tmeta 253:0 0 1020M 0 lvm │ └─docker-thinpool 253:2 0 95G 0 lvm └─docker-thinpool_tdata 253:1 0 95G 0 lvm └─docker-thinpool 253:2 0 95G 0 lvm 查看 docker 正在使用的挂载点： $ mount |grep devicemapper /dev/xvda1 on /var/lib/docker/devicemapper type xfs (rw,relatime,seclabel,attr2,inode64,noquota) 使用 devicemapper 后，Docker 将镜像和层级内容存储在 thin pool 中，并将它们挂载到 /var/lib/docker/devicemapper/ 目录中暴露给容器使用。 4.1 磁盘上的镜像和容器层 /var/lib/docker/devicemapper/metadata/ 目录中包含了有关 devicemapper 配置本身的元数据，以及卷、快照和每个卷的块或者快照同存储池中块的映射信息。devicemapper 使用了快照技术，元数据中也包含了这些快照的信息，以 json 格式保存在文本中。 /var/lib/devicemapper/mnt/ 目录包含了所有镜像和容器层的挂载点。镜像层的挂载点表现为空目录，容器层的挂载点显示的是容器内部的文件系统。 4.2 镜像分层与共享 devicemapper 存储驱动使用专用块设备而不是格式化的文件系统，通过在块级别上对文件进行操作，能够在写时复制（CoW）期间实现最佳性能。 devicemapper 驱动将所有的镜像和容器存储到 /var/lib/docker/devicemapper/ 目录，该目录由一个或多个块级设备、环回设备（仅测试）或物理硬盘组成。 使用 devicemapper 创建一个镜像的过程如下： devicemapper 存储驱动创建一个精简池(thin pool)。这个池是从块设备或循环挂载的文件。 下一步是创建一个 base 设备。一个 base 设备是具有文件系统的精简设备。你可以通过运行 docker info 命令检查 Backing filesystem 来查看使用的是哪个文件系统。 每一个新镜像(和镜像数据层)是这个 base 设备的一个快照。这些是精简置备写时拷贝快照。这意味着它们初始为空，只在往它们写入数据时才消耗池中的空间。 使用 devicemapper 驱动时，容器数据层是从其创建的镜像的快照。与镜像一样，容器快照是精简置备写时拷贝快照。容器快照存储着容器的所有更改。当数据写入容器时，devicemapper 从存储池按需分配空间。 下图显示一个具有一个base设备和两个镜像的精简池。 如果你仔细查看图表你会发现快照一个连着一个。每一个镜像数据层是它下面数据层的一个快照。每个镜像的最底端数据层是存储池中 base 设备的快照。此 base 设备是 Device Mapper 的工件，而不是 Docker 镜像数据层。 一个容器是从其创建的镜像的一个快照。下图显示两个容器： 一个基于 Ubuntu 镜像和另一个基于 Busybox 镜像。 5. devicemapper 读写数据的过程 5.1 读数据 我们来看下使用 devicemapper 存储驱动如何进行读文件。下图显示在示例容器中读取一个单独的块 [0x44f] 的过程。 一个应用程序请求读取容器中 0x44f 数据块。由于容器是一个镜像的一个精简快照，它没有那个数据，只有一个指向镜像存储的地方的指针。 存储驱动根据指针，到镜像快照的 a005e 镜像层寻找 0xf33 块区。 devicemapper 从镜像快照复制数据块 0xf33 的内容到容器内存中。 存储驱动最后将数据返回给请求的应用。 5.2 写数据 写入新数据 : 使用 devicemapper 驱动，通过按需分配（allocate-on-demand）操作来实现写入新数据到容器，所有的新数据都被写入容器的可写层中。 例如要写入 56KB 的新数据到容器： 一个应用程序请求写入56KB的新数据到容器。 按需分配操作给容器快照分配一个新的64KB数据块。如果写操作大于64KB，就分配多个新数据块给容器快照。 新的数据写入到新分配的数据块。 覆盖存在的数据 : 更新存在的数据使用写时拷贝（copy-on-write）操作，先从最近的镜像层中读取与该文件相关的数据块；然后分配新的空白数据块给容器快照并复制数据到这些数据块；最后更新好的数据写入到新分配的数据块。 删除数据 : 当从容器的可写层中删除文件或目录时，或者从镜像层中删除其父层镜像中已存在的文件时，devicemapper 存储驱动会截获对该文件或目录的进一步读取尝试，并响应该文件或目录不存在。 写入新数据并删除旧数据 : 当你向容器中写入新数据并删除旧数据时，所有这些操作都发生在容器的可写层。如果你使用的是 direct-lvm 模式，删除的数据块将会被释放；如果你使用的是 loop-lvm 模式，那么这些数据块就不会被释放。因此不建议在生产环境中使用 loop-lvm 模式。 6. Device Mapper 对 Docker 性能的影响 了解按需分配和写时拷贝操作对整体容器性能的影响很重要。 6.1 按需分配对性能的影响 devicemapper 存储驱动通过按需分配操作给容器分配新的数据块。这意味着每次应用程序写入容器内的某处时，一个或多个空数据块从存储池中分配并映射到容器中。 所有数据块为 64KB。 写小于 64KB 的数据仍然分配一个 64KB 数据块。写入超过 64KB 的数据分配多个 64KB 数据块。所以，特别是当发生很多小的写操作时，就会比较影响容器的性能。不过一旦数据块分配给容器，后续的读和写可以直接在该数据块上操作。 6.2 写时拷贝对性能的影响 每当容器首次更新现有数据时，devicemapper 存储驱动必须执行写时拷贝操作。这会从镜像快照复制数据到容器快照。此过程对容器性能产生显着影响。因此，更新一个 1GB 文件的 32KB 数据只复制一个 64KB 数据块到容器快照。这比在文件级别操作需要复制整个 1GB 文件到容器数据层有明显的性能优势。 不过在实践中，当容器执行很多小于 64KB 的写操作时，devicemapper 的性能会比 AUFS 要差。 6.3 其他注意事项 还有其他一些影响 devicemapper 存储驱动性能的因素。 模式 Docker 使用的 devicemapper 存储驱动的默认模式是 loop-lvm。这个模式使用空闲文件来构建存储池，性能非常低。不建议用到生产环境。推荐用在生产环境的模式是 direct-lvm。 存取速度 如果希望获得更佳的性能，可以将数据文件和元数据文件放在 SSD 这样的高速存储上。 内存使用 devicemapper 并不是一个有效使用内存的存储驱动。当一个容器运行 n 个时，它的文件也会被拷贝 n 份到内存中，这对 docker 宿主机的内存使用会造成明显影响。因此，不建议在 PaaS 或者资源密集场合使用。 对于写操作较大的，可以采用挂载 data volumes。使用 data volumes 可以绕过存储驱动，从而避免 thin provisioning 和 copy-on-write 引入的额外开销。 参考： Device Mapper基础教程：Docker 中使用 devicemapper 存储驱动 Docker存储驱动devicemapper配置 docker 配置 direct-lvm Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Storage/docker_storage_3_docker_thin_provisioning.html":{"url":"Docker/Storage/docker_storage_3_docker_thin_provisioning.html","title":"docker thin provisioning 实践","keywords":"","body":"docker Thin Provisioning 实践1. Thin Provisioning Snapshot 演示2. Docker devicemapperdocker Thin Provisioning 实践 1. Thin Provisioning Snapshot 演示 上一篇我们介绍了 Device Mapper 框架的技术原理及其核心概念，下面，我们用一系列的命令来演示一下 Device Mapper 的 Thin Provisioning Snapshot 是怎么玩的。 首先，我们需要先建两个文件，一个是data.img，一个是meta.data.img： $ dd if=/dev/zero of=/tmp/data.img bs=1K count=1 seek=10M 1+0 records in 1+0 records out 1024 bytes (1.0 kB) copied, 0.000621428 s, 1.6 MB/s $ dd if=/dev/zero of=/tmp/meta.data.img bs=1K count=1 seek=1G 1+0 records in 1+0 records out 1024 bytes (1.0 kB) copied, 0.000140858 s, 7.3 MB/s 注意命令中 seek 选项，其表示为略过 of 选项指定的输出文件的前 10G 个 output 的 bloksize 的空间后再写入内容。 因为 bs 是 1 个字节，所以也就是 10G 的尺寸，但其实在硬盘上是没有占有空间的，占有空间只有 1k 的内容。当向其写入内容时，才会在硬盘上为其分配空间。 我们可以用 ls 命令看一下，实际分配了 12K 和 4K。 $ ls -lsh /tmp/data.img 12K -rw-r--r--. 1 root root 11G Aug 25 23:01 /tmp/data.img $ ls -slh /tmp/meta.data.img 4.0K -rw-r--r--. 1 root root 101M Aug 25 23:17 /tmp/meta.data.img 然后，我们为这个文件创建一个 loopback 设备。（loop2015 和 loop2016 是我乱取的两个名字） $ losetup /dev/loop2015 /tmp/data.img $ losetup /dev/loop2016 /tmp/meta.data.img $ losetup -a /dev/loop2015: [64768]:103991768 (/tmp/data.img) /dev/loop2016: [64768]:103991765 (/tmp/meta.data.img) 现在，我们为这个设备建一个 Thin Provisioning 的 Pool，用 dmsetup 命令： $ dmsetup create hchen-thin-pool \\ --table \"0 20971522 thin-pool /dev/loop2016 /dev/loop2015 \\ 128 65536 1 skip_block_zeroing\" 其中的参数解释如下（更多信息可参看 Thin Provisioning 的 man page）: dmsetup create 是用来创建 thin pool 的命令 hchen-thin-pool 是自定义的一个 pool 名，不冲突就好。 –-table 是这个 pool 的参数设置 0 代表起的 sector 位置 20971522 代表结尾的 sector 号，前面说过，一个 sector 是 512 字节，所以，20971522 个正好是 10GB /dev/loop2016 是 meta 文件的设备（前面我们建好了） /dev/loop2015 是 data 文件的设备 128 是最小的可分配的 sector 数 65536 是最少可用 sector 的 water mark，也就是一个 threshold 1 代表有一个附加参数 skip_block_zeroing 是个附加参数，表示略过用 0 填充的块 然后，我们就可以看到一个 Device Mapper 的设备了： 接下来，我们的初始还没有完成，还要创建一个 Thin Provisioning 的 Volume： $ dmsetup message /dev/mapper/hchen-thin-pool 0 \"create_thin 0\" $ dmsetup create hchen-thin-volumn-001 \\ --table \"0 2097152 thin /dev/mapper/hchen-thin-pool 0\" 其中： 第一个命令中的 create_thin 是关键字，后面的 0 表示这个 Volume 的 device 的 id。 第二个命令，是真正的为这个 Volumn 创建一个可以 mount 的设备，名字叫 hchen-thin-volumn-001。2097152 只有 1GB。 好了，在 mount 前，我们还要格式化一下： $ mkfs.ext4 /dev/mapper/hchen-thin-volumn-001 mke2fs 1.42.9 (28-Dec-2013) Discarding device blocks: done Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=16 blocks, Stripe width=16 blocks 65536 inodes, 262144 blocks 13107 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=268435456 8 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done 好了，我们可以 mount 了（下面的命令中，我还创建了一个文件） $ mkdir -p /mnt/base $ mount /dev/mapper/hchen-thin-volumn-001 /mnt/base $ echo \"hello world, I am a base\" > /mnt/base/id.txt $ cat /mnt/base/id.txt hello world, I am a base 接下来，我们来看看 snapshot 怎么搞： $ dmsetup message /dev/mapper/hchen-thin-pool 0 \"create_snap 1 0\" $ dmsetup create mysnap1 \\ --table \"0 2097152 thin /dev/mapper/hchen-thin-pool 1\" $ ll /dev/mapper/mysnap1 lrwxrwxrwx. 1 root root 7 Aug 25 23:49 /dev/mapper/mysnap1 -> ../dm-5 上面的命令中： 第一条命令是向 hchen-thin-pool 发一个 create_snap 的消息，后面跟两个 id，第一个是新的 dev id，第二个是要从哪个已有的 dev id 上做 snapshot（0 这个 dev id 是我们前面就创建了了） 第二条命令是创建一个 mysnap1 的 device，并可以被 mount。 下面我们来看看： $ mkdir -p /mnt/mysnap1 $ mount /dev/mapper/mysnap1 /mnt/mysnap1 $ ll /mnt/mysnap1/ total 20 -rw-r--r--. 1 root root 25 Aug 25 23:46 id.txt drwx------. 2 root root 16384 Aug 25 23:43 lost+found $ cat /mnt/mysnap1/id.txt hello world, I am a base 我们来修改一下 /mnt/mysnap1/id.txt，并加上一个 snap1.txt 的文件： $ echo \"I am snap1\" >> /mnt/mysnap1/id.txt $ echo \"I am snap1\" > /mnt/mysnap1/snap1.txt $ cat /mnt/mysnap1/id.txt hello world, I am a base I am snap1 $ cat /mnt/mysnap1/snap1.txt I am snap1 我们再看一下 /mnt/base，你会发现没有什么变化： $ ls /mnt/base id.txt lost+found $ cat /mnt/base/id.txt hello world, I am a base 好了，我相信你看到了分层镜像的样子了。 2. Docker devicemapper $ losetup -a /dev/loop0: [64768]:38050288 (/var/lib/docker/devicemapper/devicemapper/data) /dev/loop1: [64768]:38050289 (/var/lib/docker/devicemapper/devicemapper/metadata) 其中 data 100GB，metadata 2.0GB $ ls -alsh /var/lib/docker/devicemapper/devicemapper 506M -rw-------. 1 root root 100G Sep 10 20:15 data 1.1M -rw-------. 1 root root 2.0G Sep 10 20:15 metadata 下面是相关的 thin-pool。其中，有个当一大串 hash 串的 device 是正在启动的容器： $ ll /dev/mapper/dock* lrwxrwxrwx. 1 root root 7 Aug 25 07:57 /dev/mapper/docker-253:0-104108535-pool -> ../dm-2 lrwxrwxrwx. 1 root root 7 Aug 25 11:13 /dev/mapper/docker-253:0-104108535-deefcd630a60aa5ad3e69249f58a68e717324be4258296653406ff062f605edf -> ../dm-3 我们可以看一下它的 device id（Docker 都把它们记下来了）： $ cat /var/lib/docker/devicemapper/metadata/deefcd630a60aa5ad3e69249f58a68e717324be4258296653406ff062f605edf device_id 是 24，size 是 10737418240，除以 512，就是 20971520 个 sector。 我们用这些信息来做个 snapshot 看看（注：我用了一个比较大的 dev id – 1024）： $ dmsetup message \"/dev/mapper/docker-253:0-104108535-pool\" 0 \\ \"create_snap 1024 24\" $ dmsetup create dockersnap --table \\ \"0 20971520 thin /dev/mapper/docker-253:0-104108535-pool 1024\" $ mkdir /mnt/docker $ mount /dev/mapper/dockersnap /mnt/docker/ $ ls /mnt/docker/ id lost+found rootfs $ ls /mnt/docker/rootfs/ bin dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var 我们在 docker 的容器里用 findmnt 命令也可以看到相关的 mount 的情况（因为太长，下面只是摘要）： $ findmnt TARGET SOURCE / /dev/mapper/docker-253:0-104108535-deefcd630a60[/rootfs] /etc/resolv.conf /dev/mapper/centos-root[/var/lib/docker/containers/deefcd630a60/resolv.conf] /etc/hostname /dev/mapper/centos-root[/var/lib/docker/containers/deefcd630a60/hostname] /etc/hosts /dev/mapper/centos-root[/var/lib/docker/containers/deefcd630a60/hosts] 参考： Device Mapper系列基础教程：Thin Provisioning 实践 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Storage/docker_storage_4_docker_devicemapper_scale.html":{"url":"Docker/Storage/docker_storage_4_docker_devicemapper_scale.html","title":"docker devicemapper 扩容","keywords":"","body":"docker devicemapper 扩容docker devicemapper 扩容 查看当前大小: 100G $ ls -lh /var/lib/docker/devicemapper/devicemapper/ total 82G -rw------- 1 root root 100G Dec 4 14:06 data -rw------- 1 root root 2.0G Dec 4 14:05 metadata 扩容到200G $ truncate -s 200G /var/lib/docker/devicemapper/devicemapper/data 查看扩容后磁盘文件大小(内存中大小暂未改变) $ ls -lh /var/lib/docker/devicemapper/devicemapper/ total 82G -rw------- 1 root root 200G Dec 4 14:07 data -rw------- 1 root root 2.0G Dec 4 14:07 metadata reload 从命令行调用区块设备控制程序 $ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ] 100 losetup用来将loopdevice与档案或blockdevice联结、分离.以及查询loopdevice目前的状况,如只给定loop_device的参数.则秀出loopdevice目前的状况 $ losetup -c /dev/loop0 $ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ] 200 Reload the devicemapper thin pool $ dmsetup status | grep ' thin-pool ' | awk -F ': ' {'print $1'} docker-252:0-5637144768-pool $ dmsetup table docker-252:0-5637144768-pool 0 209715200 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing $ dmsetup suspend docker-252:0-5637144768-pool $ dmsetup reload docker-252:0-5637144768-pool --table '0 419430400 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing' $ dmsetup resume docker-252:0-5637144768-pool 扩容完成查看效果(200G) $ docker info Containers: 21 Running: 20 Paused: 0 Stopped: 1 Images: 118 Server Version: 17.05.0-ce Storage Driver: devicemapper Pool Name: docker-252:0-5637144768-pool Pool Blocksize: 65.54kB Base Device Size: 64.42GB Backing Filesystem: xfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 87.43GB Data Space Total: 214.7GB Data Space Available: 127.3GB Metadata Space Used: 99.05MB Metadata Space Total: 2.147GB Metadata Space Available: 2.048GB Thin Pool Minimum Free Space: 21.47GB 参考： Use the Device Mapper storage driver Docker and the Device Mapper storage driver How Do I Change the Mode of the Docker Device Mapper? Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/Storage/docker_storage_5_nfs_volume.html":{"url":"Docker/Storage/docker_storage_5_nfs_volume.html","title":"docker nfs volume 创建与使用","keywords":"","body":"Docker NFS volume 创建与使用1. 简介2. 创建 NFS Docker 卷3. 在容器中挂载 NFS4. Docker Compose 挂载 NFS 卷Docker NFS volume 创建与使用 tagsstart 存储 tagsstop 1. 简介 Docker 卷是为Docker 容器设置持久存储的首选机制。卷是安装在容器内的主机文件系统上的现有目录。它们可以从容器和主机系统访问。 Docker 还允许用户挂载通过 NFS 远程文件共享系统共享的目录。为此目的创建的卷使用 Docker 自己的 NFS 驱动程序，无需在主机系统上挂载 NFS 目录。 2. 创建 NFS Docker 卷 创建和管理 Docker 卷的最简单方法是使用docker volume命令及其子命令。 创建 NFS Docker 卷的语法包括两个选项。 该--driver选项定义了local卷驱动程序，它接受类似于mount Linux 中的命令的选项。 --opt多次调用该选项以提供有关卷的更多详细信息。 详细信息包括： volume type write mode 远程 NFS 服务器的 IP 或 Web 地址 服务器上共享目录的路径 docker volume create --driver local \\ --opt type=nfs \\ --opt o=addr=[ip-address],rw \\ --opt device=:[path-to-directory] \\ [volume-name] 下面的示例说明了如何创建一个名为nfs-volume. 该卷包含/mnt/nfsdir位于服务器上的目录，具有rw（读/写）权限。服务器的 IP 地址是10.240.12.70。 列出可用的 Docker 卷。 docker volume ls 3. 在容器中挂载 NFS 要将 NFS 卷挂载到容器中，请nfs-common在主机系统上安装软件包。 sudo apt update sudo apt install nfs-common 注意：如果使用 YUM 或 RPM 进行包管理，则 NFS 客户端包称为nfs-utils 使用docker run 命令启动容器。在该部分中指定 NFS 卷和安装点--mount。 docker run -d -it \\ --name [container-name] \\ --mount source=[volume-name],target=[mount-point]\\ [image-name] docker inspect [container-name] docker exec -it [container-name] ls /mnt 4. Docker Compose 挂载 NFS 卷 如果您使用Docker Compose来管理您的容器，请通过在 YML 文件中定义来挂载 NFS 卷。 创建 docker-compose.yml 文件。 version: \"3.2\" services: [service-name]: image: [docker-image] ports: - \"[port]:[port]\" volumes: - type: volume source: [volume-name] target: /nfs volume: nocopy: true volumes: [volume-name]: driver_opts: type: \"nfs\" o: \"addr=[ip-address],nolock,soft,rw\" device: \":[path-to-directory]\" 注意：nolock和soft选项确保 Docker 在与 NFS 服务器的连接丢失时不会冻结( freeze) 参考： NFS Docker Volumes: How to Create and Use Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/原理/":{"url":"Docker/原理/","title":"原理","keywords":"","body":"原理一、简介1、了解 Docker 的前生 LXC2、LXC 与 docker 什么关系？3、什么是 docker4、docker 官方文档5、为什么docker越来越受欢迎6、docker 版本7、docker 和 openstack 对比8、容器在内核中支持 2 种重要技术9、了解 docker 三个重要概念10、docker 的主要用途11、docker 改变了什么二、docker 架构1、总体架构2、docker 架构 23、docker 架构 3三、docker 架构 2 各个模块的功能1、docker client2、docker daemon3、docker server4、engine5、job6、docker registry7、Graph8、driver9、libcontainer10、docker container原理 一、简介 1、了解 Docker 的前生 LXC LXC 为 Linux Container 的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。相当于 C++ 中的 NameSpace。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。 与传统虚拟化技术相比，它的优势在于： 与宿主机使用同一个内核，性能损耗小； 不需要指令级模拟； 不需要即时(Just-in-time)编译； 容器可以在 CPU 核心的本地运行指令，不需要任何专门的解释机制； 避免了准虚拟化和系统调用替换中的复杂性； 轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。 总结：Linux Container 是一种轻量级的虚拟化的手段。 Linux Container 提供了在单一可控主机节点上支持多个相互隔离的server container 同时执行的机制。Linux Container 有点像 chroot，提供了一个拥有自己进程和网络空间的虚拟环境，但又有别于虚拟机，因为 lxc 是一种操作系统层次上的资源的虚拟化。 2、LXC 与 docker 什么关系？ docker 并不是 LXC 替代品，docker 底层使用了 LXC 来实现，LXC 将 linux 进程沙盒化，使得进程之间相互隔离，并且能够课哦内阁制各进程的资源分配。 在 LXC 的基础之上，docker 提供了一系列更强大的功能。 3、什么是 docker docker 是一个开源的应用容器引擎，基于 go 语言开发并遵循了 apache2.0 协议开源。 docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 linux 服务器，也可以实现虚拟化。 容器是完全使用沙箱机制，相互之间不会有任何接口（类 iphone 的 app），并且容器开销极其低。 4、docker 官方文档 https://docs.docker.com/ 5、为什么docker越来越受欢迎 官方话语：容器化越来越受欢迎，因为容器是： 灵活：即使是最复杂的应用也可以集装箱化。 轻量级：容器利用并共享主机内核。 可互换：您可以即时部署更新和升级。 便携式：您可以在本地构建，部署到云，并在任何地方运行。 可扩展：您可以增加并自动分发容器副本。 可堆叠：您可以垂直和即时堆叠服务。 镜像和容器（contalners） 通过镜像启动一个容器，一个镜像是一个可执行的包，其中包括运行应用程序所需要的所有内容包含代码，运行时间，库、环境变量、和配置文件。容器是镜像的运行实例，当被运行时有镜像状态和用户进程，可以使用 docker ps 查看。 容器和虚拟机 容器时在 linux 上本机运行，并与其他容器共享主机的内核，它运行的一个独立的进程，不占用其他任何可执行文件的内存，非常轻量。 虚拟机运行的是一个完成的操作系统，通过虚拟机管理程序对主机资源进行虚拟访问，相比之下需要的资源更多。 6、docker 版本 Docker Community Edition（CE）社区版 Enterprise Edition(EE) 商业版 7、docker 和 openstack 对比 8、容器在内核中支持 2 种重要技术 docker 本质就是宿主机的一个进程，docker 是通过 namespace 实现资源隔离，通过 cgroup 实现资源限制，通过写时复制技术（copy-on-write）实现了高效的文件操作（类似虚拟机的磁盘比如分配 500g 并不是实际占用物理磁盘500g） 1.namespaces 名称空间 linux namespace是提供资源隔离的方案 系统可为进程分配不同的namespace 进程隔离，资源隔离 2.control Group 控制组 cgroup 的特点是： cgroup 的 api 以一个伪文件系统的实现方式，用户的程序可以通过文件系统实现 cgroup 的组件管理 cgroup 的组件管理操作单元可以细粒度到线程级别，另外用户可以创建和销毁 cgroup，从而实现资源载分配和再利用 所有资源管理的功能都以子系统的方式实现，接口统一子任务创建之初与其父任务处于同一个 cgroup 的控制组 四大功能： 资源限制：可以对任务使用的资源总额进行限制 优先级分配：通过分配的 cpu 时间片数量以及磁盘 IO 带宽大小，实际上相当于控制了任务运行优先级 资源统计：可以统计系统的资源使用量，如 cpu 时长，内存用量等 任务控制：cgroup 可以对任务执行挂起、恢复等操作9、了解 docker 三个重要概念 1.image 镜像 docker 镜像就是一个只读模板，比如，一个镜像可以包含一个完整的 centos，里面仅安装 apache 或用户的其他应用，镜像可以用来创建 docker 容器，另外 docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下周一个已经做好的镜像来直接使用 2.container 容器 docker 利用容器来运行应用，容器是从镜像创建的运行实例，它可以被启动，开始、停止、删除、每个容器都是互相隔离的，保证安全的平台，可以吧容器看做是要给简易版的 linux 环境（包括 root 用户权限、镜像空间、用户空间和网络空间等）和运行再其中的应用程序 3.repostory 仓库 仓库是集中存储镜像文件的沧桑，registry 是仓库主从服务器，实际上参考注册服务器上存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag） 仓库分为两种，公有参考，和私有仓库，最大的公开仓库是 docker Hub，存放了数量庞大的镜像供用户下载，国内的 docker pool，这里仓库的概念与 Git 类似，registry 可以理解为 github 这样的托管服务。 10、docker 的主要用途 官方就是 Bulid 、ship、run any app/any where，编译、装载、运行、任何 app/在任意地放都能运行。 就是实现了应用的封装、部署、运行的生命周期管理只要在 glibc 的环境下，都可以运行。 运维生成环境中：docker 化。 发布服务不用担心服务器的运行环境，所有的服务器都是自动分配 docker， 自动部署，自动安装，自动运行 再不用担心其他服务引擎的磁盘问题，cpu 问题，系统问题了 资源利用更出色 自动迁移，可以制作镜像，迁移使用自定义的镜像即可迁移，不会出现什么问题 管理更加方便了11、docker 改变了什么 面向产品：产品交付 面向开发：简化环境配置 面向测试：多版本测试 面向运维：环境一致性 面向架构：自动化扩容（微服务） 二、docker 架构 1、总体架构 distribution 负责与 docker registry 交互，上传洗澡镜像以及 v2 registry 有关的源数据 registry 负责 docker registry 有关的身份认证、镜像查找、镜像验证以及管理 registry mirror 等交互操作 image 负责与镜像源数据有关的存储、查找，镜像层的索引、查找以及镜像 tar 包有关的导入、导出操作 reference 负责存储本地所有镜像的 repository 和 tag 名，并维护与镜像 id 之间的映射关系 layer 模块负责与镜像层和容器层源数据有关的增删改查，并负责将镜像层的增删改查映射到实际存储镜像层文件的 graphdriver 模块 graghdriver 是所有与容器镜像相关操作的执行者 2、docker 架构 2 如果觉得上面架构图比较乱可以看这个架构： 从上图不难看出，用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求给后者。 而 Docker Daemon 作为 Docker 架构中的主体部分，首先提供 Server 的功能使其可以接受 Docker Client 的请求；而后 Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。 Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 graphdriver 将下载镜像以 Graph 的形式存储；当需要为 Docker 创建网络环境时，通过网络管理驱动 networkdriver 创建并配置 Docker 容器网络环境；当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 execdriver 来完成。 而 libcontainer 是一项独立的容器管理包，networkdriver 以及 execdriver 都是通过 libcontainer 来实现具体对容器进行的操作。当执行完运行容器的命令后，一个实际的 Docker 容器就处于运行状态，该容器拥有独立的文件系统，独立并且安全的运行环境等。 3、docker 架构 3 再来看看另外一个架构，这个个架构就简单清晰指明了 server/client 交互，容器和镜像、数据之间的一些联系。 这个架构图更加清晰了架构 docker daemon 就是 docker 的守护进程即 server 端，可以是远程的，也可以是本地的，这个不是 C/S 架构吗，客户端 Docker client 是通过 rest api 进行通信。 docker cli 用来管理容器和镜像，客户端提供一个只读镜像，然后通过镜像可以创建多个容器，这些容器可以只是一个 RFS（Root file system根文件系统），也可以是一个包含了用户应用的 RFS，容器再 docker client 中只是要给进程，两个进程之间互不可见。 用户不能与 server 直接交互，但可以通过与容器这个桥梁来交互，由于是操作系统级别的虚拟技术，中间的损耗几乎可以不计。 三、docker 架构 2 各个模块的功能 主要的模块有：Docker Client、Docker Daemon、Docker Registry、Graph、Driver、libcontainer 以及 Docker container。 1、docker client docker client 是 docker 架构中用户用来和 docker daemon 建立通信的客户端，用户使用的可执行文件为 docker，通过 docker 命令行工具可以发起众多管理 container 的请求。 docker client 可以通过一下三种方式和 docker daemon 建立通信： tcp://host:port unix:path_to_socket fd://socketfd。 docker client 可以通过设置命令行 flag 参数的形式设置安全传输层协议(TLS)的有关参数，保证传输的安全性。 docker client 发送容器管理请求后，由 docker daemon 接受并处理请求，当 docker client 接收到返回的请求相应并简单处理后，docker client 一次完整的生命周期就结束了，当需要继续发送容器管理请求时，用户必须再次通过 docker 可以执行文件创建 docker client。 2、docker daemon docker daemon 是 docker 架构中一个常驻在后台的系统进程，功能是：接收处理 docker client 发送的请求。该守护进程在后台启动一个 server，server 负载接受 docker client 发送的请求；接受请求后，server 通过路由与分发调度，找到相应的 handler 来执行请求。 docker daemon 启动所使用的可执行文件也为 docker，与 docker client 启动所使用的可执行文件 docker 相同，在 docker 命令执行时，通过传入的参数来判别 docker daemon 与 docker client。 docker daemon 的架构可以分为：docker server、engine、job。 3、docker server docker server 在 docker 架构中时专门服务于 docker client 的 server，该 server 的功能时：接受并调度分发 docker client 发送的请求，架构图如下： 在 Docker 的启动过程中，通过包 gorilla/mux（golang 的类库解析），创建了一个 mux.Router，提供请求的路由功能。在 Golang 中，gorilla/mux 是一个强大的 URL 路由器以及调度分发器。该 mux.Router 中添加了众多的路由项，每一个路由项由HTTP请求方法（PUT、POST、GET或DELETE）、URL、Handler 三部分组成。 若 Docker Client 通过 HTTP 的形式访问 Docker Daemon，创建完 mux.Router 之后，Docker 将 Server 的监听地址以及 mux.Router 作为参数，创建一个 httpSrv=http.Server{}，最终执行 httpSrv.Serve() 为请求服务。 在 Server 的服务过程中，Server 在 listener 上接受 Docker Client 的访问请求，并创建一个全新的 goroutine 来服务该请求。在 goroutine 中，首先读取请求内容，然后做解析工作，接着找到相应的路由项，随后调用相应的 Handler 来处理该请求，最后 Handler 处理完请求之后回复该请求。 需要注意的是：Docker Server 的运行在 Docker 的启动过程中，是靠一个名为 serveapi 的 job 的运行来完成的。原则上，Docker Server 的运行是众多 job 中的一个，但是为了强调 Docker Server 的重要性以及为后续 job 服务的重要特性，将该 serveapi 的 job 单独抽离出来分析，理解为 Docker Server。 4、engine Engine 是 Docker 架构中的运行引擎，同时也 Docker 运行的核心模块。它扮演 Docker container 存储仓库的角色，并且通过执行 job 的方式来操纵管理这些容器。 在 Engine 数据结构的设计与实现过程中，有一个 handler 对象。该 handler 对象存储的都是关于众多特定 job 的 handler 处理访问。举例说明，Engine 的 handler 对象中有一项为：{“create”: daemon.ContainerCreate}，则说明当名为 create 的 job 在运行时，执行的是 daemon.ContainerCreate 的 handler 。 5、job 一个 Job 可以认为是 Docker 架构中 Engine 内部最基本的工作执行单元。Docker 可以做的每一项工作，都可以抽象为一个 job。例如：在容器内部运行一个进程，这是一个 job；创建一个新的容器，这是一个 job，从 Internet 上下载一个文档，这是一个 job；包括之前在 Docker Server 部分说过的，创建 Server 服务于 HTTP 的 API，这也是一个 job，等等。 Job 的设计者，把 Job 设计得与 Unix 进程相仿。比如说：Job 有一个名称，有参数，有环境变量，有标准的输入输出，有错误处理，有返回状态等。 6、docker registry Docker Registry 是一个存储容器镜像的仓库。而容器镜像是在容器被创建时，被加载用来初始化容器的文件架构与目录。 在 Docker 的运行过程中，Docker Daemon 会与 Docker Registry 通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的 job 名称分别为 search ， pull 与 push 。 其中，在 Docker 架构中，Docker 可以使用公有的 Docker Registry ，即大家熟知的 Docker Hub，如此一来，Docker 获取容器镜像文件时，必须通过互联网访问 Docker Hub；同时 Docker 也允许用户构建本地私有的 Docker Registry，这样可以保证容器镜像的获取在内网完成。 7、Graph Graph 在 Docker 架构中扮演已下载容器镜像的保管者，以及已下载容器镜像之间关系的记录者。一方面，Graph 存储着本地具有版本信息的文件系统镜像，另一方面也通过 GraphDB 记录着所有文件系统镜像彼此之间的关系。 Graph 的架构如下： 其中，GraphDB 是一个构建在 SQLite 之上的小型图数据库，实现了节点的命名以及节点之间关联关系的记录。它仅仅实现了大多数图数据库所拥有的一个小的子集，但是提供了简单的接口表示节点之间的关系。 同时在 Graph 的本地目录中，关于每一个的容器镜像，具体存储的信息有：该容器镜像的元数据，容器镜像的大小信息，以及该容器镜像所代表的具体 rootfs。 8、driver Driver 是 Docker 架构中的驱动模块。通过 Driver 驱动，Docker 可以实现对 Docker 容器执行环境的定制。由于 Docker 运行的生命周期中，并非用户所有的操作都是针对 Docker 容器的管理，另外还有关于 Docker 运行信息的获取，Graph 的存储与记录等。因此，为了将 Docker 容器的管理从 Docker Daemon 内部业务逻辑中区分开来，设计了 Driver 层驱动来接管所有这部分请求。 在 Docker Driver 的实现中，可以分为以下三类驱动： graphdriver networkdriver execdriver graphdriver 主要用于完成容器镜像的管理，包括存储与获取。即当用户需要下载指定的容器镜像时，graphdriver 将容器镜像存储在本地的指定目录；同时当用户需要使用指定的容器镜像来创建容器的 rootfs 时，graphdriver 从本地镜像存储目录中获取指定的容器镜像。 在 graphdriver 的初始化过程之前，有 4 种文件系统或类文件系统在其内部注册，它们分别是 aufs、btrfs、vfs 和 devmapper。而 Docker 在初始化之时，通过获取系统环境变量 DOCKER_DRIVER 来提取所使用 driver 的指定类型。而之后所有的 graph 操作，都使用该 driver 来执行。 graphdriver 的架构如下： networkdriver 的用途是完成 Docker 容器网络环境的配置，其中包括 Docker 启动时为 Docker 环境创建网桥；Docker 容器创建时为其创建专属虚拟网卡设备；以及为 Docker 容器分配 IP、端口并与宿主机做端口映射，设置容器防火墙策略等。networkdriver 的架构如下： execdriver 作为 Docker 容器的执行驱动，负责创建容器运行命名空间，负责容器资源使用的统计与限制，负责容器内部进程的真正运行等。在 execdriver 的实现过程中，原先可以使用 LXC 驱动调用 LXC 的接口，来操纵容器的配置以及生命周期，而现在 execdriver 默认使用 native 驱动，不依赖于 LXC。 具体体现在 Daemon 启动过程中加载的 ExecDriverflag 参数，该参数在配置文件已经被设为 native 。这可以认为是 Docker 在 1.2 版本上一个很大的改变，或者说 Docker 实现跨平台的一个先兆。execdriver 架构如下： 9、libcontainer libcontainer 是 Docker 架构中一个使用 Go 语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的 API。 正是由于 libcontainer 的存在，Docker 可以直接调用 libcontainer，而最终操纵容器的 namespace、cgroups、apparmor、网络设备以及防火墙规则等。这一系列操作的完成都不需要依赖LXC或者其他包。libcontainer架构如下 另外，libcontainer 提供了一整套标准的接口来满足上层对容器管理的需求。或者说，libcontainer 屏蔽了 Docker 上层对容器的直接管理。又由于 libcontainer 使用 Go 这种跨平台的语言开发实现，且本身又可以被上层多种不同的编程语言访问，因此很难说，未来的 Docker 就一定会紧紧地和 Linux 捆绑在一起。而于此同时，Microsoft 在其著名云计算平台 Azure 中，也添加了对 Docker 的支持，可见 Docker 的开放程度与业界的火热度。 暂不谈 Docker，由于 libcontainer 的功能以及其本身与系统的松耦合特性，很有可能会在其他以容器为原型的平台出现，同时也很有可能催生出云计算领域全新的项目。 10、docker container Docker container（Docker容器）是 Docker 架构中服务交付的最终体现形式。 Docker 按照用户的需求与指令，订制相应的 Docker 容器： 用户通过指定容器镜像，使得Docker容器可以自定义 rootfs 等文件系统； 用户通过指定计算资源的配额，使得 Docker 容器使用指定的计算资源； 用户通过配置网络及其安全策略，使得 Docker 容器拥有独立且安全的网络环境； 用户通过指定运行的命令，使得 Docker 容器执行指定的工作。 参考： docker万字详解 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Docker/原理/docker_Container_technology_overview.html":{"url":"Docker/原理/docker_Container_technology_overview.html","title":"容器技术概述","keywords":"","body":"容器技术概述容器技术概述 软件应用程序通常依赖于运行时环境提供的其他库、配置文件或服务。软件应用程序的传统运行环境是物理主机或虚拟机，应用程序依赖项作为主机的一部分安装。 例如，考虑一个 Python 应用程序，它需要访问实现 TLS 协议的公共共享库。传统上，系统管理员会在安装 Python 应用程序之前安装提供共享库的所需包。 传统部署的软件应用程序的主要缺点是应用程序的依赖项与运行时环境密切相关。 当对基本操作系统 (OS) 应用任何更新或补丁时，应用程序可能会中断。 例如，对 TLS 共享库的操作系统更新删除了 TLS 1.0 作为受支持的协议。这会破坏已部署的 Python 应用程序，因为它被编写为使用 TLS 1.0 协议进行网络请求。这迫使系统管理员回滚操作系统更新以保持应用程序运行，从而阻止其他应用程序使用更新包的好处。 因此，开发传统软件应用程序的公司可能需要进行全套测试，以保证操作系统更新不会影响主机上运行的应用程序。 此外，传统部署的应用程序必须在更新关联的依赖项之前停止。为了最大限度地减少应用程序停机时间，组织设计和实施复杂的系统以提供其应用程序的高可用性。在单个主机上维护多个应用程序通常变得很麻烦，并且任何部署或更新都有可能破坏组织的应用程序之一。 或者，可以使用容器部署软件应用程序。 容器是一组独立于系统其余部分的一个或多个进程。 容器提供了许多与虚拟机相同的好处，例如安全性、存储和网络隔离。容器需要的硬件资源要少得多，而且启动和终止都很快。它们还隔离了应用程序的库和运行时资源（例如 CPU 和存储），以最大限度地减少任何操作系统更新对主机操作系统的影响，如图1.1：容器与操作系统的差异中所述。 容器的使用不仅有助于提高托管应用程序的效率、弹性和可重用性，还有助于提高应用程序的可移植性。Open Container Initiative (OCI) 提供了一组定义容器运行时规范和容器映像规范的行业标准。镜像规范定义了构成容器镜像的文件和元数据包的格式。当您将应用程序构建为符合 OCI 标准的容器镜像时，您可以使用任何符合 OCI 标准的容器引擎来执行该应用程序。 有许多容器引擎可用于管理和执行单个容器，包括 Rocket、Drawbridge、LXC、Docker 和 Podman。Podman 在 Red Hat Enterprise Linux 7.6 及更高版本中可用，并在本课程中用于启动、管理和终止单个容器。 以下是使用容器的其他主要优势： 低硬件占用空间 容器使用 OS 内部功能来创建一个隔离的环境，在该环境中，使用命名空间和 cgroup（控制组）等 OS 设施管理资源。与虚拟机管理程序相比，这种方法最大限度地减少了 CPU 和内存开销。在 VM 中运行应用程序是一种与运行环境隔离的方法，但它需要大量的服务来支持容器提供的相同的低硬件占用空间隔离。 环境隔离 容器在封闭环境中工作，在该环境中对主机操作系统或其他应用程序所做的更改不会影响容器。由于容器所需的库是独立的，因此应用程序可以无中断地运行。例如，每个应用程序都可以存在于具有自己的库集的自己的容器中。对一个容器所做的更新不会影响其他容器。 快速部署 容器部署很快，因为不需要安装整个底层操作系统。通常，为了支持隔离，需要在物理主机或 VM 上安装新的操作系统，任何简单的更新都可能需要完全重启操作系统。容器重启不需要停止主机操作系统上的任何服务。 多环境部署 在使用单个主机的传统部署场景中，任何环境差异都可能破坏应用程序。但是，使用容器时，所有应用程序依赖项和环境设置都封装在容器映像中。 可重用性 无需设置完整的操作系统即可重复使用同一容器。例如，每个开发人员都可以使用提供生产数据库服务的同一数据库容器在应用程序开发期间创建开发数据库。使用容器，不再需要维护单独的生产和开发数据库服务器。单个容器映像用于创建数据库服务的实例。 通常，软件应用程序及其所有相关服务（数据库、消息传递、文件系统）都在单个容器中运行。这可能会导致与传统软件部署到虚拟机或物理主机相关的相同问题。在这些情况下，多容器部署可能更合适。 此外，容器是使用微服务进行应用程序开发的理想方法。每个服务都封装在一个轻量级且可靠的容器环境中，可以部署到生产或开发环境中。应用程序所需的容器化服务集合可以托管在一台机器上，无需为每个服务管理一台机器。 相比之下，许多应用程序不太适合容器化环境。例如，由于容器限制，访问内存、文件系统和设备等低级硬件信息的应用程序可能不可靠。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 13:40:47 "},"Docker/原理/Container_architecture_overview.html":{"url":"Docker/原理/Container_architecture_overview.html","title":"容器架构概述","keywords":"","body":"容器架构概述1. 介绍容器历史2. 描述 Linux 容器架构3. Podman 管理容器容器架构概述 1. 介绍容器历史 近年来，容器迅速流行起来。然而，容器背后的技术已经存在了相对较长的时间。2001年，Linux引入了一个名为VServer的项目。VServer 是第一次尝试在高度隔离的单个服务器内运行完整的进程集。 从 VServer 开始，隔离进程的想法进一步发展，并围绕 Linux 内核的以下功能正式化： Namespaces 命名空间隔离通常对所有进程可见的特定系统资源。在命名空间内，只有属于该命名空间成员的进程才能看到这些资源。命名空间可以包括网络接口、进程 ID 列表、挂载点、IPC 资源和系统的主机名信息等资源。 Control groups (cgroups) 控制组将进程集及其子进程划分为组，以管理和限制它们消耗的资源。控制组对进程可能使用的系统资源量进行限制。这些限制使一个进程不会在主机上使用太多资源。 Seccomp Seccomp 于 2005 年开发并于 2014 年左右引入容器，它限制了进程使用系统调用的方式。Seccomp 为进程定义了一个安全配置文件，其中列出了允许它们使用的系统调用、参数和文件描述符。 SELinux Security-Enhanced Linux (SELinux) 是进程的强制访问控制系统。Linux 内核使用 SELinux 来保护进程免受彼此的影响，并保护主机系统免受其正在运行的进程的影响。进程作为受限的 SELinux 类型运行，对主机系统资源的访问权限有限。 所有这些创新和功能都集中在一个基本概念上：使进程能够隔离运行，同时仍然访问系统资源。这个概念是容器技术的基础，也是所有容器实现的基础。如今，容器是 Linux 内核中的进程，利用这些安全功能来创建一个隔离的环境。该环境禁止隔离进程滥用系统或其他容器资源。 容器的一个常见用例是在同一主机中拥有同一服务（例如，数据库服务器）的多个副本。每个副本都有独立的资源（文件系统、端口、内存），因此服务不需要处理资源共享。隔离保证故障或有害服务不会影响同一主机或底层系统中的其他服务或容器。 2. 描述 Linux 容器架构 从 Linux 内核的角度来看，容器是一个有限制的进程。但是，容器运行的不是单个二进制文件，而是一个镜像。映像是一个文件系统包，其中包含执行进程所需的所有依赖项：文件系统中的文件、已安装的包、可用资源、正在运行的进程和内核模块。 就像可执行文件是运行进程的基础一样，镜像是运行容器的基础。运行的容器使用镜像的不可变视图，允许多个容器同时重用同一个镜像。由于镜像是文件，它们可以通过版本控制系统进行管理，从而提高容器和镜像供应的自动化程度。 容器镜像需要在本地可用，以便容器运行时执行它们，但镜像通常存储和维护在镜像存储库中。镜像存储库只是一种服务（公共或私有），可以在其中存储、搜索和检索镜像。镜像存储库提供的其他功能包括远程访问、镜像元数据、授权或镜像版本控制。 有许多不同的镜像存储库可用，每一个都提供不同的功能： Red Hat Container Catalog Red Hat Quay Docker Hub Google Container Registry Amazon Elastic Container Registry 3. Podman 管理容器 容器、镜像和镜像注册表需要能够相互交互。例如，您需要能够构建镜像并将它们放入镜像注册表中。您还需要能够从镜像注册表中检索镜像并从该镜像构建容器。 Podman 是一个开源工具，用于管理容器、容器镜像以及与镜像注册表交互。它提供以下主要功能： 它使用开放容器倡议(OCI)指定的镜像格式。这些规范定义了一种标准的、社区驱动的、非专有的镜像格式。 Podman 将本地镜像存储在本地文件系统中。这样做可以避免不必要的客户端/服务器架构或在本地机器上运行守护进程。 Podman 遵循与 Docker CLI 相同的命令模式，因此无需学习新的工具集。 Podman 与 Kubernetes 兼容。Kubernetes 可以使用 Podman 来管理其容器。 REFERENCES Red Hat Quay Container Registry Podman site Open Container Initiative Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2023-02-22 13:42:06 "},"Docker/docker_1_start.html":{"url":"Docker/docker_1_start.html","title":"入门","keywords":"","body":"入门1. 运行redis容器2. 运行web容器3. 编排镜像4. 构建node.js镜像4.1 dockerfile基础定义4.2 npm install4.3 应用配置4.4 构建并运行镜像4.5 运行添加环境变量5. OnBuild优化Dockerfile6. 忽略文件.dockerignore6.1 Docker Ignore6.2 Docker 构建安全上下文6.3 优化构建7. 容器持久化数据8. 容器之间的交流9. Docker 网络9.1 创建网络9.2 连接网络9.3 连接两个容器9.4 创建别名9.5 断开容器连接10. 使用卷持久化存储10.1 --volumes，-v10.2 --volumes-from10.3 只读卷11. 管理日志12. 容器运行策略13. 容器元数据与标签14. 负载平衡的容器14.1 NGINX Proxy14.2 单机14.2 集群15. 编排docker-compose16. docker stats统计信息17. dockerfile多阶段构建创建优化docker镜像18. docker ps输出格式19. Docker非root特权配置入门 \"这是一个非常棒的docker学习历程。我把一个国外的docker实践入门教学进行了简略的翻译，比起国内博客学习的总结性文章，它更注重让小白在实战背景下容易理解与感悟，激发萌新自我疏理总结实战演练下的小细节。\" 1. 运行redis容器 第一个任务是识别配置为运行Redis的Docker映像的名称。使用Docker，所有容器都是基于Docker映像启动的。这些图像包含启动流程所需的所有内容;主机不需要任何配置或依赖项。 $ docker search redis NAME DESCRIPTION STARS OFFICIAL AUTOMATED redis Redis is an open source key-value store that… 9971 [OK] sameersbn/redis 83 [OK] grokzen/redis-cluster Redis cluster 3.0, 3.2, 4.0, 5.0, 6.0, 6.2 79 rediscommander/redis-commander Alpine image for redis-commander - Redis man… 66 [OK] redislabs/redisearch Redis With the RedisSearch module pre-loaded… 39 redislabs/redisinsight RedisInsight - The GUI for Redis 35 kubeguide/redis-master redis-master with \"Hello World!\" 33 redislabs/redis Clustered in-memory database engine compatib… 31 oliver006/redis_exporter Prometheus Exporter for Redis Metrics. Supp… 30 redislabs/rejson RedisJSON - Enhanced JSON data type processi… 27 arm32v7/redis Redis is an open source key-value store that… 24 redislabs/redisgraph A graph database module for Redis 16 [OK] arm64v8/redis Redis is an open source key-value store that… 15 redislabs/redismod An automated build of redismod - latest Redi… 15 [OK] redislabs/rebloom A probablistic datatypes module for Redis 14 [OK] webhippie/redis Docker image for redis 11 [OK] insready/redis-stat Docker image for the real-time Redis monitor… 10 [OK] s7anley/redis-sentinel-docker Redis Sentinel 10 [OK] redislabs/redistimeseries A time series database module for Redis 10 goodsmileduck/redis-cli redis-cli on alpine 9 [OK] centos/redis-32-centos7 Redis in-memory data structure store, used a… 5 clearlinux/redis Redis key-value data structure server with t… 3 wodby/redis Redis container image with orchestration 1 [OK] tiredofit/redis Redis Server w/ Zabbix monitoring and S6 Ove… 1 [OK] xetamus/redis-resource forked redis-resource 0 [OK] $ 使用搜索命令，Jane已经确定Redis Docker Image被称为Redis，并希望运行最新版本。因为Redis是一个数据库，Jane想在她继续工作的时候把它作为后台服务运行。 要完成这一步，在后台启动一个容器，运行一个基于官方图像的Redis实例。 Docker CLI有一个名为run的命令，它将基于Docker映像启动一个容器。结构是docker运行 默认情况下，Docker将在前台运行一个命令。要在后台运行，需要指定选项-d。 $ docker run -d redis 66a23eb0c3fd7ce1099f0eef043303eb286084a87e6047e851057d0ecc634ee0 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 66a23eb0c3fd redis \"docker-entrypoint.s…\" About a minute ago Up About a minute 6379/tcp zen_archimedes `docker inspect `命令提供了运行容器的详细信息，如IP地址等。 `docker logs `将显示容器写入标准错误或标准输出的消息。 #静态映射端口 $ docker run -d --name redisHostPort -p 6379:6379 redis:latest 00d11bf6c9217aa43646c32779a29d62d854e90ae3cfe70e72e61016db49fb7c #动态映射端口 $ docker run -d --name redisDynamic -p 6379 redis:latest 56a6612f70b1f35097da220339cc1ec4c3ae84137757e803aa59e41c61523a11 $ docker port redisDynamic 6379 0.0.0.0:32768 #已经启动了多个redis实例 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 56a6612f70b1 redis:latest \"docker-entrypoint.s…\" 58 seconds ago Up 57 seconds 0.0.0.0:32768->6379/tcp redisDynamic 00d11bf6c921 redis:latest \"docker-entrypoint.s…\" 2 minutes ago Up 2 minutes 0.0.0.0:6379->6379/tcp redisHostPort 66a23eb0c3fd redis \"docker-entrypoint.s…\" 5 minutes ago Up 5 minutes 6379/tcp zen_archimedes 缺省情况下，主机端口映射为0.0.0.0，即所有IP地址。在定义端口映射时，可以指定一个特定的IP地址，例如-p 127.0.0.1:6379:6379 默认情况下，Docker将运行可用的最新版本。如果需要一个特定的版本，它可以被指定为一个标记，例如，version 3.2将被docker run -d redis:3.2。 由于这是Jane第一次使用Redis映像，它将被下载到Docker Host机器上。 让数据持久存储 $ docker run -d --name redisMapped -v /opt/docker/data/redis:/data redis 71e3cfca3344c9eaa4102761fad59d135c3233b56e960eebd5b650d72996936e Docker允许您使用$PWD作为当前目录的占位符。 2. 运行web容器 Docker映像从一个基本映像开始。基本映像应该包括应用程序所需的平台依赖项，例如，安装JVM或CLR。 这个基本映像定义为Dockerfile中的一条指令。Docker映像是基于Dockerfile的内容构建的。Dockerfile是描述如何部署应用程序的说明列表。 在这个例子中，我们的基础图像是Nginx的Alpine版本。这提供了Linux Alpine发行版上配置的web服务器。 写一个网页 $ vim index.html Hello World 创建dockerfile FROM nginx:alpine COPY . /usr/share/nginx/html 构建镜像 $ docker build -t webserver-image:v1 . Sending build context to Docker daemon 3.072kB Step 1/2 : FROM nginx:alpine ---> 513f9a9d8748 Step 2/2 : COPY . /usr/share/nginx/html ---> ae7287f132f3 Successfully built ae7287f132f3 Successfully tagged webserver-image:v1 $ docker build -t webserver-image:v1 . Sending build context to Docker daemon 3.072kB Step 1/2 : FROM nginx:alpine ---> 513f9a9d8748 Step 2/2 : COPY . /usr/share/nginx/html ---> Using cache ---> ae7287f132f3 Successfully built ae7287f132f3 Successfully tagged webserver-image:v1 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE webserver-image v1 ae7287f132f3 15 seconds ago 22.9MB 通过镜像运行容器 $ docker run -d -p 80:80 webserver-image:v1 测试web $ curl ip Hello World 3. 编排镜像 所有Docker映像都从一个基本映像开始。基本映像是来自Docker 官方仓库用于启动容器的相同映像。除了图像名称，我们还可以包含图像标签，以指示我们想要的特定版本，默认情况下，这是最新的版本。 这些基本映像用作运行应用程序所需的附加更改的基础。例如，在这个场景中，在部署静态HTML文件之前，我们需要在系统上配置并运行NGINX。因此，我们想使用NGINX作为我们的基本映像。 Dockerfile是简单的文本文件 FROM nginx:1.11-alpine 定义了基本映像之后，我们需要运行各种命令来配置映像。有很多命令可以帮助实现这一点，主要的两个命令是COPY和RUN。 RUN 允许您像在命令提示符中那样执行任何命令，例如安装不同的应用程序包或运行构建命令。RUN的结果会持久化到映像中，因此不要在磁盘上留下任何不必要的或临时的文件，这一点很重要，因为这些文件将包含在映像中。 COPY 允许您将文件从包含Dockerfile的目录复制。 已经为您创建了一个新的index.html文件，我们想从我们的容器中提供该文件。在FROM命令后面的下一行，使用COPY命令将index.html复制到/usr/share/nginx/html目录中。 $ vim index.html Hello World FROM nginx:1.11-alpine COPY index.html /usr/share/nginx/html/index.html 将我们的文件复制到映像中并下载了所有依赖项后，您需要定义需要访问哪个端口应用程序。 使用EXPOSE 命令可以告诉Docker应该打开哪些端口，可以绑定到哪些端口。您可以在一条命令中定义多个端口，例如EXPOSE 80433或EXPOSE 7000-8000 FROM nginx:1.11-alpine COPY index.html /usr/share/nginx/html/index.html EXPOSE 80 配置好Docker映像并定义了我们想要访问的端口后，现在我们需要定义启动应用程序的命令。 Dockerfile中的CMD行定义了启动容器时要运行的默认命令。如果命令需要参数，那么建议使用一个数组，例如[\"cmd\"， \"-a\"， \"arga value\"， \"-b\"， \"argb-value\"]，这将被组合在一起，命令cmd -a\" arga value\" -b argb-value将被运行。 运行NGINX的命令为NGINX -g daemon off;将此设置为Dockerfile中的默认命令。 CMD的另一种方法是ENTRYPOINT。虽然CMD可以在容器启动时被重写，但ENTRYPOINT定义了一个命令，在容器启动时可以将参数传递给它。 在这个例子中，NGINX将是关闭-g守护进程的入口点;默认的命令。 FROM nginx:1.11-alpine COPY index.html /usr/share/nginx/html/index.html EXPOSE 80 CMD [\"nginx\", \"-g\", \"daemon off;\"] 写完Dockerfile后，你需要使用docker构建将它转换成一个映像。build命令接受一个包含Dockerfile的目录，执行步骤并将映像存储在本地Docker引擎中。如果由于错误而失败，则构建将停止。 docker build -t my-nginx-image:latest . docker images 使用来自build命令的ID结果或您分配给它的友好名称启动新构建映像的实例。 NGINX被设计为后台服务，所以你应该包含选项-d。要使web服务器可访问，使用-p 80:80将其绑定到端口80 $ docker run -d -p 80:80 my-nginx-image:latest $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 34e0b44c067f my-nginx-image:latest \"nginx -g 'daemon of…\" About a minute ago Up About a minute 0.0.0.0:80->80/tcp, 443/tcp gracious_ride $ curl -i http://ip HTTP/1.1 200 OK Server: nginx/1.11.13 Date: Tue, 28 Sep 2021 08:17:44 GMT Content-Type: text/html Content-Length: 21 Last-Modified: Tue, 28 Sep 2021 08:11:49 GMT Connection: keep-alive ETag: \"6152ce45-15\" Accept-Ranges: bytes Hello World 4. 构建node.js镜像 4.1 dockerfile基础定义 正如我们在前一个场景中所描述的，所有映像都从一个基本映像开始，理想情况下，该映像尽可能接近您所需的配置。Node.js为每个发布版本提供了带有标签的预构建映像。 Node:0.0的图像为Node:10-alpine。这是一个基于 Alpine-based的构建，比官方形象更小和更流线型。 除了基本映像，我们还需要创建应用程序运行的基本目录。使用RUN ，我们可以像从命令shell中运行一样执行命令，通过使用mkdir，我们可以创建目录. 我们可以使用WORKDIR 定义一个工作目录，以确保所有未来的命令都是从相对于我们的应用程序的目录执行的。 在单独的行上设置FROM :、RUN 和WORKDIR ，以配置用于部署应用程序的基本环境。 Dockerfile内容： FROM node:10-alpine RUN mkdir -p /src/app WORKDIR /src/app 4.2 npm install 在前面的集合中，我们配置了配置的基础以及希望如何部署应用程序。下一个阶段是安装运行应用程序所需的依赖项。对于Node.js，这意味着运行NPM install。 为了将构建时间保持在最小，Docker将在Dockerfile中缓存一行代码的执行结果，以便在将来的构建中使用。如果发生了更改，Docker将使当前行和以下所有行无效，以确保所有行都是最新的。 对于NPM，我们只希望在包中有东西时重新运行NPM install。Json文件已经改变。如果没有任何改变，那么我们可以使用缓存版本来加速部署。使用COPY包。我们可以使RUN npm install命令失效，如果包。Json文件已经改变。如果文件没有更改，那么缓存将不会失效，并且将使用npm install命令的缓存结果。 Dockerfile更新内容： FROM node:10-alpine RUN mkdir -p /src/app WORKDIR /src/app COPY package.json /src/app/package.json RUN npm install 如果你不想使用缓存作为构建的一部分，那么设置选项--no-cache=true作为docker构建命令的一部分。 4.3 应用配置 在安装了依赖项之后，我们希望复制应用程序的其余源代码。拆分依赖项的安装并复制源代码使我们能够在需要时使用缓存。 如果我们在运行npm install之前复制我们的代码，那么它每次都会运行，因为我们的代码会发生变化。通过复制只是包。Json，我们可以确保缓存是无效的，只有当我们的包内容已经改变。 在Dockerfile中创建所需的步骤，以完成应用程序的部署。 我们可以使用copy复制Dockerfile所在的整个目录。 复制源代码之后，使用EXPOSE 定义应用程序需要访问的端口。 最后，需要启动应用程序。使用Node.js的一个巧妙技巧是使用npm start命令。这看起来在包里。Json文件，了解如何启动保存重复命令的应用程序。 FROM node:10-alpine RUN mkdir -p /src/app WORKDIR /src/app COPY package.json /src/app/package.json RUN npm install COPY . /src/app EXPOSE 3000 CMD [ \"npm\", \"start\" ] 4.4 构建并运行镜像 $ docker build -t my-nodejs-app . $ docker run -d --name my-running-app -p 3000:3000 my-nodejs-app 您可以使用curl测试容器是否可访问。如果应用程序响应，那么您就知道一切都已正确启动。 $ curl http://docker:3000 ExpressExpressWelcome to Express 4.5 运行添加环境变量 Docker映像应该设计成可以从一个环境传输到另一个环境，而不需要做任何更改或重新构建。通过遵循这个模式，您可以确信，如果它在一个环境(如登台)中工作，那么它也将在另一个环境(如生产环境)中工作。 使用Docker，可以在启动容器时定义环境变量。例如，对于Node.js应用程序，您应该在生产环境中运行时为NODE_ENV定义一个环境变量。 使用-e选项，可以将名称和值设置为-e NODE_ENV=production $ docker run -d --name my-production-running-app -e NODE_ENV=production -p 3000:3000 my-nodejs-app 5. OnBuild优化Dockerfile 虽然Dockerfile是按从上到下的顺序执行的，但当该映像用作另一个映像的基础时，您可以触发一条指令在稍后的时间执行。 ONBUILD指令可以为镜像添加触发器。 当我们在一个Dockerfile文件中加上ONBUILD指令，该指令对利用该Dockerfile构建镜像（比如为A镜像）不会产生实质性影响。 但是当我们编写一个新的Dockerfile文件来基于A镜像构建一个镜像（比如为B镜像）时，这时构造A镜像的Dockerfile文件中的ONBUILD指令就生效了，在构建B镜像的过程中，首先会执行ONBUILD指令指定的指令，然后才会执行其它指令。 需要注意的是，如果是再利用B镜像构造新的镜像时，那个ONBUILD指令就无效了，也就是说只能再构建子镜像中执行，对孙子镜像构建无效。其实想想是合理的，因为在构建子镜像中已经执行了，如果孙子镜像构建还要执行，相当于重复执行，这就有问题了。 利用ONBUILD指令,实际上就是相当于创建一个模板镜像，后续可以根据该模板镜像创建特定的子镜像，需要在子镜像构建过程中执行的一些通用操作就可以在模板镜像对应的dockerfile文件中用ONBUILD指令指定。 从而减少dockerfile文件的重复内容编写。 下面是Node.js的OnBuild Dockerfile。与前面的场景不同，应用程序指定命令以ONBUILD作为前缀。 Dockerfile1: FROM node:7 RUN mkdir -p /usr/src/app WORKDIR /usr/src/app ONBUILD COPY package.json /usr/src/app/ ONBUILD RUN npm install ONBUILD COPY . /usr/src/app CMD [ \"npm\", \"start\" ] docker build -t node:7-onbuild -f Dockerfile1 结果是，我们可以构建这个映像，但在将构建的映像用作基本映像之前，不会执行应用程序特定的命令。然后，它们将作为基本映像构建的一部分执行。 有了复制代码、安装依赖项和启动应用程序的所有逻辑后，在应用程序级别上需要定义的唯一方面就是要公开哪个端口。 创建OnBuild映像的好处是，我们的Dockerfile现在更简单，可以轻松地跨多个项目重用，而不必重新运行相同的步骤，以提高构建时间。 dockerfile2: FROM node:7-onbuild EXPOSE 3000 已经为您创建了上一步中的Dockerfile。基于OnBuild docker文件构建映像与之前相同。OnBuild命令将像在基础Dockerfile中一样执行。 总之，唯有 ONBUILD 是为了帮助别人定制自己而准备的。而不是为了构建当前镜像的。 docker build -t my-nodejs-app -f dockerfile2 docker run -d --name my-running-app -p 3000:3000 my-nodejs-app curl http://ip:3000 6. 忽略文件.dockerignore 6.1 Docker Ignore 为了防止敏感文件或目录被错误地包含在映像中，您可以添加一个名为.dockerignore的文件。 Dockerfile将工作目录复制到Docker映像中。因此，这将包括潜在的敏感信息，如我们希望在映像外部管理的密码文件。 $ ls Dockerfile cmd.sh passwords.txt $ cat Dockerfile FROM alpine ADD . /app COPY cmd.sh /cmd.sh CMD [\"sh\", \"-c\", \"/cmd.sh\"] $ cat cmd.sh echo \"Hello World\" $ cat passwords.txt admin:admin $ docker build -t password . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM alpine ---> 3fd9065eaf02 Step 2/4 : ADD . /app ---> 8e7bc5dac978 Step 3/4 : COPY cmd.sh /cmd.sh ---> ec486638d561 Step 4/4 : CMD [\"sh\", \"-c\", \"/cmd.sh\"] ---> Running in fe4cba7a87b2 Removing intermediate container fe4cba7a87b2 ---> 4c270e87d27c Successfully built 4c270e87d27c Successfully tagged password:latest $ docker run password ls /app Dockerfile cmd.sh passwords.txt 这将包括密码文件。 下面的命令将在.dockerignore文件中包含password .txt，并确保它不会意外地出现在容器中。dockerignore文件将存储在源代码管理中，并与团队共享，以确保每个人都是一致的。 echo passwords.txt >> .dockerignore .dockerignore文件支持目录和正则表达式来定义限制，非常类似于.gitignore。这个文件还可以用来提高构建时间，我们将在下一步研究这个问题。 构建映像，因为Docker Ignore文件不应该包括密码文件。 $ docker build -t nopassword . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM alpine ---> 3fd9065eaf02 Step 2/4 : ADD . /app ---> 36ee8b3bc4ee Step 3/4 : COPY cmd.sh /cmd.sh ---> a4c8fc953352 Step 4/4 : CMD [\"sh\", \"-c\", \"/cmd.sh\"] ---> Running in 5b7774763eca Removing intermediate container 5b7774763eca ---> b6b6eac92cce Successfully built b6b6eac92cce Successfully tagged nopassword:latest $ docker run nopassword ls /app Dockerfile cmd.sh 如果您需要使用密码作为RUN命令的一部分，那么您需要复制、执行和删除文件作为单个RUN命令的一部分。只有Docker容器的最终状态被持久化到映像中。 6.2 Docker 构建安全上下文 dockerignore文件可以确保Docker映像中不包含敏感细节。但是，它们也可以用来提高映像的构建时间。 在环境中，已经创建了100M的临时文件。Dockerfile永远不会使用这个文件。当您执行构建命令时，Docker将整个路径内容发送给引擎，以便它计算要包含哪些文件。因此，发送100M文件是不需要的，并创建了一个较慢的构建。 您可以通过执行该命令看到100M的影响。 $ docker build -t large-file-context . Sending build context to Docker daemon 104.9MB Step 1/4 : FROM alpine ---> 3fd9065eaf02 Step 2/4 : ADD . /app ---> cb1e74c524af Step 3/4 : COPY cmd.sh /cmd.sh ---> e3dbbbd57ddf Step 4/4 : CMD [\"sh\", \"-c\", \"/cmd.sh\"] ---> Running in 5fcb5e771266 Removing intermediate container 5fcb5e771266 ---> 7e398a079fb0 Successfully built 7e398a079fb0 Successfully tagged large-file-context:latest 在下一步中，我们将演示如何提高构建的性能。 明智的做法是忽略.git目录以及在映像中下载/构建的依赖项，比如node_modules。在Docker容器中运行的应用程序永远不会使用它们，只会增加构建过程的开销。 6.3 优化构建 以同样的方式，我们使用.dockerignore文件来排除敏感文件，我们可以使用它来排除我们不想在构建期间发送到Docker构建上下文的文件。 要加快构建速度，只需在忽略文件中包含大文件的文件名。 echo big-temp-file.img >> .dockerignore 当我们重建图像时，它将会快得多，因为它不需要复制100M文件。 $ docker build -t no-large-file-context . Sending build context to Docker daemon 4.096kB Step 1/4 : FROM alpine ---> 3fd9065eaf02 Step 2/4 : ADD . /app ---> Using cache ---> 4a1be3423c29 Step 3/4 : COPY cmd.sh /cmd.sh ---> Using cache ---> e30db2162cca Step 4/4 : CMD [\"sh\", \"-c\", \"/cmd.sh\"] ---> Using cache ---> 4d4964ddbb00 Successfully built 4d4964ddbb00 Successfully tagged no-large-file-context:latest 当忽略.git这样的大目录时，这种优化会产生更大的影响。 7. 容器持久化数据 数据容器是唯一负责存储/管理数据的容器。 与其他容器一样，它们由主机系统管理。然而，当您执行docker ps命令时，它们不会运行。 要创建数据容器，我们首先要创建一个具有知名名称的容器以供将来参考。我们使用busybox作为基础，因为它体积小，重量轻，以防我们想要探索和移动容器到另一个主机。 在创建容器时，我们还提供了一个-v选项来定义其他容器读取/保存数据的位置。 $ docker create -v /config --name dataContainer busybox 容器就绪后，我们现在可以将文件从本地客户端目录复制到容器中。 下面的命令将config.conf文件复制到dataContainer和config.conf目录中。 $ docker cp config.conf dataContainer:/config/ 现在我们的Data Container有了配置，我们可以在启动需要配置文件的依赖容器时引用该容器。 使用——volumes-from 选项，我们可以使用正在启动的容器中来自其他容器的挂载卷。在这种情况下，我们将启动一个Ubuntu容器，它引用了我们的数据容器。当我们列出config目录时，它将显示来自附加容器的文件。 $ docker run --volumes-from dataContainer ubuntu ls /config config.conf 如果/config目录已经存在，那么volumes-from将被覆盖并成为所使用的目录。可以将多个卷映射到一个容器。 如果我们想将Data Container移动到另一台机器，那么我们可以将其导出到.tar文件。 $ docker export dataContainer > dataContainer.tar 命令docker import dataContainer.tar会将数据容器导入到docker中。 8. 容器之间的交流 连接到容器最常见的场景是应用程序连接到数据存储。创建链接时的关键方面是容器的名称。所有容器都有名称，但为了在处理链接时更容易一些，为所连接的源容器定义一个友好的名称是很重要的。 运行一个友好的名称为redis-server的redis服务器，我们将在下一步连接它。这将是源容器。 $ docker run -d --name redis-server redis Redis是一个快速的、开源的键值数据存储。 要连接到源容器，在启动新容器时使用--link :选项。容器名引用上一步中定义的源容器，而别名定义主机的友好名称。 通过设置别名，我们可以将应用程序的配置方式与基础设施的调用方式分开。这意味着应用程序配置在连接到其他环境时不需要更改。 在这个例子中，我们打开一个链接到redis-server的Alpine容器。我们已经将别名定义为redis。当一个链接被创建时，Docker将做两件事。 首先，Docker将基于链接到容器的环境变量设置一些环境变量。这些环境变量为您提供了一种通过已知名称引用端口和IP地址等信息的方法。 可以使用env命令输出所有环境变量。例如: $ docker run --link redis-server:redis alpine env PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=bae49bf11c01 REDIS_PORT=tcp://172.18.0.2:6379 REDIS_PORT_6379_TCP=tcp://172.18.0.2:6379 REDIS_PORT_6379_TCP_ADDR=172.18.0.2 REDIS_PORT_6379_TCP_PORT=6379 REDIS_PORT_6379_TCP_PROTO=tcp REDIS_NAME=/angry_franklin/redis REDIS_ENV_GOSU_VERSION=1.12 REDIS_ENV_REDIS_VERSION=6.2.5 REDIS_ENV_REDIS_DOWNLOAD_URL=http://download.redis.io/releases/redis-6.2.5.tar.gz REDIS_ENV_REDIS_DOWNLOAD_SHA=4b9a75709a1b74b3785e20a6c158cab94cf52298aa381eea947a678a60d551ae HOME=/root 通过创建链接，您可以以与在您的网络中运行的服务器相同的方式ping源容器。 $ docker run --link redis-server:redis alpine ping -c 1 redis PING redis (172.18.0.2): 56 data bytes 64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.202 ms --- redis ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.202/0.202/0.202 ms 通过创建链接，应用程序可以以通常的方式与源容器进行连接和通信，而无需考虑两个服务都运行在容器中这一事实。 这是一个简单的node.js应用程序，它使用主机名redis连接到redis。 $ docker run -d -p 3000:3000 --link redis-server:redis katacoda/redis-node-docker-example 发送一个HTTP请求到应用程序将存储请求在Redis和返回一个计数。如果发出多个请求，就会看到计数器的递增，因为条目被持久化了。 $ curl ip:3000 This page was generated after talking to redis. Application Build: 1 Total requests: 1 IP count: ::ffff:172.17.0.33: 1 $ curl ip:3000 This page was generated after talking to redis. Application Build: 1 Total requests: 2 IP count: ::ffff:172.17.0.33: 2 以同样的方式，您可以将源容器连接到应用程序，也可以将它们连接到自己的CLI工具。 下面的命令将启动一个redis -cli工具的实例，并通过它的别名连接到redis服务器。 $ docker run -it --link redis-server:redis redis redis-cli -h redis redis:6379> info KEYS *命令将输出当前存储在源redis容器中的内容。 9. Docker 网络 9.1 创建网络 第一步是使用CLI创建网络。这个网络将允许我们附加多个容器，这些容器将能够发现彼此。在本例中，我们将从创建一个后端网络开始。所有连接到我们后端的容器都将在这个网络上。 $ docker network create backend-network 89109e8de51aee15171ac6ec7257af040aecc66906777acfbbe88a715dcdb9d4 当我们启动新的容器时，我们可以使用--net属性来分配它们应该连接到哪个网络。 $ docker run -d --name=redis --net=backend-network redis 9.2 连接网络 与使用链接不同，docker网络的行为类似于传统网络，节点可以附加/分离。 首先你会注意到Docker不再分配环境变量或更新容器的hosts文件。使用下面的两个命令，您会注意到它不再提到其他容器。 $ docker run --net=backend-network alpine env PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=d566ff9c9a14 HOME=/root $ docker run --net=backend-network alpine cat /etc/hosts 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.19.0.3 97c3a236a7e6 相反，容器可以通过Docker中的嵌入式DNS服务器进行通信。这个DNS服务器通过IP 127.0.0.11分配给所有容器，并在resolv.conf文件中设置。 $ docker run --net=backend-network alpine cat /etc/resolv.conf nameserver 127.0.0.11 options ndots:0 当容器试图通过众所周知的名称(如Redis)访问其他容器时，DNS服务器将返回正确的容器的IP地址。在这种情况下，Redis的完全限定名将是Redis .backend-network。 $ docker run --net=backend-network alpine ping -c1 redis PING redis (172.19.0.2): 56 data bytes 64 bytes from 172.19.0.2: seq=0 ttl=64 time=0.324 ms --- redis ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.324/0.324/0.324 ms 9.3 连接两个容器 Docker支持多个网络和容器同时连接到多个网络。 例如，让我们用Node.js应用程序创建一个单独的网络，它与我们现有的Redis实例通信。 第一个任务是以同样的方式创建一个新的网络。 $ docker network create frontend-network 37e9702dd8f695f515b988beddd1cf4d4f7b38447a4e4177d98fcf96231321b2 当使用connect命令时，可以将现有容器附加到网络上。 $ docker network connect frontend-network redis 当我们启动web服务器时，考虑到它连接到同一个网络，它将能够与我们的Redis实例通信。 $ docker run -d -p 3000:3000 --net=frontend-network katacoda/redis-node-docker-example $ curl ping:3000 This page was generated after talking to redis. Application Build: 1 Total requests: 1 IP count: ::ffff:172.17.0.51: 1 $ curl ping:3000 This page was generated after talking to redis. Application Build: 1 Total requests: 2 IP count: ::ffff:172.17.0.51: 2 9.4 创建别名 使用docker网络时仍然支持链接，并提供了一种方法来定义容器名的别名。这将为容器提供一个额外的DNS条目名称和被发现的方式。当使用--link时，嵌入式DNS将保证本地化查找结果只在使用--link的容器上。 另一种方法是在将容器连接到网络时提供别名。 下面的命令将用db的别名将我们的Redis实例连接到前端网络。 docker network create frontend-network2 docker network connect --alias db frontend-network2 redis 当容器试图通过名称db访问服务时，他们将得到我们的Redis容器的IP地址。 $ docker run --net=frontend-network2 alpine ping -c1 db PING db (172.21.0.2): 56 data bytes 64 bytes from 172.21.0.2: seq=0 ttl=64 time=0.170 ms --- db ping statistics --- 1 packets transmitted, 1 packets received, 0% packet loss round-trip min/avg/max = 0.170/0.170/0.170 ms 9.5 断开容器连接 创建好网络后，我们可以使用CLI来探索细节。下面的命令将列出我们主机上的所有网络。 $ docker network ls NETWORK ID NAME DRIVER SCOPE 89109e8de51a backend-network bridge local 6fe697227a58 bridge bridge local 37e9702dd8f6 frontend-network bridge local b0a9dbbb0bab frontend-network2 bridge local fa054a9af353 host host local f50397115ef2 none null local 然后，我们可以探索网络，查看连接的容器及其IP地址。 $ docker network inspect frontend-network [ { \"Name\": \"frontend-network\", \"Id\": \"37e9702dd8f695f515b988beddd1cf4d4f7b38447a4e4177d98fcf96231321b2\", \"Created\": \"2021-09-28T12:52:03.799072129Z\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"172.20.0.0/16\", \"Gateway\": \"172.20.0.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": { \"Network\": \"\" }, \"ConfigOnly\": false, \"Containers\": { \"80bb046b3ac46cd0efa7b66640e0eaf297f65d39ad080193758f6d19d10c6d3e\": { \"Name\": \"redis\", \"EndpointID\": \"a8fb389c6672f0ab173c39b273064922ea252d3fce5094d1864fb2b36cdfa25d\", \"MacAddress\": \"02:42:ac:14:00:02\", \"IPv4Address\": \"172.20.0.2/16\", \"IPv6Address\": \"\" }, \"b28cbfb9b69e68d070f8fef5ddd2bbabb6e410ebeb0905b917dfd8eb103d85a3\": { \"Name\": \"inspiring_archimedes\", \"EndpointID\": \"80afb91170ab08def612373bf78be60609dbb173bb3d57b3abda7b578ff001cc\", \"MacAddress\": \"02:42:ac:14:00:03\", \"IPv4Address\": \"172.20.0.3/16\", \"IPv6Address\": \"\" } }, \"Options\": {}, \"Labels\": {} } ] 下面的命令断开redis容器与前端网络的连接。 $ docker network disconnect frontend-network redis 10. 使用卷持久化存储 10.1 --volumes，-v 在启动容器时创建和分配Docker卷。数据卷允许将主机目录映射到容器，以便共享数据。 这种映射是双向的。它允许从容器内部访问存储在主机上的数据。这还意味着进程在容器内保存的数据会持久化到主机上。 这个例子将使用Redis作为一种持久化数据的方法。在下面启动一个Redis容器，并使用-v参数创建一个数据卷。它指定容器中保存到/data目录中的任何数据都应该持久化到主机的/docker/redis-data目录中。 docker run -v /docker/redis-data:/data --name r1 -d redis redis-server --appendonly yes 我们可以使用下面的命令将数据输送到Redis实例中。 $ cat data | docker exec -i r1 redis-cli --pipe All data transferred. Waiting for the last reply... Last reply received from server. errors: 0, replies: 1 Redis会将这些数据保存到磁盘。在主机上，我们可以调查应该包含Redis数据文件的映射直接。 $ ls /docker/redis-data appendonly.aof 这个目录可以挂载到第二个容器。一种用法是让Docker容器对数据执行备份操作。 $ docker run -v /docker/redis-data:/backup ubuntu ls /backup appendonly.aof 10.2 --volumes-from 数据卷映射给主机有利于数据持久化。然而，要从另一个容器访问它们，您需要知道容易出错的确切路径。 另一种方法是使用-volumes-from。该参数将映射卷从源容器映射到正在启动的容器。 在这个例子中，我们将Redis容器的卷映射到Ubuntu容器。/data目录只存在于我们的Redis容器中，然而，因为-volumes-from,Ubuntu容器可以访问数据。 $ docker run --volumes-from r1 -it ubuntu ls /data appendonly.aof 这允许我们访问来自其他容器的卷，而不必关心它们是如何在主机上持久化的。 10.3 只读卷 挂载卷使容器对目录具有完全的读和写访问权限。通过对挂载目录添加“ro”权限，可以对该目录设置只读权限。如果容器试图修改目录中的数据，则会出错。 $ docker run -v /docker/redis-data:/data:ro -it ubuntu rm -rf /data rm: cannot remove '/data/appendonly.aof': Read-only file system 11. 管理日志 当你启动一个容器时，Docker将跟踪进程的Standard Out和Standard Error输出，并通过客户端使它们可用。 在后台，有一个名为Redis -server的Redis实例运行。通过使用Docker客户端，我们可以访问标准输出和标准错误输出 $ docker logs redis-server 默认情况下，Docker日志使用JSON -file记录器输出，这意味着输出存储在主机上的JSON文件中。这可能会导致大文件填满磁盘。因此，您可以更改日志驱动程序以移动到不同的目的地。 Syslog日志驱动程序将所有容器日志写到主机的中央Syslog日志中。syslog是一种广泛使用的消息记录标准。它允许生成消息的软件、存储消息的系统以及报告和分析消息的软件分离。 此日志驱动程序设计用于外部系统收集和聚合syslog日志。下面的命令将redis日志重定向到syslog $ docker run -d --name redis-syslog --log-driver=syslog redis 如果你试图使用客户端查看日志，你会收到错误FATA[0000] \"logs\"命令只支持\"json-file\"日志驱动程序。 相反，您需要通过syslog流访问它们。 第三个选项是禁用容器上的日志记录。这对于在日志记录中非常冗长的容器特别有用。 当容器启动时，只需将log-driver设置为none。不会记录任何输出。 docker run -d --name redis-none --log-driver=none redis inspect命令允许您识别特定容器的日志记录配置。下面的命令将为每个容器输出LogConfig部分。 $ docker inspect --format '{{ .HostConfig.LogConfig }}' redis-server {json-file map[]} $ docker inspect --format '{{ .HostConfig.LogConfig }}' redis-syslog {syslog map[]} $ docker inspect --format '{{ .HostConfig.LogConfig }}' redis-none {none map[]} 12. 容器运行策略 Docker认为任何带有非零退出代码的容器都崩溃了。默认情况下，崩溃的容器将保持停止状态。 我们已经创建了一个特殊的容器，它输出一条消息，然后使用代码1退出，以模拟崩溃。 docker run -d --name restart-default scrapbook/docker-restart-example 如果列出所有的容器，包括stopped，您将看到容器已经崩溃 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 39ef0a052236 scrapbook/docker-restart-example \"/bin/sh -c ./launch…\" 5 seconds ago Exited (1) 2 seconds ago restart-default 而日志将输出我们的消息，这在现实生活中可能会提供帮助我们诊断问题的信息。 $ docker logs restart-default Tue Sep 28 13:41:39 UTC 2021 Booting up... 根据您的场景，重新启动失败的进程可能会纠正这个问题。Docker可以在停止尝试之前自动重试启动Docker特定次数。 他的选项--restart=on-failure:#允许你说Docker应该重试多少次。在下面的示例中，Docker将在停止之前重新启动容器三次。 $ docker run -d --name restart-3 --restart=on-failure:3 scrapbook/docker-restart-example $ docker logs restart-3 Tue Sep 28 13:42:44 UTC 2021 Booting up... Tue Sep 28 13:42:46 UTC 2021 Booting up... Tue Sep 28 13:42:50 UTC 2021 Booting up... Tue Sep 28 13:42:53 UTC 2021 Booting up... 最后，Docker总是可以重新启动失败的容器，在这种情况下，Docker将一直尝试，直到容器被明确告知停止。 例如，当容器崩溃时，使用always标志自动重新启动容器 $ docker run -d --name restart-always --restart=always scrapbook/docker-restart-example $ docker logs restart-always 13. 容器元数据与标签 当容器通过docker运行启动时，标签可以被附加到容器上。一个容器可以在任何时候有多个标签。 注意，在这个例子中，因为我们使用的标签是用于CLI，而不是一个自动化的工具，所以我们没有使用DNS表示法格式。 要添加单个标签，可以使用l =选项。下面的示例为容器分配了一个名为user的带有ID的标签。这将允许我们查询与该特定用户相关的所有正在运行的容器。 docker run -l user=12345 -d redis 如果您正在添加多个标签，那么这些标签可以来自外部文件。文件的每一行都需要有一个标签，然后这些标签将被附加到正在运行的容器上。 这一行在文件中创建了两个标签，一个用于用户，另一个用于分配角色。 echo 'user=123461' >> labels && echo 'role=cache' >> labels The --label-file= option will create a label for each line in the file. docker run --label-file=labels -d redis 标签镜像的工作方式与容器相同，但在构建镜像时在Dockerfile中设置。当容器启动时，镜像的标签将应用到容器实例。 在一个Dockerfile中，你可以使用label指令分配一个标签。在标签下面创建名为“剪贴簿”的供应商。 LABEL vendor=Katacoda 如果我们想要分配多个标签，可以使用下面的格式，每行都有一个标签，使用反斜杠(\"\\\")连接。注意，我们使用的是与第三方工具相关的DNS标记格式。 LABEL vendor=Katacoda \\ com.katacoda.version=0.0.5 \\ com.katacoda.build-date=2016-07-01T10:47:29Z \\ com.katacoda.course=Docker 标签和元数据只有在您可以稍后查看/查询它们时才有用。查看特定容器或镜像的所有标签的第一种方法是使用docker inspect。 环境已经为您创建了一个名为rd的容器和一个名为katacoda-label-example的映像。 通过提供运行容器的友好名称或哈希id，您可以查询它的所有元数据。 docker inspect rd 使用-f选项，您可以过滤JSON响应，只针对我们感兴趣的标签部分。 $ docker inspect -f \"{{json .Config.Labels }}\" rd {\"com.katacoda.created\":\"automatically\",\"com.katacoda.private-msg\":\"magic\",\"user\":\"scrapbook\"} 检查镜像的方法是一样的，但是JSON格式略有不同，将其命名为ContainerConfig而不是Config。 $ docker inspect -f \"{{json .ContainerConfig.Labels }}\" katacoda-label-example {\"com.katacoda.build-date\":\"2015-07-01T10:47:29Z\",\"com.katacoda.course\":\"Docker\",\"com.katacoda.private-msg\":\"HelloWorld\",\"com.katacoda.version\":\"0.0.5\",\"vendor\":\"Katacoda\"} 这些标签将保留，即使镜像已被取消标记。当镜像未被标记时，它的名称将为. 虽然检查单个容器和映像可以为您提供更多的上下文，但在运行数千个容器的生产中，限制对您感兴趣的容器的响应是很有用的。 docker ps命令允许您根据标签名称和值指定一个过滤器。例如，下面的查询将返回所有具有值katacoda的用户标签键的容器。 $ docker ps --filter \"label=user=scrapbook\" CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0c0d9b5b90ed redis \"docker-entrypoint.s…\" 8 minutes ago Up 8 minutes 6379/tcp rd 基于构建镜像时使用的标签，可以对镜像应用相同的过滤方法。 $ docker images --filter \"label=vendor=Katacoda\" REPOSITORY TAG IMAGE ID CREATED SIZE katacoda-label-example latest 33a1689f8704 8 minutes ago 112MB 标签不仅适用于图像和容器，也适用于Docker Daemon本身。当您启动守护进程的实例时，可以为它分配标签，以帮助确定应该如何使用它，例如，它是开发服务器还是生产服务器，或者它是否更适合运行数据库等特定角色。 docker -d \\ -H unix:///var/run/docker.sock \\ --label com.katacoda.environment=\"production\" \\ --label com.katacoda.storage=\"ssd\" 14. 负载平衡的容器 14.1 NGINX Proxy 在这个场景中，我们希望运行一个NGINX服务，它可以在加载新容器时动态发现和更新它的负载平衡配置。我们已经创建了nginx-proxy。 Nginx-proxy接受HTTP请求，并根据请求主机名将请求代理到相应的容器。这对用户是透明的，不会产生任何额外的性能开销。 在启动代理容器时，需要配置三个键属性。 第一种方法是使用-p 80:80将容器绑定到主机上的80端口。这确保了所有HTTP请求都由代理处理。 第二步是挂载docker.sock文件。这是一个与运行在主机上的Docker守护进程的连接，允许容器通过API访问它的元数据。NGINX-proxy使用这个来监听事件，然后根据容器的IP地址更新NGINX配置。挂载文件的工作方式与使用-v /var/run/docker.sock:/tmp/docker.sock:ro的目录相同。指定:ro将访问限制为只读。 最后，我们可以设置一个可选的-e DEFAULTHOST=。如果传入的请求没有生成任何指定的主机，则该容器将处理请求。这使您能够在一台机器上运行多个具有不同域的网站，并可返回到一个已知的网站。 使用下面的命令启动nginx-proxy。 docker run -d -p 80:80 -e DEFAULT_HOST=proxy.example -v /var/run/docker.sock:/tmp/docker.sock:ro --name nginx jwilder/nginx-proxy ：因为我们使用的是DEFAULT_HOST，所以任何传入的请求都将被定向到已经分配了HOST代理的容器。 您可以使用curl http://ip向web服务器发出请求。由于我们没有容器，它将返回一个503错误。 $ curl http://ip 503 Service Temporarily Unavailable 503 Service Temporarily Unavailable nginx 14.2 单机 Nginx-proxy现在正在监听Docker在启动/停止时引发的事件。一个名为katacoda/docker-http-server的示例网站已经创建，它将返回运行它的机器名。这允许我们测试我们的代理是否按照预期工作。它的内部是一个PHP和Apache2应用程序，侦听端口80。 为了让Nginx-proxy开始向容器发送请求，你需要指定VIRTUAL_HOST环境变量。这个变量定义了请求来自的域，并且应该由容器处理。 在这个场景中，我们将把我们的HOST设置为与DEFAULT_HOST匹配，这样它将接受所有请求。 docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server 有时，NGINX需要几秒钟的时间来重新加载，但如果我们使用curl http://ip执行一个请求到我们的代理，那么请求将由我们的容器处理。 $ curl http://ip This request was processed by host: a6b180c03df3 14.2 集群 现在，我们已经成功地创建了一个容器来处理HTTP请求。如果我们用相同的VIRTUAL_HOST启动第二个容器，那么nginx-proxy将在一个循环负载平衡的场景中配置系统。这意味着第一个请求将发送到一个容器，第二个请求将发送到第二个容器，然后循环重复。您可以运行的节点数量没有限制。 使用与前面相同的命令启动第二个容器或者第三个 docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server docker run -d -p 80 -e VIRTUAL_HOST=proxy.example katacoda/docker-http-server 如果使用curl http://ip执行对代理的请求，则请求将由第一个容器处理。第二个HTTP请求将返回不同的机器名，这意味着它是由第二个容器处理的。 $ curl http://ip This request was processed by host: ac2c0ef08655 $ curl http://ip This request was processed by host: a6b180c03df3 当NGINX -proxy为我们自动创建和配置NGINX时，如果你对最终的配置感兴趣，你可以使用docker exec输出完整的配置文件，如下所示。 docker exec nginx cat /etc/nginx/conf.d/default.conf 关于何时重新加载配置的附加信息可以在使用 docker logs nginx 15. 编排docker-compose Docker Compose是基于docker-compose.yml文件。该文件定义启动集群集所需的所有容器和设置。属性映射到您如何使用docker运行命令. 格式yaml： container_name: property: value - or options 在这个场景中，我们有一个需要连接到Redis的Node.js应用程序。首先，我们需要定义docker-compose.yml文件来启动Node.js应用程序。 根据上面的格式，文件需要将容器命名为“web”，并将构建属性设置为当前目录。我们将在以后的步骤中介绍其他属性。 将下列yaml复制到编辑器中。这将定义一个名为web的容器，它基于当前目录的构建。 web: build: . Docker Compose支持所有可以使用Docker run定义的属性。将两个容器链接在一起以指定links属性并列出所需连接。例如，下面将链接到相同文件中定义的redis源容器，并将相同的名称分配给别名。 links: - redis 同样的格式用于端口等其他属性,有关选项的其他文档可在以下网址找到 https://docs.docker.com/compose/compose-file/ 更新我们的web容器，以暴露3001端口，并创建一个链接到我们的Redis容器。 在上一步中，我们使用当前目录中的Dockerfile作为容器的基础。在此步骤中，我们希望使用来自Docker Hub的现有映像作为第二个容器。 要找到第二个容器，只需在新行上使用与前面相同的格式。YAML格式非常灵活，可以在同一个文件中定义多个容器。 定义第二个名称为redis的容器，它使用镜像redis。按照YAML格式，容器的详细信息如下: redis: image: redis:alpine volumes: - /var/redis/data:/data 使用创建的docker-compose.yml文件就绪后，您可以使用一个up命令启动所有应用程序。如果您想调出单个容器，那么您可以使用。 参数-d表示在后台运行容器，类似于与docker run一起使用。 $ ls Dockerfile Makefile docker-compose.yml node_modules package.json server.js $ cat Dockerfile FROM ocelotuproar/alpine-node:5.7.1-onbuild EXPOSE 3000 $ cat docker-compose.yml web: build: . links: - redis ports: - \"3000\" - \"8000\" redis: image: redis:alpine volumes: - /var/redis/data:/data 启动部署运行 $ docker-compose up -d $ docker-compose ps Name Command State Ports ------------------------------------------------------------------------------------------------------------ tutorial_redis_1 docker-entrypoint.sh redis ... Up 6379/tcp tutorial_web_1 npm start Up 0.0.0.0:32769->3000/tcp, 0.0.0.0:32768->8000/tcp #查看日志 $ docker-compose logs 由于Docker Compose理解如何启动应用程序容器，所以它还可以用于扩展正在运行的容器数量。scale选项允许指定服务，然后指定所需的实例数量。如果数量大于已经运行的实例，那么它将启动额外的容器。如果数量较少，那么它将停止不需要的容器。 使用该命令扩展正在运行的web容器的数量 $ docker-compose scale web=3 WARNING: The scale command is deprecated. Use the up command with the --scale flag instead. Starting tutorial_web_1 ... done Creating tutorial_web_2 ... done Creating tutorial_web_3 ... done 你可以减少使用 $ docker-compose scale web=1 WARNING: The scale command is deprecated. Use the up command with the --scale flag instead. Stopping and removing tutorial_web_2 ... done Stopping and removing tutorial_web_3 ... done 与启动应用程序时一样，要停止一组容器，可以使用该命令 $ docker-compose stop 要删除所有容器，请使用此命令 $ docker-compose rm 16. docker stats统计信息 环境中有一个名为nginx的容器运行。你可以通过以下方法找到容器的统计信息: $ docker stats nginx CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 048931bbead9 nginx 0.12% 17.21MiB / 737.6MiB 2.33% 7.73kB / 484B 0B / 16.4kB 17 这将启动一个终端窗口，该窗口使用来自容器的实时数据来刷新自身。如果需要退出，请使用CTRL+C停止正在运行的进程。 内置的Docker允许你提供多个名称/id，并在一个窗口中显示它们的统计信息。 环境现在有三个连接的容器在运行。要查看所有这些容器的统计信息，可以使用管道和xargs。管道将一个命令的输出传递到另一个命令的输入，而xargs允许您将该输入作为参数提供给命令。 通过结合这两种方法，我们可以获取由docker ps提供的所有正在运行的容器的列表，并将它们用作docker stats的参数。这让我们对整个机器的容器有了一个概述。 $ docker ps -q | xargs docker stats 17. dockerfile多阶段构建创建优化docker镜像 多阶段特性允许一个Dockerfile包含多个阶段，以生成所需的、优化的Docker映像。 在此之前，这个问题是通过两个dockerfile解决的。一个文件将包含使用开发容器构建二进制文件和工件的步骤，第二个文件将针对生产进行优化，不包括开发工具。 通过在生产映像中删除开发工具，可以重新生成攻击面并改进部署时间。 首先部署一个示例Golang HTTP服务器。目前使用的是两阶段的Docker构建方法。这个场景将创建一个新的Dockerfile，允许使用单个命令构建映像。 git clone https://github.com/katacoda/golang-http-server.git 使用编辑器，创建一个多阶段Dockerfile。第一阶段使用Golang SDK构建二进制文件。第二阶段将生成的二进制文件复制到一个优化的Docker镜像中。 # First Stage FROM golang:1.6-alpine RUN mkdir /app ADD . /app/ WORKDIR /app RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main . # Second Stage FROM alpine EXPOSE 80 CMD [\"/app\"] # Copy from first stage COPY --from=0 /app/main /app 目前有一些关于改进语法的讨论，您可以参阅https://github.com/docker/docker/pull/31257 Dockerfile的新语法到位后，构建过程与前面相同。使用下面的build命令创建所需的Docker镜像。 docker build -f Dockerfile.multi -t golang-app . 结果将是两个镜像。一个是第一阶段使用的未标记的镜像，另一个是较小的镜像，也是我们的目标镜像。 如果你收到错误，COPY --from=0 /build/out /app/ Unknown flag: from，这意味着你正在运行一个没有多阶段支持的旧版本的Docker。本场景的步骤1升级当前Docker版本。 可以启动和部署镜像，而不需要进行任何更改。 $docker run -d -p 80:80 golang-app $ curl localhost This request was processed by host: 178cf19ec6e9 18. docker ps输出格式 $ docker run -d redis $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1bf198e56506 redis \"docker-entrypoint.s…\" 22 seconds ago Up 18 seconds 6379/tcp angry_northcutt # --format方法 $ docker ps --format '{{.Names}} container is using {{.Image}} image' angry_northcutt container is using redis image $ docker ps --format 'table {{.Names}}\\t{{.Image}}' NAMES IMAGE angry_northcutt redis 然而，format参数允许显示通过docker ps命令已经暴露的数据。如果您想包含额外的信息，比如容器的IP地址，那么数据需要通过docker inspect来获取。然后，format参数可以访问所有容器信息。下面是列出正在运行的容器的所有IP地址的示例。 $ docker ps -q | xargs docker inspect --format '{{ .Id }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}' 1bf198e565063b29b75341cf958536482d2e6ad605a8032d0572e2b1770ab924 - /angry_northcutt - 172.18.0.2 19. Docker非root特权配置 该环境当前运行的是Ubuntu 16.04，用户以root身份登录。第一步是创建一个没有这些root特权的新用户，这意味着他们将以更高的安全性运行，并且不能对系统进行关键更改。 useradd 命令将创建一个具有默认权限的用户。在终端上执行命令，添加一个名为lowprivuser的新用户。这个用户可以被称为任何名称。 $ useradd -m -d /home/lowprivuser -p $(openssl passwd -1 password) lowprivuser 使用'sudo su '，可以切换到以这个新的低权限用户运行。 $ sudo su lowprivuser lowprivuser@host01:/root$ 当作为该用户运行时，一些项会发生变化。例如，用户无法在某些位置(如根目录)创建或更改文件， lowprivuser@host01:/root$ touch /root/blocked touch: cannot touch '/root/blocked': Permission denied 户也无法访问Docker，因为之前这需要他们有根权限。 lowprivuser@host01:/root$ docker ps Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied 在下一步中，我们将部署新的Rootless版本，并允许用户启动自己的容器。Docker提供了一个脚本，用于部署新的Rootless版本所需的组件。 使用lowprivuser命令执行脚本，安装组件。 lowprivuser@host01:/root$ curl -sSL https://get.docker.com/rootless | sh # Installing stable version 20.10.8 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 58.1M 100 58.1M 0 0 55.4M 0 0:00:01 0:00:01 --:--:-- 55.5M % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 18.0M 100 18.0M 0 0 35.2M 0 --:--:-- --:--:-- --:--:-- 35.2M + PATH=/home/lowprivuser/bin:/root/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games /home/lowprivuser/bin/dockerd-rootless-setuptool.sh install [INFO] systemd not detected, dockerd-rootless.sh needs to be started manually: PATH=/home/lowprivuser/bin:/sbin:/usr/sbin:$PATH dockerd-rootless.sh [INFO] Creating CLI context \"rootless\" Successfully created context \"rootless\" [INFO] Make sure the following environment variables are set (or add them to ~/.bashrc): # WARNING: systemd not found. You have to remove XDG_RUNTIME_DIR manually on every logout. export XDG_RUNTIME_DIR=/home/lowprivuser/.docker/run export PATH=/home/lowprivuser/bin:$PATH export DOCKER_HOST=unix:///home/lowprivuser/.docker/run/docker.sock 完成这一步之后，继续下一步，设置用户环境并开始启动容器。 现在已经安装了 rootless Docker。可以使用以下脚本启动守护进程: export XDG_RUNTIME_DIR=/tmp/docker-1001 export PATH=/home/lowprivuser/bin:$PATH export DOCKER_HOST=unix:///tmp/docker-1001/docker.sock mkdir -p $XDG_RUNTIME_DIR /home/lowprivuser/bin/dockerd-rootless.sh --experimental --storage-driver vfs 这将在前台运行，并允许您查看来自rootless Docker守护进程的调试输出。 单击以下命令启动第二个终端窗口，以lowprivuser用户登录 sudo lowprivuser 要访问Docker，请设置以下环境变量。它指定了对id为1001的用户运行的Docker实例的连接，该连接应该与lowprivuser的id匹配。 export XDG_RUNTIME_DIR=/tmp/docker-1001 export PATH=/home/lowprivuser/bin:$PATH export DOCKER_HOST=unix:///tmp/docker-1001/docker.sock 在下一步中，我们可以启动容器。 现在可以访问用户1001运行的Docker守护进程。 标准的Docker CLI命令的工作方式与此相同。下面的命令列出了为用户运行的所有容器，目前它应该返回一个空列表。 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 可以查看Daemon运行的细节: docker info docker run -it ubuntu bash root@002495da35fe:/# id uid=0(root) gid=0(root) groups=0(root) 容器内的用户仍然被报告为root。他们将能够安装包和修改Docker内部运行的部分系统。然而，如果它们设法逃脱，它们将无法干扰宿主。 在单独的终端窗口中，宿主机root用户可以查看哪些进程正在运行，哪些用户启动了它们。使用ps aux可以验证我们的新容器实例是由低特权用户管理和拥有的。 $ id; ps aux | grep lowprivuser uid=0(root) gid=0(root) groups=0(root) root 21016 0.0 0.2 52700 3940 pts/0 S 03:18 0:00 sudo su lowprivuser root 21017 0.0 0.2 52280 3540 pts/0 S 03:18 0:00 su lowprivuser lowpriv+ 21145 0.0 0.4 710816 6912 pts/0 Sl+ 03:22 0:00 rootlesskit --net=vpnkit --mtu=1500 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=builtin --copy-up=/etc --copy-up=/run --propagation=rslave /home/lowprivuser/bin/dockerd-rootless.sh --experimental --storage-driver vfs lowpriv+ 21150 0.5 0.7 711712 12228 pts/0 Sl+ 03:22 0:01 /proc/self/exe --net=vpnkit --mtu=1500 --slirp4netns-sandbox=auto --slirp4netns-seccomp=auto --disable-host-loopback --port-driver=builtin --copy-up=/etc --copy-up=/run --propagation=rslave /home/lowprivuser/bin/dockerd-rootless.sh --experimental --storage-driver vfs lowpriv+ 21430 0.0 0.5 711432 8612 ? Sl 03:26 0:00 /home/lowprivuser/bin/containerd-shim-runc-v2 -namespace moby -id 002495da35fe1431fd79e2dda1bf3447c7d4d9890220067324c9d64aa0f30e9f -address /tmp/docker-1001/docker/containerd/containerd.sock root 21532 0.0 0.0 14224 920 pts/2 S+ 03:27 0:00 grep --color=auto lowprivuser 系统现在运行Docker容器，不需要任何额外的权限，允许我们的系统以更高的安全性操作。 相关阅读： docker 快速学习手册 docker 的冷门高效玩法 docker 命令使用大全 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Podman/":{"url":"Podman/","title":"Podman","keywords":"","body":"Podman Overview1. 概述2. 什么是 Podman？3. 使用 Podman 的多种方式4. Podman 是如何管理容器的？5. Podman、Buildah 和 Skopeo6. Podman 与Docker7. 为什么选择 Podman？Podman Overview 1. 概述 Podman（全称 POD 管理器）是一款用于在 Linux® 系统上开发、管理和运行容器的开源工具。Podman 最初由红帽® 工程师联合开源社区一同开发，它可利用 lipod 库来管理整个容器生态系统。 Podman 采用无守护进程的包容性架构，因此可以更安全、更简单地进行容器管理，再加上 Buildah 和 Skopeo 等与之配套的工具和功能，开发人员能够按照自身需求来量身定制容器环境。 它与Docker扮演相同的角色，并且在很大程度上与 Docker 兼容，提供几乎相同的命令。 2. 什么是 Podman？ 我们发现以下视频和文章是很好的起点。 在Podman：下一代 Linux 容器工具中，Doug Tidwell 解释了 Podman 是什么以及如何安装该工具、使用它构建映像、运行映像、将映像推送到容器注册表、将映像下载到非 Linux系统，并使用 Docker 运行映像。 从 Docker 过渡到 Podman是关于容器的最受欢迎的红帽开发人员文章之一，它使用真实示例向您展示如何安装Podman、使用其基本命令以及从 Docker 命令行界面 (CLI) 过渡到 Podman . 您还将了解如何使用 Podman 运行现有映像以及如何设置端口转发。 在Podman 的无根容器：基础知识中，Prakhar Sethi 解释了使用容器和 Podman 的好处。本文介绍了无根容器并解释了它们的重要性，然后通过一个示例场景向您展示了如何在 Podman 中使用无根容器。 要获得一些实践，请参阅使用容器工具部署容器，这是一个简短（仅 10 分钟）的课程，将教您如何部署和控制已定义的容器映像。 3. 使用 Podman 的多种方式 以下是在各种环境中使用它的一些资源： 将您的应用程序交付到无根容器中的边缘和物联网设备向您展示了如何使用systemd Podman 和红帽 Ansible 自动化来自动化并将软件作为容器推送到小型边缘和物联网(IoT) 网关设备。 使用 Podman 构建应用程序映像企业 Linux。 Kubernetes开发人员应该查看使用 Podman 将开发环境迁移到生产环境。该视频展示了如何将容器从桌面移动到生产Kubernetes。Podman 的generate-kube工具可以提供帮助。该视频首先使用 Podman 生成一个Kubernetes YAML 文件，然后介绍使用该 YAML 将环境从本地开发迁移到 OpenShift 生产所需的步骤。 Podman：用于处理容器和 Pod 的 Linux 工具：本教程向您展示如何安装 Podman，使用它构建映像，使用 Podman 运行映像，将映像推送到容器注册表，然后将映像下载到非 Linux系统并使用 Docker 运行它。 Podman 入门：加入实习生 Cedric Clyburn，他将向您介绍 Podman 的基础知识。使用它来运行现有映像、端口转发和构建映像。 使用最佳实践和 IBM Cloud Code Engine 容器化和部署您的 Node.js 应用程序：采用最佳实践，使用多阶段 Dockerfile、ubi8/nodejs-14-minimal基本映像、Buildah、Podman 和安全容器注册表来容器化您的Node.js 应用程序。然后将您的应用程序容器部署到 IBM Cloud Code Engine，这是一个完全托管的 Knative无服务器平台，可运行您的容器化工作负载，包括 Web 应用程序、微服务、事件驱动函数和批处理作业。 最后，下载 Podman 基础备忘单，以获得更快、更轻松的 Podman 体验。 4. Podman 是如何管理容器的？ 用户可以从命令行调用 Podman，以便从存储库拉取容器并运行它们。Podman 调用配置好的容器运行时来创建运行的容器。不过，由于没有专门的守护进程，Podman 使用 systemd（一种用于 Linux 操作系统的系统和服务管理器）来进行更新并让容器在后台保持运行。通过将 systemd 和 Podman 集成，您可以为容器生成控制单元，并在自动启用 systemd 的前提下运行它们。 用户可以在系统上管理自有的存储库，也可通过 systemd 单元来控制自有容器的自动启动和管理。允许用户管理自己的资源并使容器以无根方式运行，可以阻止诸如使 /var/lib/containers 目录可被写入等不良做法，或防范可能会导致应用暴露于额外安全问题的其他系统管理做法。这样也可确保每一用户具有单独的容器和镜像集合，并可在同一主机上同步使用 Podman，而不会彼此干扰。用户完成自己的工作时，可将变更推送到共有的镜像仓库，将他们的镜像共享给其他人。 Podman 也可部署 RESTful API（REST API）来管理容器。REST 是表述性状态传递的英文缩写。REST API 是遵循 REST 架构规范的应用编程接口，支持与 RESTful Web 服务进行交互。借助 REST API，您可以从 cURL、Postman 和 Google 的 Advanced REST 客户端等许多平台调用 Podman。 5. Podman、Buildah 和 Skopeo Podman 是一种模块化容器引擎，因此必须与 Buildah 和 Skopeo 等工具搭配使用才能构建和移动容器。使用 Buildah 时，您可以从头开始构建容器，也可将某个镜像用作起点来构建。Skopeo 可在不同类型的存储系统之间移动容器镜像，允许您在不同镜像仓库（例如 docker.io、quay.io 和您的内部镜像仓库）之间以及本地系统上不同类型的存储之间复制镜像。这种模块式的容器化工具有助于生成灵活的轻量型环境，减小开销并隔离您实现目标所需的功能。工具的体量越小、模块化程度越高，演进发展的速度也越快，而且每一工具也能专注于单一用途。 我们可以把 Podman、Buildah 和 Skopeo 比喻成一套特殊的瑞士军刀，它们彼此互补，几乎能满足所有容器用例的需求。Podman 就是这套刀中最大的一把。 Podman 和 Buildah 默认使用 runC（OCI 运行时）来启动容器。您可以构建和运行镜像，也可通过 runC 运行 docker 格式的镜像。Buildah 由 Go 语言编写，可读取运行时规范，配置 Linux 内核，最终创建并启动容器进程。在更改一些配置后，您也可以将 Podman 与 crun 等其他工具搭配使用。 6. Podman 与Docker Docker 是支持创建和使用 Linux 容器的一种容器化技术。Podman 和 Docker 的主要区别在于 Podman 采用无守护进程架构。Podman 容器一直是无根的，而 Docker 最近才将无根模式添加到其守护进程配置中。Docker 是用于创建和管理容器的一体化工具，而 Podman 以及 Buildah 和 Skopeo 等相关工具则更擅长承担某些方面的容器化工作，让您能够根据自己在云原生应用中的需求来进行自定义。 Podman 具有替代 Docker 的强大实力，但两者也可搭配使用。用户可以将 Docker 别名设置为 Podman（alias docker=podman）或反之，在这两者之间轻松切换。此外，一个叫做 podman-docker 的 rpm 可以将\"docker\"置入系统应用 PATH，从 Docker 轻松切换过来，在需要\"docker\"命令时对这些环境调用 Podman。Podman 的 CLI 与 Docker 容器引擎类似，熟悉其中之一的用户也能快速上手使用另一个。 一些开发人员会一起使用 Podman 和 Docker，在开发阶段使用 Docker，然后在运行时环境中将程序转移到 Podman，享受其更好的安全性。 Podman 非常适合不使用 Kubernetes 或 OpenShift 容器平台来运行容器的开发人员。CRI-O 是一个用于 Kubernetes 容器编排系统（例如红帽 OpenShift® 容器平台）的社区推动的开源容器引擎。 7. 为什么选择 Podman？ Podman 改变了容器格局，它不仅提供比肩领先容器引擎的高性能功能，而且具有如今许多开发团队迫切需要的灵活性、可访问性和高安全性。Podman 可以帮助您： 管理容器镜像和完整的容器生命周期，包括运行、联网、检查和下线。 为无根容器和容器集运行和隔离资源。 支持 OCI 和 Docker 镜像，以及与 Docker 兼容的 CLI。 打造一个无守护进程的环境，提高安全性并减少闲置资源消耗。 部署 REST API 来支持 Podman 的高级功能。 借助 checkpoint/restore in userspace（CRIU），实施面向 Linux 容器的检查点/恢复功能。CRIU 可以冻结正在运行的容器，并将其内存内容和状态保存到磁盘，以便容器化工作负载能够更快地重新启动。 自动更新容器。Podman 会检测更新的容器是否无法启动，并自动回滚到上一个工作版本。这可以将应用的可靠性提升到新的水平。 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Podman/podman_deploy_private_registry.html":{"url":"Podman/podman_deploy_private_registry.html","title":"podman 部署私有镜像仓库","keywords":"","body":"Podman 部署私有镜像仓库1. 安装 Podman 和 httpd-tools2. 配置仓库存储位置3. 生成访问仓库的凭据3.1 htpasswd 用户名和密码3.2 TLS 密钥对4. 启动容器5. 测试5.1 登陆5.2 API访问5.3 镜像入库5.4 查询镜像信息Podman 部署私有镜像仓库 tagsstart registry tagsstop Podman是一个无守护进程的开源 Linux 原生工具，旨在使用开放容器倡议 ( OCI )容器和容器映像轻松查找、运行、构建、共享和部署应用程序。主要是由RedHat推动改进。 关于了解 Podman 更多内容： Podman 下一代 Linux 容器工具 Podman 入门指南 1. 安装 Podman 和 httpd-tools yum install -y podman httpd-tools 2. 配置仓库存储位置 存储目录为 /opt/registry/ mkdir -p /opt/registry/{auth,certs,data} Auth子目录存储htpasswd用于身份验证的文件。 Certs子目录存储仓库使用的证书验证。 Data目录存储存储在仓库中的实际镜像。 如果你想单独挂载一块盘来存储数据可以利用parted命令 sudo parted -s -a optimal -- /dev/sdb mklabel gpt sudo parted -s -a optimal -- /dev/sdb mkpart primary 0% 100% sudo parted -s -- /dev/sdb align-check optimal 1 sudo pvcreate /dev/sdb1 sudo vgcreate vg0 /dev/sdb1 sudo lvcreate -n registry -l +100%FREE vg0 sudo mkfs.xfs /dev/vg0/registry echo \"/dev/vg0/registry /opt/registry/data xfs defaults 0 0\" | sudo tee -a /etc/fstab 挂载验证 $ sudo mount -a $ df -hT /opt/registry/data Filesystem Type Size Used Avail Use% Mounted on /dev/mapper/vg0-registry xfs 200G 1.5G 199G 1% /opt/registry/data 3. 生成访问仓库的凭据 3.1 htpasswd 用户名和密码 身份验证由一个简单的htpasswd文件和一个 SSL 密钥对提供 htpasswd将在该/opt/registry/auth/目录中创建一个名为 Bcrypt Htpasswd 的文件 htpasswd -bBc /opt/registry/auth/htpasswd registryuser registryuserpassword b通过命令提供密码。 B使用 Bcrypt 加密存储密码。 c创建文件。 用户名为 registryuser。 密码是 registryuserpassword。 查看文件 $ tac /opt/registry/auth/htpasswd registryuser:$2y$05$XciI1wfzkUETe7XazJfc/uftBnMQfYOV1jOnbV/QOXw/SXhmLsApK 3.2 TLS 密钥对 通过使用由可信机构（内部或外部）签名的密钥和证书或简单的自签名证书，仓库通过 TLS 得到保护。要使用自签名证书： cat ssl.conf [ req ] prompt = no distinguished_name = req_subj x509_extensions = x509_ext [ req_subj ] CN = Localhost [ x509_ext ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer basicConstraints = CA:true subjectAltName = @alternate_names [ alternate_names ] DNS.1 = localhost IP.1 = 192.168.10.80 EOF openssl req -config ssl.conf -new -x509 -nodes -sha256 -days 365 -newkey rsa:4096 -keyout /opt/registry/certs/domain.key -out /opt/registry/certs/domain.crt openssl x509 -inform PEM -in /opt/registry/certs/domain.crt -out /opt/registry/certs/domain.cert req OpenSSL 生成和处理证书请求。 -newkey OpenSSL 创建一个新的私钥和匹配的证书请求。 rsa:4096 OpenSSL 生成一个 4096 位的 RSA 密钥。 -nodes OpenSSL 私钥没有密码要求。私钥不会被加密。 -sha256 OpenSSL 使用 sha256 来签署请求。 -keyout OpenSSL 存储新密钥的名称和位置。 -x509 OpenSSL 生成一个自签名证书。 -days OpenSSL 密钥对有效的天数。 -out OpenSSL 在哪里存储证书。 输入证书的相应选项。CN=值是您的主机的主机名。主机的主机名应该可由 DNS 或/etc/hosts文件解析。 $ ll /opt/registry/certs/ total 12 -rw-r--r-- 1 root root 1842 Nov 21 20:01 domain.cert -rw-r--r-- 1 root root 1842 Nov 21 20:01 domain.crt -rw------- 1 root root 3272 Nov 21 20:01 domain.key 将服务器证书、密钥和 CA 文件复制到 podman证书文件夹中。您必须首先创建适当的文件夹 mkdir -p /etc/containers/certs.d/192.168.10.80\\:5000/ cp -r /opt/registry/certs/* /etc/containers/certs.d/192.168.10.80\\:5000/ 注意：如果仓库未使用 TLS 保护，则/etc/containers/registries.conf可能必须为仓库配置文件中的不安全设置。 该证书还必须得到您的主机和客户端的信任： cp /opt/registry/certs/domain.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust trust list | grep -i \"\" 4. 启动容器 $ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/registry latest 81c944c2288b 9 days ago 24.7 MB podman run --name myregistry \\ -p 5000:5000 \\ -v /opt/registry/data:/var/lib/registry:z \\ -v /opt/registry/auth:/auth:z \\ -e \"REGISTRY_AUTH=htpasswd\" \\ -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v /opt/registry/certs:/certs:z \\ -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt\" \\ -e \"REGISTRY_HTTP_TLS_KEY=/certs/domain.key\" \\ -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \\ -e REGISTRY_STORAGE_DELETE_ENABLED=true \\ -d \\ docker.io/library/registry:latest 选项的详细信息是： --name myregistry将容器命名为myregistry。 -p 5000:5000将容器中的端口 5000 公开为主机上的端口 5000。 -v /opt/registry/data:/var/lib/registry:z像 在具有正确 SELinux 上下文的容器中一样安装/opt/registry/data 在主机/var/lib/registry -v /opt/registry/auth:/auth:z/opt/registry/auth在主机上安装，就像/auth 在具有正确 SELinux 上下文的容器中一样。 -v opt/registry/certs:/certs:z像 在具有正确 SELinux 上下文的容器中一样安装/opt/registry/certs 在主机上 。/certs -e \"REGISTRY_AUTH=htpasswd\" 使用bcrypt加密htpasswd文件进行身份验证。由容器的 REGISTRY_AUTH_HTPASSWD_PATH 环境变量设置的文件位置。 -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" 指定用于htpasswd. -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd使用容器中的 bcrypt 加密/auth/htpasswd 文件。 -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt\"设置证书文件的路径。 -e \"REGISTRY_HTTP_TLS_KEY=/certs/domain.key\"设置私钥路径。 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true为 schema1 清单提供向后兼容性。 -e REGISTRY_STORAGE_DELETE_ENABLED=true 可以通过API 删除镜像 -d docker.io/library/registry:latest是一个允许存储和分发镜像的仓库应用程序。 注意：如果防火墙在主机上运行，​​则需要允许暴露的端口 (5000)。 firewall-cmd --add-port=5000/tcp --zone=internal --permanent firewall-cmd --add-port=5000/tcp --zone=public --permanent firewall-cmd --reload 或者直接关闭 systemctl stop firewalld && systemctl disable firewalld setenforce 0 5. 测试 5.1 登陆 docker login -u registryuser -p registryuserpassword 192.168.10.80:5000 Login Succeeded! 5.2 API访问 $ curl -k -u \"registryuser:registryuserpassword\" https://192.168.10.80:5000/v2/_catalog {\"repositories\":[]} 更多API 访问策略请参考这里 5.3 镜像入库 从公共拉取alpine:latest镜像 $ podman pull alpine:latest Resolved \"alpine\" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf) Trying to pull docker.io/library/alpine:latest... Getting image source signatures Copying blob ca7dd9ec2225 [--------------------------------------] 0.0b / 0.0b Copying config bfe296a525 done Writing manifest to image destination Storing signatures bfe296a525011f7eb76075d688c681ca4feaad5afe3b142b36e30f1a171dc99a 打标签 podman tag alpine:latest 192.168.10.80:5000/alpine:latest 推送入库 podman push 192.168.10.80:5000/alpine:latest 5.4 查询镜像信息 查询是否入库 $ curl -k -u \"registryuser:registryuserpassword\" https://192.168.10.80:5000/v2/_catalog {\"repositories\":[\"alpine\"]} 查看镜像标签 $ curl -k -u \"registryuser:registryuserpassword\" https://192.168.10.80:5000/v2/alpine/tags/list {\"name\":\"alpine\",\"tags\":[\"latest\"]} 查看镜像 manifests $ curl -k -u \"registryuser:registryuserpassword\"https://192.168.10.80:5000/v2/alpine/manifests/latest { \"schemaVersion\": 1, \"name\": \"alpine\", \"tag\": \"latest\", \"architecture\": \"amd64\", \"fsLayers\": [ { \"blobSum\": \"sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\" }, { \"blobSum\": \"sha256:60f8044dac9f779802600470f375c7ca7a8f7ad50e05b0ceb9e3b336fa5e7ad3\" } ], \"history\": [ { \"v1Compatibility\": \"{\\\"architecture\\\":\\\"amd64\\\",\\\"config\\\":{\\\"Hostname\\\":\\\"\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":[\\\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\\"],\\\"Cmd\\\":[\\\"/bin/sh\\\"],\\\"Image\\\":\\\"sha256:18f412e359de0426344f4fe1151796e2d9dc121b01d737e953f043a10464d0b7\\\",\\\"Volumes\\\":null,\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"OnBuild\\\":null,\\\"Labels\\\":null},\\\"container\\\":\\\"3cd2ce612b9119be9673860022420eee020f0a6d44e9072ca25196f4f0a4613d\\\",\\\"container_config\\\":{\\\"Hostname\\\":\\\"3cd2ce612b91\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":[\\\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\\"],\\\"Cmd\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"#(nop) \\\",\\\"CMD [\\\\\\\"/bin/sh\\\\\\\"]\\\"],\\\"Image\\\":\\\"sha256:18f412e359de0426344f4fe1151796e2d9dc121b01d737e953f043a10464d0b7\\\",\\\"Volumes\\\":null,\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"OnBuild\\\":null,\\\"Labels\\\":{}},\\\"created\\\":\\\"2022-11-12T04:19:23.199716539Z\\\",\\\"docker_version\\\":\\\"20.10.12\\\",\\\"id\\\":\\\"260323e12fa2abcb1ff61576931037c6f8538afeb5ff82fa256670a20b384b6b\\\",\\\"os\\\":\\\"linux\\\",\\\"parent\\\":\\\"faa2cddd53c99ad978614b839a2a20a47f143a4d6ecb86bda576dfb3124c0cad\\\",\\\"throwaway\\\":true}\" }, { \"v1Compatibility\": \"{\\\"id\\\":\\\"faa2cddd53c99ad978614b839a2a20a47f143a4d6ecb86bda576dfb3124c0cad\\\",\\\"created\\\":\\\"2022-11-12T04:19:23.05154209Z\\\",\\\"container_config\\\":{\\\"Cmd\\\":[\\\"/bin/sh -c #(nop) ADD file:ceeb6e8632fafc657116cbf3afbd522185a16963230b57881073dad22eb0e1a3 in / \\\"]}}\" } ], \"signatures\": [ { \"header\": { \"jwk\": { \"crv\": \"P-256\", \"kid\": \"5BQE:5CXW:TWNN:OFV7:ZPNY:ARAG:ZJ7K:Z5GI:ZVQ3:SZYQ:2M3J:D7YG\", \"kty\": \"EC\", \"x\": \"-JvBdARI6NPMx8g6d1zyPzmSkkZ8rKIcxdz2BEonpzU\", \"y\": \"4OlY36zLCvLHXzMrb4w8W2TZSJdVc5ijM0Y9DieEkWY\" }, \"alg\": \"ES256\" }, \"signature\": \"ZL0HFyuq9G9cYsBzZZqMlwGK3aQMJHFKeQ2Dh8XByzGKtfoJCJ5kQY0W3yynzb3Mj9WYrzeabZwey-dZIHt_7Q\", \"protected\": \"eyJmb3JtYXRMZW5ndGgiOjIwODgsImZvcm1hdFRhaWwiOiJDbjAiLCJ0aW1lIjoiMjAyMi0xMS0yMVQxMjoyNjowM1oifQ\" } ] } 参考： How to implement a simple personal/private Linux container image registry for internal use docker registry仓库私搭并配置证书 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "},"Podman/podman_start.html":{"url":"Podman/podman_start.html","title":"podman 快速入门","keywords":"","body":"Podman 快速入门1. 什么是podman2. 安装3. 配置3.1 podman包附带文件3.2 /etc/cni3.3 registries.conf3.4 mounts.conf3.5 seccomp.json3.6 policy.json4 镜像管理4.1 镜像搜索4.2 镜像拉取4.3 镜像列出4.4 镜像检查4.5 镜像删除4.6 镜像构建5. 容器管理5.1 容器运行5.2 容器查看5.3 容器停止5.4 容器启动5.5 容器删除5.6 容器checkpoint/restore5.7 容器健康检查5.8 容器用systemd管理6. k8s管理6.1 pod管理6.2 yaml管理Podman 快速入门 1. 什么是podman Podman是一个无守护进程的开源 Linux 原生工具，旨在使用开放容器倡议 ( OCI )容器和容器映像轻松查找、运行、构建、共享和部署应用程序。Podman 提供了任何使用过 Docker容器引擎的人都熟悉的命令行界面 (CLI) 。大多数用户可以简单地将 Docker 别名为 Podman（别名 docker=podman），由谷歌，Redhat、微软、IBM、Intel、思科联合成立的组织（OCI）容器运行时（runc、crun、runv 等）制定了一系列容器运行的规范。主要是由RedHat推动，完成Docker所有的功能和新扩展功能，并且对docker的问题进行了改良：包括不需要守护程序或访问有root权限的组；容器架构基于fork/exec模型创建容器，更加安全可靠；所以是更先进、高效和安全的下一代容器容器工具。Podman是该工具套件的核心，用来替换Docker中了大多数子命令（RUN，PUSH，PULL等）。Podman无需守护进程，使用用户命名空间来模拟容器中的root，无需连接到具有root权限的套接字保证容器的体系安全。 Podman专注于维护和修改OCI镜像的所有命令和功能，例如拉动和标记。它还允许我们创建，运行和维护从这些镜像创建的容器。目前已有相当数量的分发版本开始采用Podman作为容器运行时。与其他常见的容器引擎（Docker、CRI-O、containerd）类似 Podman 比较简单粗暴，它不使用 Daemon，而是直接通过 OCI runtime（默认也是 runc）来启动容器，所以容器的进程是 podman 的子进程。这比较像 Linux 的 fork/exec 模型，而 Docker 采用的是 C/S（客户端/服务器）模型。与 C/S 模型相比，fork/exec 模型有很多优势，比如： 系统管理员可以知道某个容器进程到底是谁启动的。 如果利用 cgroup 对 podman 做一些限制，那么所有创建的容器都会被限制。 SD_NOTIFY : 如果将 podman 命令放入 systemd 单元文件中，容器进程可以通过 podman返回通知，表明服务已准备好接收任务。 socket 激活 : 可以将连接的 socket 从 systemd 传递到 podman，并传递到容器进程以便使用它们。 Podman 控制下的容器可以由 root 或非特权用户运行。Podman 使用libpod库管理整个容器生态系统，包括 pod、容器、容器镜像和容器卷。 有一个 RESTFul API 来管理容器。我们还有一个可以与 RESTFul 服务交互的远程 Podman 客户端。我们目前支持 Linux、Mac 和 Windows 上的客户端。RESTFul 服务仅在 Linux 上受支持。 如果您完全不熟悉容器，我们建议您查看简介。对于高级用户或来自 Docker 的用户，请查看我们的教程。对于高级用户和贡献者，您可以通过查看我们的命令页面来获取有关 Podman CLI 的非常详细的信息。最后，对于寻求如何与 Podman API 交互的开发人员，请参阅我们的 API 文档参考。 2. 安装 官方安装 我的机器环境CentOS Linux 7 yum -y install podman 3. 配置 3.1 podman包附带文件 $ rpm -ql podman |grep -v '/usr/share/man/' # 去除 man 手册中内容 /etc/cni/net.d/87-podman-bridge.conflist /usr/bin/podman /usr/lib/.build-id /usr/lib/.build-id/37 /usr/lib/.build-id/37/e7f04d352e5dbde603e9701baedb0b1be6bc37 /usr/lib/.build-id/9a /usr/lib/.build-id/9a/2b43332ca5756f9e2a086bae9b953009ef5a37 /usr/lib/systemd/system/io.podman.service /usr/lib/systemd/system/io.podman.socket /usr/lib/tmpfiles.d/podman.conf /usr/libexec/podman/conmon /usr/share/bash-completion/completions/podman /usr/share/containers/libpod.conf /usr/share/licenses/podman /usr/share/licenses/podman/LICENSE 3.2 /etc/cni 可以看到只有一个配置文件是在 /etc/cni 路径下的，与 Bridge 的配置有关： $ cat /etc/cni/net.d/87-podman-bridge.conflist { \"cniVersion\": \"0.4.0\", \"name\": \"podman\", \"plugins\": [ { \"type\": \"bridge\", \"bridge\": \"cni-podman0\", \"isGateway\": true, \"ipMasq\": true, \"ipam\": { \"type\": \"host-local\", \"routes\": [ { \"dst\": \"0.0.0.0/0\" } ], \"ranges\": [ [ { \"subnet\": \"10.88.0.0/16\", \"gateway\": \"10.88.0.1\" } ] ] } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } }, { \"type\": \"firewall\" } ] } 3.3 registries.conf /etc/containers/registries.conf 用于保存 registries 相关配置： $ cat /etc/containers/registries.conf |grep -v '#' |grep -v ^$ [registries.search] registries = ['registry.access.redhat.com', 'registry.redhat.io', 'docker.io'] [registries.insecure] registries = [] [registries.block] registries = [] 3.4 mounts.conf /usr/share/containers/mounts.conf 在执行 podman run 或者 podman build 命令时自动挂载的路径，该路径只会在容器运行时挂载，不会提交到容器镜像中。 $ cat /usr/share/containers/mounts.conf /usr/share/rhel/secrets:/run/secrets 3.5 seccomp.json /usr/share/containers/seccomp.json 是容器内允许的 seccomp 规则白名单。 seccomp（secure computing）是一种安全保护机制，一般情况下，程序可以使用所有的 syscall，但是为了避免安全问题发生，通常会指定相应的规则来保证。 $ cat /usr/share/containers/seccomp.json { \"defaultAction\": \"SCMP_ACT_ERRNO\", \"archMap\": [ { \"architecture\": \"SCMP_ARCH_X86_64\", \"subArchitectures\": [ \"SCMP_ARCH_X86\", \"SCMP_ARCH_X32\" ] }, { \"architecture\": \"SCMP_ARCH_AARCH64\", \"subArchitectures\": [ \"SCMP_ARCH_ARM\" ] }, { \"architecture\": \"SCMP_ARCH_MIPS64\", \"subArchitectures\": [ \"SCMP_ARCH_MIPS\", \"SCMP_ARCH_MIPS64N32\" ] }, ······················· 3.6 policy.json /etc/containers/policy.json 证书安全相关配置： $ cat /etc/containers/policy.json { \"default\": [ { \"type\": \"insecureAcceptAnything\" } ], \"transports\": { \"docker-daemon\": { \"\": [{\"type\":\"insecureAcceptAnything\"}] } } } 4 镜像管理 4.1 镜像搜索 $ podman search busybox INDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATED docker.io docker.io/library/busybox Busybox base image. 2410 [OK] docker.io docker.io/radial/busyboxplus Full-chain, Internet enabled, busybox made f... 43 [OK] docker.io docker.io/yauritux/busybox-curl Busybox with CURL 16 docker.io docker.io/arm64v8/busybox Busybox base image. 3 docker.io docker.io/vukomir/busybox busybox and curl 1 docker.io docker.io/odise/busybox-curl 4 [OK] docker.io docker.io/amd64/busybox Busybox base image. 0 docker.io docker.io/prom/busybox Prometheus Busybox Docker base images 2 [OK] docker.io docker.io/ppc64le/busybox Busybox base image. 1 docker.io docker.io/s390x/busybox Busybox base image. 2 docker.io docker.io/arm32v7/busybox Busybox base image. 10 docker.io docker.io/i386/busybox Busybox base image. 2 docker.io docker.io/joeshaw/busybox-nonroot Busybox container with non-root user nobody 2 docker.io docker.io/p7ppc64/busybox Busybox base image for ppc64. 2 docker.io docker.io/arm32v5/busybox Busybox base image. 0 docker.io docker.io/arm32v6/busybox Busybox base image. 3 docker.io docker.io/armhf/busybox Busybox base image. 6 docker.io docker.io/mips64le/busybox Busybox base image. 1 docker.io docker.io/spotify/busybox Spotify fork of https://hub.docker.com/_/bus... 1 docker.io docker.io/aarch64/busybox Busybox base image. 3 docker.io docker.io/progrium/busybox 70 [OK] docker.io docker.io/concourse/busyboxplus 0 docker.io docker.io/lqshow/busybox-curl Busybox image adds a curl binary to /usr/bin 1 [OK] docker.io docker.io/emccorp/busybox Busybox 0 docker.io docker.io/ggtools/busybox-ubuntu Busybox ubuntu version with extra goodies 0 [OK] 4.2 镜像拉取 $ podman image pull nginx Getting image source signatures Copying blob eff15d958d66 done Copying blob 9171c7ae368c done Copying blob 1e5351450a59 [======================================] 24.2MiB / 24.2MiB Copying blob 020f975acd28 done Copying blob 266f639b35ad done Copying blob 2df63e6ce2be done Copying config ea335eea17 done Writing manifest to image destination Storing signatures ea335eea17ab984571cd4a3bcf90a0413773b559c75ef4cda07d0ce952b00291 4.3 镜像列出 $ podman images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/nginx latest ea335eea17ab 12 days ago 146 MB 4.4 镜像检查 $ podman image inspect docker.io/library/nginx [ { \"Id\": \"ea335eea17ab984571cd4a3bcf90a0413773b559c75ef4cda07d0ce952b00291\", \"Digest\": \"sha256:097c3a0913d7e3a5b01b6c685a60c03632fc7a2b50bc8e35bcaa3691d788226e\", \"RepoTags\": [ \"docker.io/library/nginx:latest\" ], \"RepoDigests\": [ \"docker.io/library/nginx@sha256:097c3a0913d7e3a5b01b6c685a60c03632fc7a2b50bc8e35bcaa3691d788226e\", \"docker.io/library/nginx@sha256:2f14a471f2c2819a3faf88b72f56a0372ff5af4cb42ec45aab00c03ca5c9989f\" ], \"Parent\": \"\", \"Comment\": \"\", \"Created\": \"2021-11-17T10:38:14.652464384Z\", \"Config\": { \"ExposedPorts\": { \"80/tcp\": {} }, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"NGINX_VERSION=1.21.4\", \"NJS_VERSION=0.7.0\", \"PKG_RELEASE=1~bullseye\" ], ............................ 4.5 镜像删除 $ podman image rm docker.io/library/nginx f949e7d76d63befffc8eec2cbf8a6f509780f96fb3bacbdc24068d594a77f043 4.6 镜像构建 构建镜像参数参考 # cat Dockerfile FROM registry.access.redhat.com/rhel7/rhel-minimal ENTRYPOINT \"echo \"Podman build this container.\" # podman build -t podbuilt . STEP 1: FROM registry.access... ... Writing manifest to image destination Storing signatures 91e043c11617c08d4f8... # podman run podbuilt Podman build this container. 5. 容器管理 5.1 容器运行 #直接进入好容器 $ podman run -it docker.io/library/busybox #容器在后台运行 $ podman run -itd docker.io/library/busybox --name busybox sh 8d4cf68f9873b19bf8d7ac3a7be4447a9519d931ddfc4162de5ab576c66696af 5.2 容器查看 #查看运行的容器 $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4b05d6c539b3 docker.io/library/busybox:latest sh 17 seconds ago Up 16 seconds ago upbeat_lederberg #查看状态为running的容器 $ podman ps -a -f status=running CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4b05d6c539b3 docker.io/library/busybox:latest sh 5 minutes ago Up 3 seconds ago upbeat_lederberg #查看全部容器 $ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4b05d6c539b3 docker.io/library/busybox:latest sh 2 seconds ago Up 1 second ago upbeat_lederberg 35b58e980d08 docker.io/library/busybox:latest sh 2 minutes ago Exited (0) 46 seconds ago amazing_colden #只看全部容器的ID $ podman ps -aq 4b05d6c539b3 35b58e980d08 5.3 容器停止 $ podman stop upbeat_lederberg 4b05d6c539b387139cb8488a3b988244f4e317cd616e46d9af87c89b79c4edfa 5.4 容器启动 $ podman start upbeat_lederberg upbeat_lederberg 5.5 容器删除 #先停止容器再删除 $ podman rm upbeat_lederberg Error: cannot remove container 4b05d6c539b387139cb8488a3b988244f4e317cd616e46d9af87c89b79c4edfa as it is running - running or paused containers cannot be removed without force: container state improper $ podman stop upbeat_lederberg 4b05d6c539b387139cb8488a3b988244f4e317cd616e46d9af87c89b79c4edfa $ podman rm upbeat_lederberg 4b05d6c539b387139cb8488a3b988244f4e317cd616e46d9af87c89b79c4edfa #强制删除正在运行的容器 $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c7fee70ab080 docker.io/library/busybox:latest sh 2 minutes ago Up 2 minutes ago awesome_morse 78717b0f9b0c docker.io/library/busybox:latest sh 2 minutes ago Up 2 minutes ago strange_fermat $ podman rm -f awesome_morse c7fee70ab0805e31b7834f1fd5748e8995a4fc499009ab95e6bf11ae4954b1bd 5.6 容器checkpoint/restore 检查点目前仅适用于根容器。因此，您必须以 root 身份运行示例容器。除了为每个命令添加前缀之外 sudo，您还可以通过 预先切换到 root 用户sudo -i。 $ podman run -dt -p 8080:80/tcp docker.io/library/httpd Trying to pull docker.io/library/httpd... Getting image source signatures Copying blob eff15d958d66 skipped: already exists Copying blob 0d58b11d2867 [======================================] 23.0MiB / 23.0MiB Copying blob ab86dc02235d done Copying blob e88da7cb925c done Copying blob ba1caf8ba86c done Copying config ad17c88403 done Writing manifest to image destination Storing signatures 51eb5d6f607c97392426b717326e6fdfc5e066ecd617ac273a34d320e3e9afca $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 51eb5d6f607c docker.io/library/httpd:latest httpd-foreground 13 seconds ago Up 13 seconds ago 0.0.0.0:8080->80/tcp suspicious_jones 检查点容器会停止容器，同时将容器中所有进程的状态写入磁盘。有了这个，容器可以在以后恢复并在与检查点完全相同的时间点继续运行。此功能需要在系统上安装CRIU 3.11或更高版本。 centos安装criu $ sudo yum install protobuf protobuf-c gcc protobuf-c-devel protobuf-compiler protobuf-devel protobuf-python -y $ sudo yum install pkg-config python-ipaddr libbsd iproute2 libcap-devel libnet-devel libnl3-devel -y $ sudo yum install asciidoc xmlto -y #目前最新版本v3.16,克隆下载 $ git clone -b v3.16 https://github.com/checkpoint-restore/criu.git $ cd criu #编译安装 $ make & make install #检测 $ criu -V Version: 3.16 GitID: v3.16 $ criu check Looks good. $ podman container checkpoint suspicious_jones ERRO[0000] container is not destroyed ERRO[0000] criu failed: type NOTIFY errno 0 log file: /var/lib/containers/storage/overlay-containers/51eb5d6f607c97392426b717326e6fdfc5e066ecd617ac273a34d320e3e9afca/userdata/dump.log Error: failed to checkpoint container 51eb5d6f607c97392426b717326e6fdfc5e066ecd617ac273a34d320e3e9afca: `/usr/bin/runc checkpoint --image-path /var/lib/containers/storage/overlay-containers/51eb5d6f607c97392426b717326e6fdfc5e066ecd617ac273a34d320e3e9afca/userdata/checkpoint --work-path /var/lib/containers/storage/overlay-containers/51eb5d6f607c97392426b717326e6fdfc5e066ecd617ac273a34d320e3e9afca/userdata 51eb5d6f607c97392426b717326e6fdfc5e066ecd617ac273a34d320e3e9afca` failed: exit status 1 解决方法请参考 $ podman container checkpoint exciting_neumann 2135a176f1b5f75668836c3a4e374cd1bcfc17545414ba5fb41be6558078f470 $ podman container restore exciting_neumann 2135a176f1b5f75668836c3a4e374cd1bcfc17545414ba5fb41be6558078f470 $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2135a176f1b5 docker.io/library/httpd:latest httpd-foreground About an hour ago Up 23 minutes ago 0.0.0.0:8080->80/tcp exciting_neumann 更多细节请参考官方 5.7 容器健康检查 健康检查”是一种用户可以确定在容器内运行的主进程的“健康”或准备情况的方式。这不仅仅是一个简单的“我的容器在运行吗？” 题。更像是“我的应用程序准备好了吗？” 因此，健康检查实际上是一种验证容器及其应用程序是否响应的方法。 健康检查由五个基本组成部分组成： Command Retries Interval Start-period Timeout $ sudo podman run -dt --name hc1 --healthcheck-command 'CMD-SHELL curl http://localhost || exit 1' --healthcheck-interval=0 quay.io/libpod/alpine_nginx:latest d25ee6faaf6e5e12c09e734b1ac675385fe4d4e8b52504dd01a60e1b726e3edb $ sudo podman healthcheck run hc1 Healthy $ echo $? 0 在podman run命令上，请注意--healthcheck-command定义 healthcheck 命令本身的位置的使用。请记住，这是一个在容器本身“内部”运行的命令。在这种情况下，curl 命令需要存在于容器中。另外，请注意--healthcheck-interval标志及其“0”值。间隔标志定义运行健康检查的时间频率。“0”值表示我们要手动运行健康检查。 healthcheck更多细节请参考 5.8 容器用systemd管理 由于 Podman 没有 daemon ，所以没办法像 docker 一样通过指定参数 --restart=always 在 docker 进程启动时自动拉起镜像。 Podman 通过 systemd 来支持该功能。 首先，我们需要准备一个已经可以正常运行的容器： $ podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 94de914cfc45 docker.io/library/busybox:latest sh 5 minutes ago Up 5 minutes ago hello 编写 systemd 配置文件，通常默认路径为： /usr/lib/systemd/system/ $ vim /usr/lib/systemd/system/hello.service [Unit] After=network.target [Service] Restart=always ExecStart=/usr/bin/podman start -a hello ExecStop=/usr/bin/podman stop -t 10 hello [Install] WantedBy=multi-user.target 编写完成后，我们需要执行下 systemctl daemon-reload 重新加载一次配置，然后就可以通过 systemctl 来控制容器的启停、开机自启动了。 $ systemctl daemon-reload $ systemctl status hello ● hello.service Loaded: loaded (/usr/lib/systemd/system/hello.service; disabled; vendor preset: disabled) Active: inactive (dead) $ systemctl start hello $ systemctl status hello ● hello.service Loaded: loaded (/usr/lib/systemd/system/hello.service; disabled; vendor preset: disabled) Active: active (running) since 一 2021-11-29 21:52:50 CST; 3s ago Main PID: 4760 (podman) Tasks: 8 CGroup: /system.slice/hello.service └─4760 /usr/bin/podman start -a hello 11月 29 21:52:50 localhost.localdomain systemd[1]: Started hello.service. $ systemctl enable hello Created symlink from /etc/systemd/system/multi-user.target.wants/hello.service to /usr/lib/systemd/system/hello.service. #停止hello容器 [root@localhost ~]# podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 94de914cfc45 docker.io/library/busybox:latest sh 8 minutes ago Up 8 minutes ago hello [root@localhost ~]# systemctl stop hello [root@localhost ~]# podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES #启动hello容器 [root@localhost ~]# systemctl start hello [root@localhost ~]# podman ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 94de914cfc45 docker.io/library/busybox:latest sh 9 minutes ago Up 2 seconds ago hello 6. k8s管理 6.1 pod管理 pod不但管理起来像个docker，它还可以管理pod Podman 的 YAML 和 k8s pod yaml 文件格式是兼容的。 首先，我们来创建一个 Pod： $ podman pod create --name postgresql -p 5432 -p 9187 ERRO[0042] Error freeing pod lock after failed creation: no such file or directory Error: unable to create pod: error adding Infra Container: unable to pull k8s.gcr.io/pause:3.1: unable to pull image: Error initializing source docker://k8s.gcr.io/pause:3.1: error pinging docker registry k8s.gcr.io: Get https://k8s.gcr.io/v2/: dial tcp 74.125.204.82:443: connect: connection refused 如何绕过尴尬的k8s.gcr.io $ podman pull registry.aliyuncs.com/google_containers/pause:3.1 Trying to pull registry.aliyuncs.com/google_containers/pause:3.1... Getting image source signatures Copying blob 67ddbfb20a22 done Copying config da86e6ba6c [======================================] 1.6KiB / 1.6KiB Writing manifest to image destination Storing signatures da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e $ podman tag registry.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1 [root@localhost ~]# podman pod create --name postgresql -p 5432 -p 9187 d6825c6afe78a471d39190c8caf57b7f034e604b2ad64a4d1a2918894d5ae890 #容器模式查看 $ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2a7e0325d459 registry.aliyuncs.com/google_containers/pause:3.1 22 seconds ago Created 0.0.0.0:5432->5432/tcp d6825c6afe78-infra #pod模式查看 podman pod ls POD ID NAME STATUS CREATED # OF CONTAINERS INFRA ID d6825c6afe78 postgresql Created 2 minutes ago 1 2a7e0325d459 $ podman pod ps POD ID NAME STATUS CREATED # OF CONTAINERS INFRA ID d6825c6afe78 postgresql Created 3 minutes ago 1 2a7e0325d459 运行一个demo #运行postgress容器 $ podman run -d --pod postgresql -e POSTGRES_PASSWORD=password postgres:latest Trying to pull docker.io/library/postgres:latest... Getting image source signatures Copying blob eff15d958d66 skipped: already exists Copying blob 108afa831d95 done Copying blob 7037ade1772d done Copying blob c877668f09b8 done Copying blob e5821d5963ce done Copying blob 5be06220aa99 done Copying blob de2b4ab3ade5 done Copying blob a60906bcf87f done Copying blob d46bc4e17ddc done Copying blob a1c85f71c941 done Copying blob 69f50e484cab done Copying blob 2f8a286a55a4 done Copying blob 8d590b0d720c done Copying config 577410342f done Writing manifest to image destination Storing signatures 8906e9ea92780c6c550ca4ae26dd398300ddb5d3f518d122bebb88a33a26ffeb #运行postgress_exporter容器 podman run -d --pod postgresql -e DATA_SOURCE_NAME=\"postgresql://postgres:password@localhost:5432/postgres?sslmode=disable\" wrouesnel/postgres_exporter Trying to pull docker.io/wrouesnel/postgres_exporter... Getting image source signatures Copying blob 45b42c59be33 done Copying blob 4634a89d50c2 done Copying blob fbcf7c278f83 done Copying config 9fe9d3d021 done Writing manifest to image destination Storing signatures 36f67b3e0fd06c1385595320288b0e41b183407859c1f4c48abf028599b06971 $ podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 36f67b3e0fd0 docker.io/wrouesnel/postgres_exporter:latest 3 minutes ago Up 3 minutes ago 0.0.0.0:5432->5432/tcp objective_lehmann 8906e9ea9278 docker.io/library/postgres:latest postgres 9 minutes ago Up 9 minutes ago 0.0.0.0:5432->5432/tcp reverent_shaw 首先我们创建了一个 Pod，端口映射是在 Pod 这个级别配置的，然后在这个 Pod 中，我们创建了两个 container，分别是：postgres 和 postgres_exporter ，其中 postgres_exporter 主要是暴露 metrics 用于 Prometheus 抓取进行监控。 我们可以通过 curl 相应端口来验证是否正常工作： $ curl localhost:9187/metrics # HELP go_gc_duration_seconds A summary of the GC invocation durations. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 0 go_gc_duration_seconds{quantile=\"0.25\"} 0 go_gc_duration_seconds{quantile=\"0.5\"} 0 go_gc_duration_seconds{quantile=\"0.75\"} 0 go_gc_duration_seconds{quantile=\"1\"} 0 go_gc_duration_seconds_sum 0 go_gc_duration_seconds_count 0 ······················· 可以看到已经正确的获取到了相应 metrics 数值，可以通过 podman pod top 来获取当前进程状态： $ podman pod top postgresql USER PID PPID %CPU ELAPSED TTY TIME COMMAND 0 1 0 0.000 11m43.122459727s ? 0s /pause postgres_exporter 1 0 0.000 5m51.124897013s ? 0s /postgres_exporter postgres 1 0 0.000 11m43.127713837s ? 0s postgres postgres 59 1 0.000 11m41.127813057s ? 0s postgres: checkpointer postgres 60 1 0.000 11m41.127878822s ? 0s postgres: background writer postgres 61 1 0.000 11m41.127943364s ? 0s postgres: walwriter postgres 62 1 0.000 11m41.128005075s ? 0s postgres: autovacuum launcher postgres 63 1 0.000 11m41.128133869s ? 0s postgres: stats collector postgres 64 1 0.000 11m41.128229337s ? 0s postgres: logical replication launcher postgres 74 1 0.000 1m49.12829442s ? 0s postgres: postgres postgres ::1(34118) idle 6.2 yaml管理 通过 podman generate 命令可以生成 k8s 可用的 YAML 文件： $ podman generate kube postgresql > postgresql.yaml $ podman generate kube postgresql # Generation of Kubernetes YAML is still under development! # # Save the output of this file and use kubectl create -f to import # it into Kubernetes. # # Created with podman-1.6.4 apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2021-11-29T14:34:40Z\" labels: app: postgresql name: postgresql spec: containers: - env: - name: PATH value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin - name: TERM value: xterm - name: HOSTNAME value: postgresql - name: container value: podman - name: DATA_SOURCE_NAME value: postgresql://postgres:password@localhost:5432/postgres?sslmode=disable image: docker.io/wrouesnel/postgres_exporter:latest name: objectivelehmann ports: - containerPort: 5432 hostPort: 5432 protocol: TCP - containerPort: 9187 hostPort: 9187 protocol: TCP resources: {} securityContext: allowPrivilegeEscalation: true capabilities: {} privileged: false readOnlyRootFilesystem: false runAsUser: 20001 workingDir: / - command: - postgres env: - name: PATH value: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/postgresql/14/bin - name: TERM value: xterm - name: HOSTNAME value: postgresql - name: container value: podman - name: PGDATA value: /var/lib/postgresql/data - name: POSTGRES_PASSWORD value: password - name: GOSU_VERSION value: \"1.12\" - name: LANG value: en_US.utf8 - name: PG_MAJOR value: \"14\" - name: PG_VERSION value: 14.1-1.pgdg110+1 image: docker.io/library/postgres:latest name: reverentshaw resources: {} securityContext: allowPrivilegeEscalation: true capabilities: {} privileged: false readOnlyRootFilesystem: false workingDir: / status: {} 使用 podman play 命令可以直接创建完整的 Pod 及其所拥有的容器： podman play kube postgresql.yaml ✈推荐阅读： docker 命令 podman 命令 crictl 命令 operator-sdk 命令 podman【2】 命令手册 podman【3】高级管理 Copyright © ghostwritten 浙ICP备2020032454号 2022 all right reserved，powered by Gitbook该文件修订时间： 2022-12-23 16:26:48 "}}